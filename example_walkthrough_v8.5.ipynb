{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6601183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMMY DO NOT RUN, this is just for my venv\n",
    "import sys\n",
    "sys.path.append('/Users/tunadorable/local-repos/next-concept-predictor/venv/lib/python3.11/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1555bf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439597f8",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "\n",
    "make sure to read comments & whatnot to keep track of what's happening. the accompanying pdf is very important for a conceptual background\n",
    "\n",
    "1. recap on the basic stuff (bulk of the transformer) which remains unchanged\n",
    "2. recap on what a regular GPT output layer looks like\n",
    "3. recap on how a regular GPT's training works\n",
    "4. recap on how a regular GPT's inference works\n",
    "5. my model's output layer\n",
    "6. my model's weird conditional logic\n",
    "7. my model's training\n",
    "8. my model's inference\n",
    "- 8a. first we'll get the user their output tokens (in batch)\n",
    "- 8b. then we'll create what the model is going to want for its next round of inference\n",
    "9. Turning everything into useful functions with printouts\n",
    "- 9a. gpt initialization\n",
    "- 9b. gpt guts\n",
    "- 9c. output/training/inference components of regular GPTs which we will not be using. I only wrote them out here for demonstration purposes.\n",
    "- 9d. my model's output layer\n",
    "- 9e. my model's weird conditional logic\n",
    "- 9f. my model's training\n",
    "- 9g. my model's inference process\n",
    "10. making a GPT training loop\n",
    "11. making an GPT inference loop\n",
    "12. training loop for my model\n",
    "13. inference loop for my model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855ea06b",
   "metadata": {},
   "source": [
    "# 1. the basic stuff / bulk of the transformer\n",
    "\n",
    "this section is left unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c290f5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting hyperparameters\n",
    "b=2\n",
    "n=7\n",
    "d=4\n",
    "v=12\n",
    "seed = 69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de6f74e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' I': 0,\n",
       " ' think': 1,\n",
       " ' there': 2,\n",
       " 'fore': 3,\n",
       " ' am': 4,\n",
       " 'Every': 5,\n",
       " ' cloud': 6,\n",
       " ' has': 7,\n",
       " ' a': 8,\n",
       " ' silver': 9,\n",
       " ' lining': 10,\n",
       " '<endoftext>': 11}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E_dict = {\" I\":0,\n",
    "          \" think\":1,\n",
    "         \" there\":2,\n",
    "         \"fore\":3,\n",
    "         \" am\":4,\n",
    "         \"Every\":5,\n",
    "         \" cloud\":6,\n",
    "         \" has\":7,\n",
    "         \" a\":8,\n",
    "         \" silver\":9,\n",
    "         \" lining\":10,\n",
    "         \"<endoftext>\":11}\n",
    "E_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89dda006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sn:  torch.int64 torch.Size([2, 7]) tensor([[ 0,  1,  2,  3,  0,  4, 11],\n",
      "        [ 5,  6,  7,  8,  9, 10, 11]])\n",
      "0\n",
      "S_i:  torch.int64 torch.Size([2, 1]) tensor([[0],\n",
      "        [5]])\n"
     ]
    }
   ],
   "source": [
    "# create sequence of tokens\n",
    "Sn_text = [[' I', ' think', ' there', 'fore', ' I', ' am', '<endoftext>'],\n",
    "               ['Every', ' cloud', ' has', ' a', ' silver', ' lining', '<endoftext>']]\n",
    "# turn into indices\n",
    "Sn_indices = [[E_dict[word] for word in sentence] for sentence in Sn_text]\n",
    "# turn into a tensor\n",
    "Sn = torch.tensor(Sn_indices)\n",
    "print(\"Sn: \", Sn.dtype, Sn.shape, Sn)\n",
    "\n",
    "# starting off with the first token for each sequence\n",
    "i=0\n",
    "if i==0: \n",
    "    print(i)\n",
    "    S_i = Sn[:,i].unsqueeze(dim=1) \n",
    "else: \n",
    "    print(i)\n",
    "    S_i = Sn[:,0:i+1]\n",
    "print(\"S_i: \", S_i.dtype, S_i.shape, S_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a0caae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:  torch.Size([12, 4]) tensor([[-0.5100, -1.3183,  0.5075,  1.3208],\n",
      "        [ 1.0348,  0.7062, -1.5466, -0.1943],\n",
      "        [-1.3227, -0.5735,  0.6994,  1.1968],\n",
      "        [-1.6470,  1.0348,  0.1860,  0.4262],\n",
      "        [-1.2501,  1.2517, -0.6605,  0.6589],\n",
      "        [-0.3892, -1.4613,  0.9034,  0.9471],\n",
      "        [-1.2375,  0.7945,  1.1540, -0.7109],\n",
      "        [-0.2447, -0.7559,  1.6978, -0.6973],\n",
      "        [-0.3729, -0.4507,  1.6999, -0.8764],\n",
      "        [ 1.6578, -0.3649, -1.0225, -0.2704],\n",
      "        [ 0.0544, -1.6133,  1.0787,  0.4802],\n",
      "        [-1.3672,  0.7058, -0.5100,  1.1714]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "X0:  torch.Size([2, 1, 4]) tensor([[[-0.5100, -1.3183,  0.5075,  1.3208]],\n",
      "\n",
      "        [[-0.3892, -1.4613,  0.9034,  0.9471]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# embedding matrix\n",
    "torch.manual_seed(seed)\n",
    "E = torch.randn(v,d)\n",
    "\n",
    "layer_norm = nn.LayerNorm(d)\n",
    "\n",
    "# Apply layer normalization to the matrix\n",
    "E = layer_norm(E)\n",
    "print(\"E: \", E.shape, E)\n",
    "\n",
    "# Look up the embeddings\n",
    "X0 = F.embedding(S_i, E)\n",
    "print(\"X0: \", X0.shape, X0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b0eaea",
   "metadata": {},
   "source": [
    "#### let's not actually code up the attention mechanism and say we did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b03e2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xf:  torch.Size([2, 1, 4]) tensor([[[-0.7480, -1.0568,  0.3128,  1.4920]],\n",
      "\n",
      "        [[-0.7732, -1.0541,  0.3562,  1.4711]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# ffn layer & residual connection\n",
    "torch.manual_seed(seed)\n",
    "W1 = torch.randn(d,d**2)\n",
    "X1 = torch.matmul(X0,W1)\n",
    "\n",
    "relu = nn.ReLU()\n",
    "X2 = relu(X1)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "W2 = torch.randn(d**2,d)\n",
    "Xf = layer_norm(torch.matmul(X2,W2)+X0)\n",
    "\n",
    "print(\"Xf: \", Xf.shape, Xf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41024b0",
   "metadata": {},
   "source": [
    "# 2. regular GPT output layer\n",
    "\n",
    "imma show how regular GPT does it just for reference when writing out mine later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60715203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z:  torch.Size([2, 1, 12]) tensor([[[ 3.9039, -2.2939,  3.5998,  0.8324,  0.3887,  3.5309, -0.6137,\n",
      "           0.4725, -0.0207, -1.5775,  2.7181,  1.8649]],\n",
      "\n",
      "        [[ 3.9077, -2.3812,  3.6370,  0.8759,  0.3811,  3.5563, -0.5154,\n",
      "           0.5650,  0.0796, -1.6590,  2.7493,  1.8547]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Z = torch.matmul(Xf,E.T)\n",
    "print(\"Z: \", Z.shape, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07cbec1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zn:  torch.Size([2, 12]) tensor([[ 3.9039, -2.2939,  3.5998,  0.8324,  0.3887,  3.5309, -0.6137,  0.4725,\n",
      "         -0.0207, -1.5775,  2.7181,  1.8649],\n",
      "        [ 3.9077, -2.3812,  3.6370,  0.8759,  0.3811,  3.5563, -0.5154,  0.5650,\n",
      "          0.0796, -1.6590,  2.7493,  1.8547]], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Z_prime = Z[:,-1,:]\n",
    "print(\"Z_prime: \", Z_prime.shape, Z_prime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f58b105",
   "metadata": {},
   "source": [
    "# 3. regular GPT training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "451eef5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_iplus1:  torch.int64 torch.Size([2]) tensor([1, 6])\n"
     ]
    }
   ],
   "source": [
    "# targets\n",
    "s_iplus1 = Sn[:,i+1]\n",
    "print(\"s_iplus1: \", s_iplus1.dtype, s_iplus1.shape, s_iplus1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d72ecb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  torch.Size([]) tensor(6.4204, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# cross-entropy loss\n",
    "loss = F.cross_entropy(Z_prime, s_iplus1)\n",
    "print(\"loss: \", loss.shape, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e060a9",
   "metadata": {},
   "source": [
    "#### then do backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1442bfa",
   "metadata": {},
   "source": [
    "# 4. regular GPT inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4f4d847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P:  torch.Size([2, 12]) tensor([[0.3325, 0.0007, 0.2453, 0.0154, 0.0099, 0.2290, 0.0036, 0.0108, 0.0066,\n",
      "         0.0014, 0.1016, 0.0433],\n",
      "        [0.3267, 0.0006, 0.2492, 0.0158, 0.0096, 0.2299, 0.0039, 0.0115, 0.0071,\n",
      "         0.0012, 0.1026, 0.0419]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "P = F.softmax(Zn, dim=-1)\n",
    "print(\"P: \", P.shape, P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf31be41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_iplus1:  torch.Size([2]) tensor([0, 0])\n"
     ]
    }
   ],
   "source": [
    "s_iplus1 = torch.max(P, dim=1).indices\n",
    "print(\"s_iplus1: \", s_iplus1.shape, s_iplus1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4b37ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S_iplus1:  torch.Size([2, 2]) tensor([[0, 0],\n",
      "        [5, 0]])\n"
     ]
    }
   ],
   "source": [
    "S_iplus1 = torch.cat((S_i, s_iplus1.unsqueeze(dim=1)), dim=1)\n",
    "print(\"S_iplus1: \", S_iplus1.shape, S_iplus1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fa91e6",
   "metadata": {},
   "source": [
    "# 5. my model's output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f110e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y:  torch.Size([2, 4]) tensor([[-0.7480, -1.0568,  0.3128,  1.4920],\n",
      "        [-0.7732, -1.0541,  0.3562,  1.4711]], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# select final row BEFORE multiplying by the embedding matrix\n",
    "Y = Xf[:,-1:].squeeze(dim=1)\n",
    "print(\"Y: \", Y.shape, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626fd9ab",
   "metadata": {},
   "source": [
    "in a normal GPT we'd first multiply by E.T and then select the row, because normal gpt's want to immediately select from a b,v tensor whereas we need a b,d tensor to give us concept vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a37b46cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z:  torch.Size([2, 12]) tensor([[ 3.9039, -2.2939,  3.5998,  0.8324,  0.3887,  3.5309, -0.6137,  0.4725,\n",
      "         -0.0207, -1.5775,  2.7181,  1.8649],\n",
      "        [ 3.9077, -2.3812,  3.6370,  0.8759,  0.3811,  3.5563, -0.5154,  0.5650,\n",
      "          0.0796, -1.6590,  2.7493,  1.8547]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Z = torch.matmul(Y,E.T)\n",
    "print(\"Z: \", Z.shape, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1e6c231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P:  torch.Size([2, 12]) tensor([[0.3325, 0.0007, 0.2453, 0.0154, 0.0099, 0.2290, 0.0036, 0.0108, 0.0066,\n",
      "         0.0014, 0.1016, 0.0433],\n",
      "        [0.3267, 0.0006, 0.2492, 0.0158, 0.0096, 0.2299, 0.0039, 0.0115, 0.0071,\n",
      "         0.0012, 0.1026, 0.0419]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "P = F.softmax(Z, dim=-1)\n",
    "print(\"P: \", P.shape, P)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df279e2",
   "metadata": {},
   "source": [
    "# 6. my model's conditional weird stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5a45949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_bar:  torch.Size([2]) tensor([0.3325, 0.3267], grad_fn=<MaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "P_bar = torch.max(P, dim=1).values\n",
    "print(\"P_bar: \", P_bar.shape, P_bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3118327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma0:  tensor(0.3296, grad_fn=<MeanBackward0>)\n",
      "Gamma_i:  torch.Size([2]) tensor([0.3296, 0.3296], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# to guarantee we get interesting results\n",
    "# in reality we'd set this as a hyperparameter\n",
    "gamma0 = torch.mean(P_bar)\n",
    "print(\"gamma0: \", gamma0)\n",
    "\n",
    "Gamma_i, D_i = None, None\n",
    "\n",
    "if i==0:\n",
    "    Gamma_i = torch.ones(b)*gamma0\n",
    "else:\n",
    "    try:\n",
    "        Gamma_i = Gamma_i + D_i\n",
    "    except TypeError:\n",
    "        print(\"Gamma_i and D_i are not defined. restart the sequence beginning at i=0\")\n",
    "        \n",
    "    \n",
    "try:\n",
    "    print(\"Gamma_i: \", Gamma_i.shape, Gamma_i)\n",
    "except:\n",
    "    print(\"Gamma_i and D_i are not defined. restart the sequence beginning at i=0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f0750a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_bar_mask:  torch.Size([2]) tensor([ True, False])\n",
      "A:  torch.float32 torch.Size([2]) tensor([1., 0.])\n"
     ]
    }
   ],
   "source": [
    "P_bar_mask = P_bar >= gamma0\n",
    "print(\"P_bar_mask: \", P_bar_mask.shape, P_bar_mask)\n",
    "\n",
    "A = torch.zeros(b)\n",
    "A[P_bar_mask] = 1\n",
    "print(\"A: \", A.dtype, A.shape, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7da44e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O:  torch.float32 torch.Size([2]) tensor([1., 1.])\n",
      "A_prime:  torch.float32 torch.Size([2]) tensor([0., 1.])\n"
     ]
    }
   ],
   "source": [
    "O = torch.ones(b)\n",
    "print(\"O: \", O.dtype, O.shape, O)\n",
    "\n",
    "A_prime = O-A\n",
    "print(\"A_prime: \", A_prime.dtype, A_prime.shape, A_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43ef405c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmax:  tensor(2.)\n",
      "delta_gamma:  tensor(0.3352, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# maximum allowable % increase in compute\n",
    "Cmax = torch.tensor(.7) \n",
    "\n",
    "# resulting maximum allowable extra tokens (since attention is O(n^2))\n",
    "tmax = torch.floor(torch.sqrt(Cmax*n))\n",
    "print(\"tmax: \", tmax)\n",
    "\n",
    "# resulting increment of gamma each time a concept vector is created\n",
    "delta_gamma = (torch.tensor(1)-gamma0)/tmax\n",
    "print(\"delta_gamma: \", delta_gamma)\n",
    "# it shouldn't be differentiable, that's just an artifact of me setting gamma0 to the mean of P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33dcd51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_i:  torch.Size([2]) tensor([0.0000, 0.3352], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "D_i = A_prime * delta_gamma\n",
    "print(\"D_i: \", D_i.shape, D_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31b4474f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma_iplus1:  torch.Size([2]) tensor([0.3296, 0.6648], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Gamma_iplus1 = Gamma_i + D_i\n",
    "print(\"Gamma_iplus1: \", Gamma_iplus1.shape, Gamma_iplus1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cfdc97",
   "metadata": {},
   "source": [
    "# 7. training with my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "630fb233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_iplus1:  torch.int64 torch.Size([2]) tensor([1, 6])\n"
     ]
    }
   ],
   "source": [
    "# targets\n",
    "s_iplus1 = Sn[:,i+1]\n",
    "print(\"s_iplus1: \", s_iplus1.dtype, s_iplus1.shape, s_iplus1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c346bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(logits, target, mask):\n",
    "    \n",
    "    assert isinstance(logits, torch.Tensor), \"logits must be a PyTorch tensor\"\n",
    "    assert isinstance(target, torch.Tensor), \"target must be a PyTorch tensor\"\n",
    "    assert isinstance(mask, torch.Tensor), \"mask must be a PyTorch tensor\"\n",
    "    \n",
    "    assert logits.shape == (b, v), f\"logits must have shape {b} (batch), {v} (vocab), but has shape {logits.shape}\"\n",
    "    assert target.shape == torch.Size([b]), f\"target must have shape {b} (batch), but has shape {target.shape}\"\n",
    "    assert mask.shape == torch.Size([b]), f\"mask must have shape {b} (batch), but has shape {mask.shape}\"\n",
    "    \n",
    "    # Apply LogSoftmax\n",
    "    log_probs = F.log_softmax(logits, dim=1)\n",
    "    print(\"log_probs: \", log_probs.shape, log_probs)\n",
    "    \n",
    "    # Gather the log probabilities corresponding to the true classes\n",
    "    gathered_probs = log_probs.gather(1, target.unsqueeze(1)).squeeze(1)\n",
    "    print(\"gathered_probs: \", gathered_probs.shape, gathered_probs)\n",
    "    \n",
    "    # Apply the mask\n",
    "    masked_probs = gathered_probs * mask\n",
    "    print(\"masked_probs: \", masked_probs.shape, masked_probs)\n",
    "\n",
    "    # Compute the average loss over unmasked entries\n",
    "    # Use A.sum() to count the number of unmasked entries\n",
    "    loss = -torch.sum(masked_probs) / torch.clamp(mask.sum(), min=1)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c6d2a1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_probs:  torch.Size([2, 12]) tensor([[-1.1011, -7.2988, -1.4052, -4.1725, -4.6162, -1.4741, -5.6186, -4.5325,\n",
      "         -5.0257, -6.5824, -2.2869, -3.1400],\n",
      "        [-1.1188, -7.4078, -1.3895, -4.1506, -4.6454, -1.4702, -5.5419, -4.4616,\n",
      "         -4.9469, -6.6856, -2.2773, -3.1719]], grad_fn=<LogSoftmaxBackward0>)\n",
      "gathered_probs:  torch.Size([2]) tensor([-7.2988, -5.5419], grad_fn=<SqueezeBackward1>)\n",
      "masked_probs:  torch.Size([2]) tensor([-0.0000, -5.5419], grad_fn=<MulBackward0>)\n",
      "Custom Loss: tensor(5.5419, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "loss = custom_loss(Z, s_iplus1, A)\n",
    "print(\"Custom Loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89de5ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogSoftmax Output: tensor([[-1.1011, -7.2988, -1.4052, -4.1725, -4.6162, -1.4741, -5.6186, -4.5325,\n",
      "         -5.0257, -6.5824, -2.2869, -3.1400],\n",
      "        [-1.1188, -7.4078, -1.3895, -4.1506, -4.6454, -1.4702, -5.5419, -4.4616,\n",
      "         -4.9469, -6.6856, -2.2773, -3.1719]], grad_fn=<LogSoftmaxBackward0>)\n",
      "NLLLoss Output: tensor(6.4204, grad_fn=<NllLossBackward0>)\n",
      "CrossEntropy Loss: tensor(6.4204, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# let's do regular cross-entropy for comparison (not gonna actually use it)\n",
    "# manual-ish calc\n",
    "log_softmax_output = F.log_softmax(Z, dim=1)\n",
    "nll_loss_output = F.nll_loss(log_softmax_output, s_iplus1)\n",
    "print(\"LogSoftmax Output:\", log_softmax_output)\n",
    "print(\"NLLLoss Output:\", nll_loss_output)\n",
    "# auto calc\n",
    "cross_entropy_loss = F.cross_entropy(Z, s_iplus1)\n",
    "print(\"CrossEntropy Loss:\", cross_entropy_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3460cba0",
   "metadata": {},
   "source": [
    "# 8. inference using my model\n",
    "\n",
    "#### 8a. first let's get the user their output tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e577a4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_iplus1:  torch.Size([2]) tensor([0, 0])\n"
     ]
    }
   ],
   "source": [
    "# boring non-masked predicted indices\n",
    "s_iplus1 = torch.max(P, dim=1).indices\n",
    "print(\"s_iplus1: \", s_iplus1.shape, s_iplus1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8fd9b9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_iplus1_prime:  torch.Size([2]) tensor([ 0, 12])\n",
      "remember an index value of 12 is equal to an embedding vector of entirely zeros and the empty token ''\n"
     ]
    }
   ],
   "source": [
    "# masked indices - the ones left will actually be shown to the user\n",
    "s_iplus1_prime = s_iplus1\n",
    "s_iplus1_prime[A_prime.bool()] = v\n",
    "print(\"s_iplus1_prime: \", s_iplus1_prime.shape, s_iplus1_prime)\n",
    "print(f\"remember an index value of {v} is equal to an embedding vector of entirely zeros and the empty token ''\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "330ec5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S_iplus1:  torch.Size([2, 2]) tensor([[ 0,  0],\n",
      "        [ 5, 12]])\n"
     ]
    }
   ],
   "source": [
    "# append to the current running sequence to give the user their output\n",
    "S_iplus1 = torch.cat((S_i, s_iplus1_prime.unsqueeze(dim=1)),dim=1)\n",
    "print(\"S_iplus1: \", S_iplus1.shape, S_iplus1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f78c44",
   "metadata": {},
   "source": [
    "#### that's the tokens that the user will see. notice how 12 is an empty token '' & embedding vector of 0's\n",
    "\n",
    "#### 8b. now let's get what the model needs for the next round of inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8efb41d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon aka the embedding vector of the empty token '':  torch.Size([4]) tensor([0., 0., 0., 0.])\n",
      "E_prime:  torch.Size([13, 4]) tensor([[-0.5100, -1.3183,  0.5075,  1.3208],\n",
      "        [ 1.0348,  0.7062, -1.5466, -0.1943],\n",
      "        [-1.3227, -0.5735,  0.6994,  1.1968],\n",
      "        [-1.6470,  1.0348,  0.1860,  0.4262],\n",
      "        [-1.2501,  1.2517, -0.6605,  0.6589],\n",
      "        [-0.3892, -1.4613,  0.9034,  0.9471],\n",
      "        [-1.2375,  0.7945,  1.1540, -0.7109],\n",
      "        [-0.2447, -0.7559,  1.6978, -0.6973],\n",
      "        [-0.3729, -0.4507,  1.6999, -0.8764],\n",
      "        [ 1.6578, -0.3649, -1.0225, -0.2704],\n",
      "        [ 0.0544, -1.6133,  1.0787,  0.4802],\n",
      "        [-1.3672,  0.7058, -0.5100,  1.1714],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "epsilon = torch.zeros(d)\n",
    "print(\"epsilon aka the embedding vector of the empty token '': \", epsilon.shape, epsilon)\n",
    "\n",
    "E_prime = torch.cat((E,epsilon.unsqueeze(0)), dim=0)\n",
    "print(\"E_prime: \", E_prime.shape, E_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bf98f1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_token:  torch.Size([2, 4]) tensor([[-0.5100, -1.3183,  0.5075,  1.3208],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Y_token = F.embedding(s_iplus1_prime, E_prime)\n",
    "print(\"Y_token: \", Y_token.shape, Y_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "53728a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_prime_expand:  torch.Size([2, 1]) tensor([[0.],\n",
      "        [1.]])\n",
      "Y_concept:  torch.Size([2, 4]) tensor([[-0.0000, -0.0000,  0.0000,  0.0000],\n",
      "        [-0.7732, -1.0541,  0.3562,  1.4711]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "A_prime_expand = A_prime.unsqueeze(dim=1)\n",
    "print(\"A_prime_expand: \", A_prime_expand.shape, A_prime_expand)\n",
    "\n",
    "Y_concept = Y*A_prime_expand\n",
    "print(\"Y_concept: \", Y_concept.shape, Y_concept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3aa41898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_inference:  torch.Size([2, 4]) tensor([[-0.5100, -1.3183,  0.5075,  1.3208],\n",
      "        [-0.7732, -1.0541,  0.3562,  1.4711]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Y_inference = Y_token + Y_concept\n",
    "print(\"Y_inference: \", Y_inference.shape, Y_inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e845c09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X0_i:  torch.Size([2, 1, 4]) tensor([[[-0.5100, -1.3183,  0.5075,  1.3208]],\n",
      "\n",
      "        [[-0.3892, -1.4613,  0.9034,  0.9471]]], grad_fn=<EmbeddingBackward0>)\n",
      "Y_inference_prime:  torch.Size([2, 1, 4]) tensor([[[-0.5100, -1.3183,  0.5075,  1.3208]],\n",
      "\n",
      "        [[-0.7732, -1.0541,  0.3562,  1.4711]]], grad_fn=<UnsqueezeBackward0>)\n",
      "X0_iplus1:  torch.Size([2, 2, 4]) tensor([[[-0.5100, -1.3183,  0.5075,  1.3208],\n",
      "         [-0.5100, -1.3183,  0.5075,  1.3208]],\n",
      "\n",
      "        [[-0.3892, -1.4613,  0.9034,  0.9471],\n",
      "         [-0.7732, -1.0541,  0.3562,  1.4711]]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"X0_i: \", X0.shape, X0)\n",
    "\n",
    "Y_inference_prime = Y_inference.unsqueeze(dim=1)\n",
    "print(\"Y_inference_prime: \", Y_inference_prime.shape, Y_inference_prime)\n",
    "\n",
    "X0_iplus1 = torch.cat((X0, Y_inference_prime), dim=1)\n",
    "print(\"X0_iplus1: \", X0_iplus1.shape, X0_iplus1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426bf3f7",
   "metadata": {},
   "source": [
    "#### and that's the residual state that the model gets to use on its next inference pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec11b62",
   "metadata": {},
   "source": [
    "# 9. turning it into useful functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac81c271",
   "metadata": {},
   "source": [
    "#### 9a. first we'll make the setup functions that both regular GPT's and our model use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69338e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_sequence(i, S_n_text):\n",
    "    global verbose, seed\n",
    "    \n",
    "    # turn into indices\n",
    "    S_n_indices = [[E_dict[word] for word in sentence] for sentence in S_n_text]\n",
    "    # turn into a tensor\n",
    "    S_n = torch.tensor(S_n_indices)\n",
    "    if verbose:\n",
    "        print(\"S_n: \", S_n.dtype, S_n.shape, S_n)\n",
    "\n",
    "    # starting off with the first token for each sequence\n",
    "    if i==0: \n",
    "        if verbose:\n",
    "            print(i)\n",
    "        S_i = S_n[:,i].unsqueeze(dim=1) \n",
    "    else: \n",
    "        if verbose:\n",
    "            print(i)\n",
    "        S_i = S_n[:,0:i+1]\n",
    "        \n",
    "    if verbose:\n",
    "        print(\"S_i: \", S_i.dtype, Si.shape, S_i)\n",
    "        \n",
    "    return S_n, S_i\n",
    "\n",
    "def create_embeddings(v, d):\n",
    "    global verbose, seed, layer_norm\n",
    "    \n",
    "    # embedding matrix\n",
    "    torch.manual_seed(seed)\n",
    "    E = torch.randn(v,d)\n",
    "\n",
    "    # Apply layer normalization to the matrix\n",
    "    E = layer_norm(E)\n",
    "    if verbose:\n",
    "        print(\"E: \", E.shape, E)\n",
    "        \n",
    "    return E\n",
    "\n",
    "def create_first_resid(S_i, E):\n",
    "    global verbose\n",
    "    \n",
    "    # Look up the embeddings\n",
    "    X0 = F.embedding(S_i, E)\n",
    "    if verbose:\n",
    "        print(\"X0: \", X0.shape, X0)\n",
    "        \n",
    "    return X0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4363549",
   "metadata": {},
   "source": [
    "#### 9b. the guts of the model, which we will not be editing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c11b9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mha(x, h):\n",
    "    # fuck this i'm not editing attention so no need to mess with it\n",
    "    return x\n",
    "\n",
    "def ffn(X0):\n",
    "    global verbose, seed\n",
    "    \"\"\"\n",
    "    I know i didn't do mha but i think i might need a nonlinearity in here \n",
    "    so i guess i'll do the ffn\n",
    "    \"\"\"\n",
    "    \n",
    "    # first linear layer\n",
    "    torch.manual_seed(seed)\n",
    "    W1 = torch.randn(d,d**2)\n",
    "    if verbose:\n",
    "        print(\"W1: \", W1.shape, W1)\n",
    "    X1 = torch.matmul(X0,W1)\n",
    "    if verbose:\n",
    "        print(\"X1: \", X1.shape, X1)\n",
    "\n",
    "    # activation function\n",
    "    relu = nn.ReLU()\n",
    "    X2 = relu(X1)\n",
    "    if verbose:\n",
    "        print(\"X2: \", X2.shape, X2)\n",
    "\n",
    "    # second linear layer\n",
    "    torch.manual_seed(seed)\n",
    "    W2 = torch.randn(d**2,d)\n",
    "    if verbose:\n",
    "        print(\"W2: \", W2.shape, W2)\n",
    "    X3 = torch.matmul(X2,W2)\n",
    "    if verbose:\n",
    "        print(\"X3: \", X3.shape, X3)\n",
    "    \n",
    "    return X3\n",
    "    \n",
    "def resid_connection(X0, Xi):\n",
    "    global verbose, layer_norm\n",
    "    \n",
    "    # can't remember when / how frequently you layernorm\n",
    "    # too lazy to check, i don't think it matters\n",
    "    Xf = layer_norm(Xi+X0)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Xf: \", Xf.shape, Xf)\n",
    "        \n",
    "    return Xf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832de2c6",
   "metadata": {},
   "source": [
    "#### 9c. the stuff a regular next-token predictor GPT uses which we will not be interested in. I still wrote it out for sake of demonstration tho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "934def6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_output_layer(Xf, E):\n",
    "    global verbose\n",
    "    \n",
    "    Z = torch.matmul(Xf,E.T)\n",
    "    if verbose:\n",
    "        print(\"E^T: \", E.T.shape, E.T)\n",
    "        print(\"Z: \", Z.shape, Z)\n",
    "    \n",
    "    Z_prime = Z[:,-1,:]\n",
    "    if verbose:\n",
    "        print(\"Z_prime: \", Z_prime.shape, Z_prime)\n",
    "    \n",
    "    return Z_prime\n",
    "\n",
    "def gpt_get_loss(S_n, i, Z_prime):\n",
    "    global verbose\n",
    "    \n",
    "    # targets\n",
    "    s_iplus1 = S_n[:,i+1]\n",
    "    if verbose:\n",
    "        print(\"s_iplus1: \", s_iplus1.dtype, s_iplus1.shape, s_iplus1)\n",
    "    \n",
    "    # cross-entropy loss\n",
    "    loss = F.cross_entropy(Z_prime, s_iplus1)\n",
    "    if verbose:\n",
    "        print(\"loss: \", loss.shape, loss)\n",
    "        \n",
    "    return s_iplus1, loss\n",
    "\n",
    "def gpt_perform_inference(Zn, S_i):\n",
    "    global verbose\n",
    "    \n",
    "    P = F.softmax(Zn, dim=-1)\n",
    "    if verbose:\n",
    "        print(\"P: \", P.shape, P)\n",
    "\n",
    "    s_iplus1 = torch.max(P, dim=1).indices\n",
    "    if verbose:\n",
    "        print(\"s_iplus1: \", s_iplus1.shape, s_iplus1)\n",
    "\n",
    "    # capital S means whole sequence while lowercase s means just that timestep of tokens\n",
    "    S_iplus1 = torch.cat((S_i, s_iplus1.unsqueeze(dim=1)), dim=1)\n",
    "    if verbose:\n",
    "        print(\"S_iplus1: \", S_iplus1.shape, S_iplus1)\n",
    "        \n",
    "    return P, s_iplus1, S_iplus1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0e47f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2D tensor: ['hello world', 'world hello']\n",
      "1D tensor: hello world\n"
     ]
    }
   ],
   "source": [
    "# a function to give us a pretty read off of the sequences\n",
    "def tensor_to_string(tensor, index_to_token):\n",
    "    \"\"\"\n",
    "    Convert a PyTorch tensor of indices into readable strings using a mapping dictionary.\n",
    "\n",
    "    :param tensor: PyTorch tensor of indices. Can be 1D or 2D.\n",
    "    :param index_to_token: Dictionary mapping indices to tokens.\n",
    "    :return: A single string if the tensor is 1D, or a list of strings if the tensor is 2D.\n",
    "    \"\"\"\n",
    "    # Check if the tensor is 1D or 2D\n",
    "    if len(tensor.shape) == 1:\n",
    "        # Convert the 1D tensor to a single string\n",
    "        return ' '.join([index_to_token[int(idx)] for idx in tensor])\n",
    "    elif len(tensor.shape) == 2:\n",
    "        # Convert the tensor to a list of lists and then to strings\n",
    "        sequences = tensor.tolist()\n",
    "        return [' '.join([index_to_token[int(idx)] for idx in seq]) for seq in sequences]\n",
    "    else:\n",
    "        raise ValueError(\"Tensor must be either 1D or 2D\")\n",
    "\n",
    "# Example usage\n",
    "token_to_index = {'hello': 1, 'world': 2}\n",
    "index_to_token = {v: k for k, v in token_to_index.items()}\n",
    "S = torch.tensor([[1, 2], [2, 1]])  # Example tensor\n",
    "S_single = torch.tensor([1, 2])     # Example 1D tensor\n",
    "\n",
    "readable_strings_2d = tensor_to_string(S, index_to_token)\n",
    "readable_string_1d = tensor_to_string(S_single, index_to_token)\n",
    "\n",
    "print(\"2D tensor:\", readable_strings_2d)\n",
    "print(\"1D tensor:\", readable_string_1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e772f8d",
   "metadata": {},
   "source": [
    "#### 9d. my model's output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a861be12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gamma_model_output_layer(Xf, E):\n",
    "    global verbose\n",
    "    \n",
    "    # select final row BEFORE multiplying by the embedding matrix\n",
    "    Y = Xf[:,-1:].squeeze(dim=1)\n",
    "    if verbose:\n",
    "        print(\"Y: \", Y.shape, Y)\n",
    "\n",
    "    Z = torch.matmul(Y,E.T)\n",
    "    if verbose:\n",
    "        print(\"Z: \", Z.shape, Z)\n",
    "\n",
    "    P = F.softmax(Z, dim=-1)\n",
    "    if verbose:\n",
    "        print(\"P: \", P.shape, P)\n",
    "        \n",
    "    return Y, Z, P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870755dd",
   "metadata": {},
   "source": [
    "#### 9e. my model's weird conditional logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec7de70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceError(Exception):\n",
    "    pass\n",
    "\n",
    "def initialize_Gamma_and_D(i):\n",
    "    if i!=0:\n",
    "        raise SequenceError(\"This function should only be called when i=0, aka at the beginning of the batch/sequence\")\n",
    "    else:\n",
    "        Gamma_i = torch.ones(b)*gamma0\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Gamma_i: \", Gamma_i.shape, Gamma_i)\n",
    "        \n",
    "    return Gamma_i\n",
    "\n",
    "def iterate_Gamma_and_D(i, Gamma_i, D_i):\n",
    "    if i<=0:\n",
    "        raise SequenceError(\"This function should only be called when i>0, aka NOT at the beginning of the batch/sequence\")\n",
    "    else:\n",
    "        Gamma_i = torch.clamp(Gamma_i + D_i, max=1)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Gamma_i: \", Gamma_i.shape, Gamma_i)\n",
    "        \n",
    "    return Gamma_i\n",
    "   \n",
    "def creating_conditionals(P, gamma0):\n",
    "    \n",
    "    P_bar = torch.max(P, dim=1).values\n",
    "    if verbose:\n",
    "        print(\"P_bar: \", P_bar.shape, P_bar)\n",
    "\n",
    "    P_bar_mask = P_bar >= gamma0\n",
    "    if verbose:\n",
    "        print(\"P_bar_mask: \", P_bar_mask.shape, P_bar_mask)\n",
    "\n",
    "    A = torch.zeros(b)\n",
    "    A[P_bar_mask] = 1\n",
    "    if verbose:\n",
    "        print(\"A: \", A.dtype, A.shape, A)\n",
    "\n",
    "    O = torch.ones(b)\n",
    "    if verbose:\n",
    "        print(\"O: \", O.dtype, O.shape, O)\n",
    "\n",
    "    A_prime = O-A\n",
    "    if verbose:\n",
    "        print(\"A_prime: \", A_prime.dtype, A_prime.shape, A_prime)\n",
    "        \n",
    "    return A, O, A_prime\n",
    "\n",
    "def calc_delta_gamma(Cmax, n, A_prime):\n",
    "    \n",
    "    # resulting maximum allowable extra tokens (since attention is O(n^2))\n",
    "    tmax = torch.floor(torch.sqrt(Cmax*n))\n",
    "    if verbose:\n",
    "        print(\"tmax: \", tmax)\n",
    "\n",
    "    # resulting increment of gamma each time a concept vector is created\n",
    "    delta_gamma = (torch.tensor(1)-gamma0)/tmax\n",
    "    if verbose:\n",
    "        print(\"delta_gamma: \", delta_gamma)\n",
    "    \n",
    "    D_i = A_prime * delta_gamma\n",
    "    if verbose:\n",
    "        print(\"D_i: \", D_i.shape, D_i)\n",
    "\n",
    "    return delta_gamma, D_i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e97417",
   "metadata": {},
   "source": [
    "#### 9f. training w/ my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "034c089c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_tokens(i, S_n):\n",
    "    global verbose\n",
    "    \n",
    "    # targets\n",
    "    s_iplus1 = S_n[:,i+1]\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"s_iplus1: \", s_iplus1.dtype, s_iplus1.shape, s_iplus1)\n",
    "        \n",
    "    return s_iplus1\n",
    "\n",
    "def custom_loss(logits, target, mask):\n",
    "    \n",
    "    assert isinstance(logits, torch.Tensor), \"logits must be a PyTorch tensor\"\n",
    "    assert isinstance(target, torch.Tensor), \"target must be a PyTorch tensor\"\n",
    "    assert isinstance(mask, torch.Tensor), \"mask must be a PyTorch tensor\"\n",
    "    \n",
    "    assert logits.shape == (b, v), f\"logits must have shape {b} (batch), {v} (vocab), but has shape {logits.shape}\"\n",
    "    assert target.shape == torch.Size([b]), f\"target must have shape {b} (batch), but has shape {target.shape}\"\n",
    "    assert mask.shape == torch.Size([b]), f\"mask must have shape {b} (batch), but has shape {mask.shape}\"\n",
    "    \n",
    "    # Apply LogSoftmax\n",
    "    log_probs = F.log_softmax(logits, dim=1)\n",
    "    if verbose:\n",
    "        print(\"log_probs: \", log_probs.shape, log_probs)\n",
    "    \n",
    "    # Gather the log probabilities corresponding to the true classes\n",
    "    gathered_probs = log_probs.gather(1, target.unsqueeze(1)).squeeze(1)\n",
    "    if verbose:\n",
    "        print(\"gathered_probs: \", gathered_probs.shape, gathered_probs)\n",
    "    \n",
    "    # Apply the mask\n",
    "    masked_probs = gathered_probs * mask\n",
    "    if verbose:\n",
    "        print(\"masked_probs: \", masked_probs.shape, masked_probs)\n",
    "\n",
    "    # Compute the average loss over unmasked entries\n",
    "    # Use A.sum() to count the number of unmasked entries\n",
    "    loss = -torch.sum(masked_probs) / torch.clamp(mask.sum(), min=1)\n",
    "    if verbose:\n",
    "        print(\"Custom Loss:\", loss)\n",
    "        \n",
    "    return loss\n",
    "\n",
    "# Example usage\n",
    "#loss = custom_loss(Z, s_iplus1, A_prime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c77283",
   "metadata": {},
   "source": [
    "#### 9g. inference w/ my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06e52b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tokens_that_user_sees(P, A_prime, v, S_i):\n",
    "    global verbose\n",
    "\n",
    "    # boring non-masked predicted indices from inference\n",
    "    s_iplus1 = torch.max(P, dim=1).indices\n",
    "    if verbose:\n",
    "        print(\"s_iplus1: \", s_iplus1.shape, s_iplus1)\n",
    "\n",
    "    # masked indices - the ones left will actually be shown to the user\n",
    "    s_iplus1_prime = s_iplus1\n",
    "    s_iplus1_prime[A_prime.bool()] = v\n",
    "    if verbose:\n",
    "        print(\"s_iplus1_prime: \", s_iplus1_prime.shape, s_iplus1_prime)\n",
    "        print(f\"remember an index value of {v} is equal to an embedding vector of entirely zeros and the empty token ''\")\n",
    "\n",
    "    # append to the current running sequence to give the user their output\n",
    "    S_iplus1 = torch.cat((S_i, s_iplus1_prime.unsqueeze(dim=1)),dim=1)\n",
    "    if verbose:\n",
    "        print(\"S_iplus1: \", S_iplus1.shape, S_iplus1)\n",
    "        \n",
    "    return s_iplus1_prime, S_iplus1\n",
    "\n",
    "def make_next_input_for_model(d, E, s_iplus1_prime, A_prime, Y, X0):\n",
    "    global verbose\n",
    "    \n",
    "    epsilon = torch.zeros(d)\n",
    "    if verbose:\n",
    "        print(\"epsilon aka the embedding vector of the empty token '': \", epsilon.shape, epsilon)\n",
    "\n",
    "    E_prime = torch.cat((E,epsilon.unsqueeze(0)), dim=0)\n",
    "    if verbose:\n",
    "        print(\"E_prime: \", E_prime.shape, E_prime)\n",
    "\n",
    "    Y_token = F.embedding(s_iplus1_prime, E_prime)\n",
    "    if verbose:\n",
    "        print(\"Y_token: \", Y_token.shape, Y_token)\n",
    "\n",
    "    A_prime_expand = A_prime.unsqueeze(dim=1)\n",
    "    if verbose:\n",
    "        print(\"A_prime_expand: \", A_prime_expand.shape, A_prime_expand)\n",
    "\n",
    "    Y_concept = Y*A_prime_expand\n",
    "    if verbose:\n",
    "        print(\"Y_concept: \", Y_concept.shape, Y_concept)\n",
    "\n",
    "    Y_inference = Y_token + Y_concept\n",
    "    if verbose:\n",
    "        print(\"Y_inference: \", Y_inference.shape, Y_inference)\n",
    "        print(\"X0_i: \", X0.shape, X0)\n",
    "\n",
    "    Y_inference_prime = Y_inference.unsqueeze(dim=1)\n",
    "    if verbose:\n",
    "        print(\"Y_inference_prime: \", Y_inference_prime.shape, Y_inference_prime)\n",
    "\n",
    "    X0_iplus1 = torch.cat((X0, Y_inference_prime), dim=1)\n",
    "    if verbose:\n",
    "        print(\"X0_iplus1: \", X0_iplus1.shape, X0_iplus1)\n",
    "        \n",
    "    return X0_iplus1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fd958f",
   "metadata": {},
   "source": [
    "# 10. making a GPT training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72e34be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting regular hyperparameters\n",
    "b=2\n",
    "n=7\n",
    "d=4\n",
    "v=12\n",
    "layers = 2\n",
    "h=4\n",
    "\n",
    "layer_norm = nn.LayerNorm(d)\n",
    "\n",
    "# our tokens being mapped to indices. obvi a stupidly minimally small vocab\n",
    "E_dict = {\" I\":0,\n",
    "          \" think\":1,\n",
    "         \" there\":2,\n",
    "         \"fore\":3,\n",
    "         \" am\":4,\n",
    "         \"Every\":5,\n",
    "         \" cloud\":6,\n",
    "         \" has\":7,\n",
    "         \" a\":8,\n",
    "         \" silver\":9,\n",
    "         \" lining\":10,\n",
    "         \"<endoftext>\":11}\n",
    "\n",
    "S_n_text = [[' I', ' think', ' there', 'fore', ' I', ' am', '<endoftext>'],\n",
    "               ['Every', ' cloud', ' has', ' a', ' silver', ' lining', '<endoftext>']]\n",
    "\n",
    "# ofc\n",
    "seed = 42069\n",
    "\n",
    "# to print literally everything\n",
    "verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "351bc96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0\n",
      "context S_i:  [' I', 'Every']\n",
      "target s_iplus1:   think  cloud\n",
      "loss:  torch.Size([]) tensor(2.0714, grad_fn=<NllLossBackward0>)\n",
      "i = 1\n",
      "context S_i:  [' I  think', 'Every  cloud']\n",
      "target s_iplus1:   there  has\n",
      "loss:  torch.Size([]) tensor(5.4166, grad_fn=<NllLossBackward0>)\n",
      "i = 2\n",
      "context S_i:  [' I  think  there', 'Every  cloud  has']\n",
      "target s_iplus1:  fore  a\n",
      "loss:  torch.Size([]) tensor(5.7736, grad_fn=<NllLossBackward0>)\n",
      "i = 3\n",
      "context S_i:  [' I  think  there fore', 'Every  cloud  has  a']\n",
      "target s_iplus1:   I  silver\n",
      "loss:  torch.Size([]) tensor(2.4356, grad_fn=<NllLossBackward0>)\n",
      "i = 4\n",
      "context S_i:  [' I  think  there fore  I', 'Every  cloud  has  a  silver']\n",
      "target s_iplus1:   am  lining\n",
      "loss:  torch.Size([]) tensor(5.7470, grad_fn=<NllLossBackward0>)\n",
      "i = 5\n",
      "context S_i:  [' I  think  there fore  I  am', 'Every  cloud  has  a  silver  lining']\n",
      "target s_iplus1:  <endoftext> <endoftext>\n",
      "loss:  torch.Size([]) tensor(4.1991, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for i in range(n-1):\n",
    "    \n",
    "    print(f\"i = {i}\")\n",
    "    \n",
    "    # initialize everything\n",
    "    if i==0:\n",
    "        E = create_embeddings(v, d)\n",
    "    \n",
    "    S_n, S_i = create_input_sequence(i, S_n_text)\n",
    "    \n",
    "    E_dict_invert = {g: h for h, g in E_dict.items()}\n",
    "    readable_strings = tensor_to_string(S_i, E_dict_invert)\n",
    "    print(\"context S_i: \", readable_strings)\n",
    "    \n",
    "    X0 = create_first_resid(S_i, E)\n",
    "    \n",
    "    for l in range(layers):\n",
    "        \n",
    "        # multi-head attention, feedforward network, & residual connection\n",
    "        if l== 0: # first layer\n",
    "            Xi = mha(X0, h)\n",
    "            Xi = ffn(Xi)\n",
    "            Xi = resid_connection(X0, Xi)\n",
    "            \n",
    "        elif l == layers-1: # last layer\n",
    "            X_iplus1 = mha(Xi, h)\n",
    "            X_iplus1 = ffn(X_iplus1)\n",
    "            Xf = resid_connection(Xi, X_iplus1)\n",
    "            \n",
    "        else: # all layers in the middle\n",
    "            X_iplus1 = mha(Xi, h)\n",
    "            X_iplus1 = ffn(X_iplus1)\n",
    "            Xi = resid_connection(Xi, X_iplus1)\n",
    "            \n",
    "    Z_prime = gpt_output_layer(Xf, E)\n",
    "\n",
    "    s_iplus1, loss = gpt_get_loss(S_n, i, Z_prime)\n",
    "    \n",
    "    readable_strings = tensor_to_string(s_iplus1, E_dict_invert)\n",
    "    print(\"target s_iplus1: \", readable_strings)\n",
    "        \n",
    "    print(\"loss: \", loss.shape, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38293c38",
   "metadata": {},
   "source": [
    "# 11. making a GPT inference loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73267aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting regular hyperparameters\n",
    "b=1\n",
    "n=7\n",
    "d=4\n",
    "v=12\n",
    "layers = 2\n",
    "h=4\n",
    "\n",
    "layer_norm = nn.LayerNorm(d)\n",
    "\n",
    "# our tokens being mapped to indices. obvi a stupidly minimally small vocab\n",
    "E_dict = {\" I\":0,\n",
    "          \" think\":1,\n",
    "         \" there\":2,\n",
    "         \"fore\":3,\n",
    "         \" am\":4,\n",
    "         \"Every\":5,\n",
    "         \" cloud\":6,\n",
    "         \" has\":7,\n",
    "         \" a\":8,\n",
    "         \" silver\":9,\n",
    "         \" lining\":10,\n",
    "         \"<endoftext>\":11}\n",
    "\n",
    "S_n_text = [[' I', ' think', ' there', 'fore', ' I', ' am', '<endoftext>']]\n",
    "\n",
    "# ofc\n",
    "seed = 42069\n",
    "\n",
    "# to print literally everything\n",
    "verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d309734e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0\n",
      "context S_i:  [' I']\n",
      "predicted s_iplus1:   silver\n",
      "new total text visible to user:  [' I  silver']\n",
      "i = 1\n",
      "context S_i:  [' I  silver']\n",
      "predicted s_iplus1:   silver\n",
      "new total text visible to user:  [' I  silver  silver']\n",
      "i = 2\n",
      "context S_i:  [' I  silver  silver']\n",
      "predicted s_iplus1:   silver\n",
      "new total text visible to user:  [' I  silver  silver  silver']\n",
      "i = 3\n",
      "context S_i:  [' I  silver  silver  silver']\n",
      "predicted s_iplus1:   silver\n",
      "new total text visible to user:  [' I  silver  silver  silver  silver']\n",
      "i = 4\n",
      "context S_i:  [' I  silver  silver  silver  silver']\n",
      "predicted s_iplus1:   silver\n",
      "new total text visible to user:  [' I  silver  silver  silver  silver  silver']\n",
      "i = 5\n",
      "context S_i:  [' I  silver  silver  silver  silver  silver']\n",
      "predicted s_iplus1:   silver\n",
      "new total text visible to user:  [' I  silver  silver  silver  silver  silver  silver']\n"
     ]
    }
   ],
   "source": [
    "# inference loop\n",
    "\n",
    "for i in range(n-1):\n",
    "    \n",
    "    print(f\"i = {i}\")\n",
    "    \n",
    "    # initialize everything\n",
    "    if i==0:\n",
    "        E = create_embeddings(v, d)\n",
    "        S_n, S_i = create_input_sequence(i, S_n_text)\n",
    "    \n",
    "    E_dict_invert = {g: h for h, g in E_dict.items()}\n",
    "    readable_strings = tensor_to_string(S_i, E_dict_invert)\n",
    "    print(\"context S_i: \", readable_strings)\n",
    "    \n",
    "    X0 = create_first_resid(S_i, E)\n",
    "    \n",
    "    for l in range(layers):\n",
    "        \n",
    "        # multi-head attention, feedforward network, & residual connection\n",
    "        if l== 0: # first layer\n",
    "            Xi = mha(X0, h)\n",
    "            Xi = ffn(Xi)\n",
    "            Xi = resid_connection(X0, Xi)\n",
    "            \n",
    "        elif l == layers-1: # last layer\n",
    "            X_iplus1 = mha(Xi, h)\n",
    "            X_iplus1 = ffn(X_iplus1)\n",
    "            Xf = resid_connection(Xi, X_iplus1)\n",
    "            \n",
    "        else: # all layers in the middle\n",
    "            X_iplus1 = mha(Xi, h)\n",
    "            X_iplus1 = ffn(X_iplus1)\n",
    "            Xi = resid_connection(Xi, X_iplus1)\n",
    "            \n",
    "    Z_prime = gpt_output_layer(Xf, E)\n",
    "\n",
    "    P, s_iplus1, S_iplus1 = gpt_perform_inference(Z_prime, S_i)\n",
    "    \n",
    "    readable_strings = tensor_to_string(s_iplus1, E_dict_invert)\n",
    "    print(\"predicted s_iplus1: \", readable_strings)\n",
    "    \n",
    "    readable_strings = tensor_to_string(S_iplus1, E_dict_invert)\n",
    "    print(\"new total text visible to user: \", readable_strings)\n",
    "    \n",
    "    S_i = S_iplus1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824ec62b",
   "metadata": {},
   "source": [
    "#### obvi this has not gone through any training so repetition isn't surprising. also i didn't implement a way for it to finish at \\<endoftext> bc i didn't realistically expect it to end then"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd3c20e",
   "metadata": {},
   "source": [
    "# 12. gamma-model training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "19e7704c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting regular hyperparameters\n",
    "b=2\n",
    "n=7\n",
    "d=4\n",
    "v=12\n",
    "layers = 4\n",
    "h=4\n",
    "\n",
    "layer_norm = nn.LayerNorm(d)\n",
    "\n",
    "# our tokens being mapped to indices. obvi a stupidly minimally small vocab\n",
    "E_dict = {\" I\":0,\n",
    "          \" think\":1,\n",
    "         \" there\":2,\n",
    "         \"fore\":3,\n",
    "         \" am\":4,\n",
    "         \"Every\":5,\n",
    "         \" cloud\":6,\n",
    "         \" has\":7,\n",
    "         \" a\":8,\n",
    "         \" silver\":9,\n",
    "         \" lining\":10,\n",
    "         \"<endoftext>\":11}\n",
    "\n",
    "S_n_text = [[' I', ' think', ' there', 'fore', ' I', ' am', '<endoftext>'],\n",
    "               ['Every', ' cloud', ' has', ' a', ' silver', ' lining', '<endoftext>']]\n",
    "\n",
    "# ofc\n",
    "seed = 42069\n",
    "\n",
    "# to print literally everything\n",
    "verbose = False\n",
    "\n",
    "# hyperparameter we'll have to tune\n",
    "gamma0 = 0.244\n",
    "\n",
    "# maximum allowable % increase in compute\n",
    "Cmax = torch.tensor(.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b957c1d1",
   "metadata": {},
   "source": [
    "#### currently this setup doesn't actually create the next X0; likely among many other issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e18f4446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0\n",
      "Gamma_i:  torch.Size([2]) tensor([0.2440, 0.2440])\n",
      "context S_i:  [' I', 'Every']\n",
      "A:  torch.Size([2]) tensor([0., 1.])\n",
      "target s_iplus1:   think  cloud\n",
      "loss:  torch.Size([]) tensor(2.6002, grad_fn=<DivBackward0>)\n",
      "D_i:  torch.Size([2]) tensor([0.3780, 0.0000])\n",
      "i = 1\n",
      "Gamma_i:  torch.Size([2]) tensor([0.2440, 0.2440])\n",
      "context S_i:  [' I', 'Every']\n",
      "A:  torch.Size([2]) tensor([0., 1.])\n",
      "target s_iplus1:   there  has\n",
      "loss:  torch.Size([]) tensor(7.1135, grad_fn=<DivBackward0>)\n",
      "D_i:  torch.Size([2]) tensor([0.3780, 0.0000])\n",
      "i = 2\n",
      "Gamma_i:  torch.Size([2]) tensor([0.6220, 0.2440])\n",
      "context S_i:  [' I', 'Every']\n",
      "A:  torch.Size([2]) tensor([0., 1.])\n",
      "target s_iplus1:  fore  a\n",
      "loss:  torch.Size([]) tensor(2.0648, grad_fn=<DivBackward0>)\n",
      "D_i:  torch.Size([2]) tensor([0.3780, 0.0000])\n",
      "i = 3\n",
      "Gamma_i:  torch.Size([2]) tensor([1.0000, 0.2440])\n",
      "context S_i:  [' I', 'Every']\n",
      "A:  torch.Size([2]) tensor([0., 1.])\n",
      "target s_iplus1:   I  silver\n",
      "loss:  torch.Size([]) tensor(3.2351, grad_fn=<DivBackward0>)\n",
      "D_i:  torch.Size([2]) tensor([0.3780, 0.0000])\n",
      "i = 4\n",
      "Gamma_i:  torch.Size([2]) tensor([1.0000, 0.2440])\n",
      "context S_i:  [' I', 'Every']\n",
      "A:  torch.Size([2]) tensor([0., 1.])\n",
      "target s_iplus1:   am  lining\n",
      "loss:  torch.Size([]) tensor(2.9113, grad_fn=<DivBackward0>)\n",
      "D_i:  torch.Size([2]) tensor([0.3780, 0.0000])\n",
      "i = 5\n",
      "Gamma_i:  torch.Size([2]) tensor([1.0000, 0.2440])\n",
      "context S_i:  [' I', 'Every']\n",
      "A:  torch.Size([2]) tensor([0., 1.])\n",
      "target s_iplus1:  <endoftext> <endoftext>\n",
      "loss:  torch.Size([]) tensor(3.1582, grad_fn=<DivBackward0>)\n",
      "D_i:  torch.Size([2]) tensor([0.3780, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "\n",
    "E_dict_invert = {g: h for h, g in E_dict.items()}\n",
    "\n",
    "E_dict_prime = E_dict\n",
    "E_dict_prime[''] = 12\n",
    "E_dict_prime_invert = {g: h for h, g in E_dict_prime.items()}\n",
    "\n",
    "for i in range(n-1):\n",
    "    \n",
    "    print(f\"i = {i}\")\n",
    "    \n",
    "    # initialize everything\n",
    "    if i==0:\n",
    "        E = create_embeddings(v, d)\n",
    "        Gamma_i = initialize_Gamma_and_D(i)\n",
    "        S_n, S_i = create_input_sequence(i, S_n_text)\n",
    "        X0 = create_first_resid(S_i, E)\n",
    "    else:\n",
    "        Gamma_i = iterate_Gamma_and_D(i, Gamma_i, D_i)\n",
    "        \n",
    "        # initialize S_iminus1 (not actually correct but simpler)\n",
    "        S_n, S_iminus1 = create_input_sequence(i-1, S_n_text)\n",
    "        # create s_i\n",
    "        s_i = s_iplus1_prime\n",
    "        \n",
    "        # create Y_token with s_i and E_prime\n",
    "        # create Y_concept by masking Xf with A_prime_expand\n",
    "        S_i = \n",
    "        X0 = X0_iplus1\n",
    "        \n",
    "    print(\"Gamma_i: \", Gamma_i.shape, Gamma_i)\n",
    "    \n",
    "    # need to have a way to iterate S_i and X0, but i think that goes down below\n",
    "    \n",
    "    readable_strings = tensor_to_string(S_i, E_dict_invert)\n",
    "    print(\"context S_i: \", readable_strings)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for l in range(layers):\n",
    "        \n",
    "        # multi-head attention, feedforward network, & residual connection\n",
    "        if l== 0: # first layer\n",
    "            Xi = mha(X0, h)\n",
    "            Xi = ffn(Xi)\n",
    "            Xi = resid_connection(X0, Xi)\n",
    "            \n",
    "        elif l == layers-1: # last layer\n",
    "            X_iplus1 = mha(Xi, h)\n",
    "            X_iplus1 = ffn(X_iplus1)\n",
    "            Xf = resid_connection(Xi, X_iplus1)\n",
    "            \n",
    "        else: # all layers in the middle\n",
    "            X_iplus1 = mha(Xi, h)\n",
    "            X_iplus1 = ffn(X_iplus1)\n",
    "            Xi = resid_connection(Xi, X_iplus1)\n",
    "         \n",
    "    Y, Z, P = gamma_model_output_layer(Xf, E)\n",
    "    \n",
    "    A, O, A_prime = creating_conditionals(P, gamma0)\n",
    "    print(\"A: \", A.shape, A)\n",
    "    \n",
    "    s_iplus1 = get_target_tokens(i, S_n)\n",
    "    \n",
    "    readable_strings = tensor_to_string(s_iplus1, E_dict_prime_invert)\n",
    "    print(\"theoretical target s_iplus1: \", readable_strings)\n",
    "    \n",
    "    loss = custom_loss(Z, s_iplus1, A_prime)\n",
    "    print(\"loss: \", loss.shape, loss)\n",
    "    \n",
    "    _, D_i = calc_delta_gamma(Cmax, n, A_prime)\n",
    "    print(\"D_i: \", D_i.shape, D_i)\n",
    "    \n",
    "    s_iplus1_prime = s_iplus1 * \n",
    "    print(\"s_iplus1_prime: \", s_iplus1_prime.shape, s_iplus1_prime)\n",
    "\n",
    "    X0_iplus1 = make_next_input_for_model(d, E, s_iplus1_prime, A_prime, Y, X0)\n",
    "    print(\"X0_iplus1: \", X0_iplus1.shape, X0_iplus1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5b0793",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
