{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e31e531-c8f4-4641-8a87-90d7c1dd933a",
   "metadata": {},
   "source": [
    "# v11.3\n",
    "\n",
    "primary todo's:\n",
    "- [ ] copy & paste stuff that doesn't need to be changed\n",
    "- [ ] copy, paste, & remove references to matryoshka embeddings for modules w/ em\n",
    "- [ ] bring over todo items that haven't been completed from v11.2 and from notes app\n",
    "- [ ] setup single attention mechanism module for both self & cross? i feel like separate is prolly still easier\n",
    "- [ ] setup single decoder layer body\n",
    "- [ ] mess with model.forward() to work with new setup\n",
    "- [ ] mess with model.inference() to work with new setup\n",
    "\n",
    "other todo's:\n",
    "- [ ] setup concept loss to use MSE and COS simultaneously\n",
    "- [ ] get predictive attention mask working in crossMQA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a84226-3000-41f2-b05c-cdbc41396595",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16628521-543c-4de8-a5aa-d830e7215bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my virtual environments are rarely properly connected to jupyter so this fixes that\n",
    "import sys\n",
    "import os\n",
    "current_dir = os.getcwd()  # Get the current working directory\n",
    "venv_dir = os.path.join(current_dir, 'venv') \n",
    "python_version = str(sys.version_info.major) + '.' + str(sys.version_info.minor)\n",
    "site_packages_path = os.path.join(venv_dir, 'lib', 'python' + python_version, 'site-packages')\n",
    "sys.path.append(site_packages_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8381842d-1256-4d00-a8a1-dd6e4fdc6192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# imports for the debugging/demonstration setup\n",
    "import functools\n",
    "import inspect\n",
    "\n",
    "# imports for the tokenizer\n",
    "from tokenizer import SimpleTokenizer, loaded_stoi, loaded_merges\n",
    "\n",
    "# Imports used for the config\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "# Imports used for the model\n",
    "import re\n",
    "from typing import Any, List, Sequence, Tuple, Union\n",
    "\n",
    "# used for training\n",
    "import random\n",
    "import time\n",
    "\n",
    "# used to save & load models\n",
    "import json\n",
    "from dataclasses import asdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdeda5ff-46cc-49f5-b496-6d316bda3835",
   "metadata": {},
   "source": [
    "# Demonstration/Debugging wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02b4aea8-31e9-4fa9-8b62-187a042b33d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will be used throughout for debugging/demonstration purposes\n",
    "# using this is way cleaner than cluttering up our code with print statements\n",
    "def log_io(func):\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(self, *args, **kwargs):\n",
    "        # Check if logging is enabled globally and for the specific function\n",
    "        if not self.logging_enabled or func.__name__ in self.disabled_logging_functions:\n",
    "            return func(self, *args, **kwargs)\n",
    "        #if not self.logging_enabled:\n",
    "            #return func(self, *args, **kwargs)\n",
    "\n",
    "        def log_item(item, name, level=0, is_root=False):\n",
    "            indent = \"    \" * level\n",
    "            if isinstance(item, torch.Tensor):\n",
    "                print(f\"{indent}Tensor '{name}' shape: {item.shape}\")\n",
    "            elif isinstance(item, tuple):\n",
    "                if is_root and level == 0:\n",
    "                    # Root level tuple, don't print it as a tuple unless it's a \"true\" tuple\n",
    "                    for idx, sub_item in enumerate(item):\n",
    "                        log_item(sub_item, f\"{name}[{idx}]\", level)\n",
    "                else:\n",
    "                    print(f\"{indent}Tuple '{name}':\")\n",
    "                    for idx, sub_item in enumerate(item):\n",
    "                        log_item(sub_item, f\"{name}[{idx}]\", level + 1)\n",
    "            elif isinstance(item, int):\n",
    "                print(f\"{indent}Integer '{name}': Value={item}\")\n",
    "            elif isinstance(item, float):\n",
    "                print(f\"{indent}Float '{name}': Value={item}\")\n",
    "            else:\n",
    "                print(f\"{indent}Other-type '{name}': Type={type(item).__name__}, Value={item}\")\n",
    "\n",
    "        print(f\"\\n{'='*10}Entering {self.__class__.__name__}.{func.__name__}{'='*10}\")\n",
    "        print(\"Inputs:\")\n",
    "        arg_names = inspect.getfullargspec(func).args[1:]  # Excluding 'self'\n",
    "        arg_values = args + tuple(kwargs.values())\n",
    "        for name, value in zip(arg_names, arg_values):\n",
    "            log_item(value, name)\n",
    "\n",
    "        result = func(self, *args, **kwargs)\n",
    "        print(\"\\nOutputs:\")\n",
    "        if isinstance(result, tuple):\n",
    "            log_item(result, \"output\", is_root=True)\n",
    "        else:\n",
    "            log_item(result, \"output\")\n",
    "\n",
    "        print(f\"{'='*10}Exiting {self.__class__.__name__}.{func.__name__}{'='*10}\")\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3214e4-7a9b-4e82-a5d3-3fea93f378e2",
   "metadata": {},
   "source": [
    "# Config & tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cff0f146-fcdc-4ef6-bc0b-c79e0c7c98c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# and the tokenizer\n",
    "tokenizer = SimpleTokenizer(loaded_stoi, loaded_merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b307756e-23ca-4869-adc5-224128aede9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(vocab_size=128, max_seq_len=256, num_layers=6, sa_q_heads=2, sa_kv_heads=1, attn_bias=False, embed_dim=256, mlp_multiplier=4, sa_head_dim=64, theta=100.0, dropout_rate=0.05, norm_affine=True, levels=3, combo=4, concept_loss='cos', level_loss_weight=1.0, ca_q_heads=2, ca_kv_heads=1, ca_head_dim=64, ca_use_RoPE=False, predictive_mask=True)\n",
      "sequence length of each model: [256, 64, 16]\n",
      "loss discounts starting from lowest level: [1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "@dataclass # a class meant specifically to just hold data\n",
    "class Config:\n",
    "    \"\"\" \n",
    "    The default configuration & hyperparameters for my next-concept predictor\n",
    "    \"\"\"\n",
    "    ### boring hyperparameters\n",
    "    vocab_size: int = tokenizer.vocab_len\n",
    "    max_seq_len: int = 256\n",
    "    num_layers: int = 6\n",
    "    sa_q_heads: int = 2\n",
    "    sa_kv_heads: int = 1\n",
    "    attn_bias: bool = False\n",
    "    embed_dim: int = 256\n",
    "    mlp_multiplier: int = 4\n",
    "    sa_head_dim: int = 64\n",
    "    theta: float = 100.0\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    dropout_rate: float = 0.05\n",
    "    eps = 1e-6\n",
    "    norm_affine: bool = True # whether norms should have a linear & bias after them\n",
    "    norm_type = \"RMSNorm\"  # Options are RMSNorm, CosineNorm and LayerNorm\n",
    "\n",
    "    ### Concept embedding vectors\n",
    "    levels: int = 3\n",
    "    combo: int = 4 # how many lower-level tokens/concepts to combine into the next level's concept\n",
    "    concept_loss: str = \"cos\" # options are 'mae', 'mse', and 'cos'(default)\n",
    "    @property\n",
    "    def seq_len_list(self):\n",
    "        return [(self.max_seq_len // (self.combo ** (i-1))) for i in range(1, self.levels + 1)]\n",
    "    level_loss_weight: float = 1.0 # how much to discount each higher level in the loss function compared to the last\n",
    "\n",
    "    ### Dualcoder cross-attention\n",
    "    # how many times to do the cross-attention\n",
    "    ca_q_heads: int = sa_q_heads\n",
    "    ca_kv_heads: int = sa_kv_heads\n",
    "    ca_head_dim: int = sa_head_dim\n",
    "    ca_use_RoPE: bool = False # True: expands out k & v tensors to be usable with rope. False: leaves k & v same size but no positional encodings\n",
    "    predictive_mask: bool = True # True: upper-triangular predictive mask to focus model's attention. False: no mask like a regular encoder\n",
    "\n",
    "    ### assertions\n",
    "    assert sa_q_heads % sa_kv_heads == 0, 'the number of query heads must be divisible by the number of key-value heads in self-attention'\n",
    "    assert ca_q_heads % ca_kv_heads == 0, 'the number of query heads must be divisible by the number of key-value heads in cross-attention'     \n",
    "        \n",
    "config = Config()\n",
    "print(config)\n",
    "print(f\"sequence length of each model: {config.seq_len_list}\")\n",
    "print(f\"loss discounts starting from lowest level: {[config.level_loss_weight**i for i in range(config.levels)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3f0865-495e-4c43-bff3-e87756818725",
   "metadata": {},
   "source": [
    "# RoPE\n",
    "\n",
    "i like the idea of pre-computing RoPE embeddings but at this point i don't think it's worth the effort bc i'd have to not only use this code but also pipe the two instantiations of this class through from `Body` all the way to `selfMQA` and `crossMQA` and I'm not even sure if it matters. I really should learn more about RoPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14adecb-9c7c-464e-8b40-d3d22ff93cc0",
   "metadata": {},
   "source": [
    "```Python\n",
    "# this class has not been implemented nor even tested. on my todo list\n",
    "class RoPE(nn.Module):\n",
    "    def __init__(self, \n",
    "                 dim: int, \n",
    "                 max_seq_len:int = config.max_seq_len, \n",
    "                 device: str = config.device):\n",
    "        super().__init__()\n",
    "        # Validate that dim is even since we split it by 2 for real and imaginary parts\n",
    "        if dim % 2 != 0: raise ValueError(\"Dimension 'dim' must be an even number.\")\n",
    "            \n",
    "        # Precompute frequencies based on configuration\n",
    "        theta = config.theta if hasattr(config, 'theta') else 10000.0\n",
    "        \n",
    "        freqs = 1.0 / (theta ** (torch.arange(0, config.dim, 2, device=config.device).float() / config.dim))\n",
    "        t = torch.arange(config.max_seq_len, device=config.device)\n",
    "        freqs = torch.outer(t, freqs).to(config.device).float()\n",
    "        \n",
    "        # Register as buffer to prevent gradient tracking\n",
    "        self.register_buffer('freqs_cis', torch.polar(torch.ones_like(freqs), freqs)) # complex64\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply rotary embeddings to the input tensor\n",
    "        x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "        x_out = torch.view_as_real(x_ * self.freqs_cis.unsqueeze(0)).type_as(x)\n",
    "        x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "        x_out = x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "        return x_out\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac722ba-d1f5-4466-ad22-9148f614123f",
   "metadata": {},
   "source": [
    "```Python\n",
    "# demonstration/debugging\n",
    "module = RoPE(dim=10)\n",
    "module.enable_logging()\n",
    "output = module(torch.randn(, 5))\n",
    "del module, output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "374a3ab5-9cba-4dd3-b5e9-5d0e4d419952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RoPE(x: torch.Tensor, dim: int, theta: float = 10000.0) -> torch.Tensor:\n",
    "    \"\"\"Applies the rotary embedding to the inputted query or key tensor\"\"\"\n",
    "    # Validate that dim is even since we split it by 2 for real and imaginary parts\n",
    "    if dim % 2 != 0: raise ValueError(\"Dimension 'dim' must be an even number.\")\n",
    "            \n",
    "    # Get sequence length\n",
    "    seq_len = x.size(1)\n",
    "    device = x.device\n",
    "\n",
    "    # Dynamically compute frequency cis based on the input sequence length\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2, device=device).float() / dim))\n",
    "    t = torch.arange(seq_len, device=device)\n",
    "    freqs = torch.outer(t, freqs).float()\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    # it's important to train on a wide variety of sequence lengths within your context length so that the model learns to generalize\n",
    "\n",
    "    # Apply rotary embeddings to the input tensor\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    x_out = x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "    return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851a4b6f-2dc7-4c59-b71f-0e6740101acb",
   "metadata": {},
   "source": [
    "# selfMQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e735756c-9806-4000-84fd-7008bfa0d251",
   "metadata": {},
   "outputs": [],
   "source": [
    "class selfMQA(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.sa_q_heads = config.sa_q_heads\n",
    "        self.sa_kv_heads = config.sa_kv_heads\n",
    "        assert self.sa_q_heads % self.sa_kv_heads == 0\n",
    "        self.num_queries_per_kv = self.sa_q_heads // self.sa_kv_heads\n",
    "\n",
    "        self.embed_dim = config.embed_dim\n",
    "        self.sa_head_dim = config.sa_head_dim\n",
    "        self.theta = config.theta\n",
    "        self.dropout_rate = config.dropout_rate\n",
    "\n",
    "        #self.Wqkv = nn.Linear(self.embed_dim, \n",
    "        #                      (self.sa_q_heads + 2 * self.sq_kv_heads) * self.sa_head_dim, \n",
    "        #                      bias = config.attn_bias)\n",
    "        #self.Wo = nn.Linear(self.sa_q_heads * self.sa_head_dim,\n",
    "        #                    self.embed_dim,\n",
    "        #                    bias = config.attn_bias)\n",
    "        \n",
    "        self.Wqkv = nn.Parameter(torch.Tensor(self.embed_dim, (self.sa_q_heads + 2 * self.sa_kv_heads) * self.sa_head_dim))\n",
    "        nn.init.uniform_(self.Wqkv, -((1 / self.embed_dim) ** 0.5), (1 / self.embed_dim) ** 0.5)\n",
    "\n",
    "        self.Wo = nn.Parameter(torch.Tensor(self.sa_q_heads * self.sa_head_dim, self.embed_dim))\n",
    "        nn.init.uniform_(self.Wo, -((1 / (self.sa_q_heads * self.sa_head_dim)) ** 0.5), (1 / (self.sa_q_heads * self.sa_head_dim)) ** 0.5)\n",
    "\n",
    "        # for our attention mask we'll create a boolean mask that'll later be turned into large negative values\n",
    "        self.mask = torch.tril(torch.ones((config.max_seq_len, config.max_seq_len), dtype=torch.uint8)\n",
    "                              ).view(1, 1, config.max_seq_len, config.max_seq_len).to(dtype=torch.bool)\n",
    "        \n",
    "        self.logging_enabled = False\n",
    "        self.disabled_logging_functions = set()\n",
    "    def enable_logging(self):\n",
    "        self.logging_enabled = True\n",
    "    def disable_logging(self):\n",
    "        self.logging_enabled = False\n",
    "    def disable_function_logging(self, func_name):\n",
    "        self.disabled_logging_functions.add(func_name)\n",
    "    def enable_function_logging(self, func_name):\n",
    "        self.disabled_logging_functions.discard(func_name)\n",
    "\n",
    "    @log_io\n",
    "    def forward(self, x: torch.Tensor, training: bool = False) -> torch.Tensor:\n",
    "        # Extracts batch size and input sequence length from the hidden states tensor.\n",
    "        batch_size, input_len, _ = x.shape\n",
    "\n",
    "        # splicing our primary projection to get the correct sub-matrices\n",
    "        Wq, Wk, Wv = self.weight_splicing(self.Wqkv)\n",
    "        # technically self.weight_splicing has access to self.Wqkv & Wo but this way our debugger can see them\n",
    "\n",
    "        # Applies the linear projection to the hidden state to retrieve our q, k & v projections\n",
    "        xq = F.dropout(x @ Wq, p=self.dropout_rate, training=training) # also dropout if we're training\n",
    "        xk = F.dropout(x @ Wk, p=self.dropout_rate, training=training)\n",
    "        xv = F.dropout(x @ Wv, p=self.dropout_rate, training=training)\n",
    "\n",
    "        # Reshapes each of the Q, K, and V tensors to separate the heads and align the dimensions for attention operations.\n",
    "        xq = xq.view(batch_size, -1, self.sa_q_heads, self.sa_head_dim)\n",
    "        xk = xk.view(batch_size, -1, self.sa_kv_heads, self.sa_head_dim)\n",
    "        xv = xv.view(batch_size, -1, self.sa_kv_heads, self.sa_head_dim)\n",
    "\n",
    "        # Applies rotary positional embeddings to queries and keys to incorporate positional information.\n",
    "        xq, xk = self.RoPE(xq, xk)\n",
    "\n",
    "        # If the number of KV heads is different from the number of query heads, adjusts keys and values to match the query heads count.\n",
    "        if self.sa_kv_heads != self.sa_q_heads:\n",
    "            xk, xv = self.match_headcount(xk, xv) # [batch_size, input_len, n_local_heads, sa_head_dim]\n",
    "\n",
    "        # Transposes Q, K, and V tensors to align them for the batch matrix multiplication in attention calculation.\n",
    "        xq = xq.transpose(1, 2) # [batch_size, n_local_heads, input_len, sa_head_dim]\n",
    "        xk = xk.transpose(1, 2)\n",
    "        xv = xv.transpose(1, 2)\n",
    "\n",
    "        # Calculates attention scores by performing a batch matrix multiplication between queries and keys, followed by scaling.\n",
    "        logits = self.attend(xq, xk) # [batch_size, n_local_heads, input_len, input_len]\n",
    "        \n",
    "        # Applies the lower-triangular mask to the attention logits\n",
    "        logits = self.apply_mask(logits, input_len)\n",
    "\n",
    "        # applies values to get final output\n",
    "        output = self.calc_output(logits, xv, batch_size, input_len, training) \n",
    "\n",
    "        # Applies the final linear projection to the attention output, mapping it back to self.embed_dim\n",
    "        return F.dropout(output @ self.Wo, p=self.dropout_rate, training=training) # also dropout if we're training\n",
    "\n",
    "    @log_io\n",
    "    def weight_splicing(self, Wqkv):\n",
    "        Wq, Wk, Wv = Wqkv.split([self.sa_q_heads * self.sa_head_dim,\n",
    "                                 self.sa_kv_heads * self.sa_head_dim,\n",
    "                                 self.sa_kv_heads * self.sa_head_dim],dim = -1)\n",
    "        return Wq, Wk, Wv\n",
    "\n",
    "    @log_io\n",
    "    def RoPE(self, xq, xk):\n",
    "        xq = RoPE(xq, self.sa_head_dim, self.theta)\n",
    "        xk = RoPE(xk, self.sa_head_dim, self.theta)\n",
    "        return xq, xk\n",
    "\n",
    "    @log_io\n",
    "    def match_headcount(self, xk, xv):\n",
    "        xk = torch.repeat_interleave(xk, self.num_queries_per_kv, dim=2)\n",
    "        xv = torch.repeat_interleave(xv, self.num_queries_per_kv, dim=2)\n",
    "        return xk, xv\n",
    "\n",
    "    @log_io\n",
    "    def attend(self, xq, xk):\n",
    "        return torch.matmul(xq, xk.transpose(2, 3)) * (self.sa_head_dim ** -0.5)\n",
    "        \n",
    "    @log_io\n",
    "    def apply_mask(self, logits, input_len):\n",
    "        return torch.where(self.mask[..., :input_len, :input_len].expand_as(logits),\n",
    "                           logits,\n",
    "                           torch.tensor(-1e30, device=logits.device, dtype=logits.dtype))\n",
    "    \n",
    "    @log_io\n",
    "    def calc_output(self, logits, xv, batch_size, input_len, training):\n",
    "        # Applies softmax to the logits to obtain attention probabilities\n",
    "        scores = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # also applies dropout if we're training\n",
    "        scores = F.dropout(scores, p=self.dropout_rate, training=training)\n",
    "        \n",
    "        # Computes the weighted sum of values based on the attention scores to obtain the output of the attention mechanism.\n",
    "        output = scores @ xv # [batch_size, n_local_heads, input_len, sa_head_dim]\n",
    "\n",
    "        # Reshapes the attention output to match the expected output dimensions, combining the heads back into the hidden dimension.\n",
    "        return output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5e3b57-78fc-4243-81c4-4cbb898f9e27",
   "metadata": {},
   "source": [
    "### demonstration/debugging\n",
    "I've setup these little snippets after each nn.Module to help you see what's happening and for my own debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bf8af24f-0fab-4457-8aca-26f1ecc479b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering selfMQA.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 256, 256])\n",
      "\n",
      "==========Entering selfMQA.weight_splicing==========\n",
      "Inputs:\n",
      "Tensor 'Wqkv' shape: torch.Size([256, 256])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([256, 128])\n",
      "Tensor 'output[1]' shape: torch.Size([256, 64])\n",
      "Tensor 'output[2]' shape: torch.Size([256, 64])\n",
      "==========Exiting selfMQA.weight_splicing==========\n",
      "\n",
      "==========Entering selfMQA.RoPE==========\n",
      "Inputs:\n",
      "Tensor 'xq' shape: torch.Size([32, 256, 2, 64])\n",
      "Tensor 'xk' shape: torch.Size([32, 256, 1, 64])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([32, 256, 2, 64])\n",
      "Tensor 'output[1]' shape: torch.Size([32, 256, 1, 64])\n",
      "==========Exiting selfMQA.RoPE==========\n",
      "\n",
      "==========Entering selfMQA.match_headcount==========\n",
      "Inputs:\n",
      "Tensor 'xk' shape: torch.Size([32, 256, 1, 64])\n",
      "Tensor 'xv' shape: torch.Size([32, 256, 1, 64])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([32, 256, 2, 64])\n",
      "Tensor 'output[1]' shape: torch.Size([32, 256, 2, 64])\n",
      "==========Exiting selfMQA.match_headcount==========\n",
      "\n",
      "==========Entering selfMQA.attend==========\n",
      "Inputs:\n",
      "Tensor 'xq' shape: torch.Size([32, 2, 256, 64])\n",
      "Tensor 'xk' shape: torch.Size([32, 2, 256, 64])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 2, 256, 256])\n",
      "==========Exiting selfMQA.attend==========\n",
      "\n",
      "==========Entering selfMQA.apply_mask==========\n",
      "Inputs:\n",
      "Tensor 'logits' shape: torch.Size([32, 2, 256, 256])\n",
      "Integer 'input_len': Value=256\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 2, 256, 256])\n",
      "==========Exiting selfMQA.apply_mask==========\n",
      "\n",
      "==========Entering selfMQA.calc_output==========\n",
      "Inputs:\n",
      "Tensor 'logits' shape: torch.Size([32, 2, 256, 256])\n",
      "Tensor 'xv' shape: torch.Size([32, 2, 256, 64])\n",
      "Integer 'batch_size': Value=32\n",
      "Integer 'input_len': Value=256\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 256, 128])\n",
      "==========Exiting selfMQA.calc_output==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 256, 256])\n",
      "==========Exiting selfMQA.forward==========\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of selfMQA\n",
    "module = selfMQA(config)\n",
    "\n",
    "# Initially, logging is disabled\n",
    "# Enable logging\n",
    "module.enable_logging()\n",
    "\n",
    "### Optionally disabling printing for sub-functions\n",
    "#module.disable_function_logging('weight_splicing')\n",
    "#module.disable_function_logging('RoPE')\n",
    "#module.disable_function_logging('match_headcount')\n",
    "#module.disable_function_logging('attend')\n",
    "#module.disable_function_logging('apply_mask')\n",
    "#module.disable_function_logging('calc_output')\n",
    "\n",
    "# Call the forward method - logging will occur\n",
    "output = module(torch.randn(32,config.max_seq_len,config.embed_dim))#, training=True)\n",
    "\n",
    "# Disable logging. \n",
    "# This isn't actually necessary since we won't be using this object again but that's how you'd do it\n",
    "module.disable_logging()\n",
    "\n",
    "# clearing up ram jic we're training later\n",
    "del module, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8dc236-465c-40a4-9430-5bcbe10789da",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6fc6e8d9-3d0a-44d7-9389-b64414d086eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_dim: int,\n",
    "                 mlp_multiplier: int,\n",
    "                 dropout_rate: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.mlp_multiplier = mlp_multiplier\n",
    "        self.hidden_size = embed_dim * mlp_multiplier\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # the gate, up and down projections\n",
    "        self.gate_proj = nn.Linear(embed_dim, self.hidden_size)\n",
    "        self.up_proj = nn.Linear(embed_dim, self.hidden_size)\n",
    "        self.down_proj = nn.Linear(self.hidden_size, embed_dim)\n",
    "        \n",
    "        self.logging_enabled = False\n",
    "        self.disabled_logging_functions = set()\n",
    "    def enable_logging(self):\n",
    "        self.logging_enabled = True\n",
    "    def disable_logging(self):\n",
    "        self.logging_enabled = False\n",
    "    def disable_function_logging(self, func_name):\n",
    "        self.disabled_logging_functions.add(func_name)\n",
    "    def enable_function_logging(self, func_name):\n",
    "        self.disabled_logging_functions.discard(func_name)\n",
    "        \n",
    "    @log_io\n",
    "    def forward(self, x: torch.Tensor, training: bool = False ) -> torch.Tensor:\n",
    "        output = self.down_proj(F.gelu(self.gate_proj(x)) * self.up_proj(x))\n",
    "        return F.dropout(output, p=self.dropout_rate, training=training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c357e34-cf79-4363-a97f-d96b8a56e837",
   "metadata": {},
   "source": [
    "### demonstration/debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "410acc17-013b-4a26-bc59-f5780cef9073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering MLP.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 256, 256])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 256, 256])\n",
      "==========Exiting MLP.forward==========\n"
     ]
    }
   ],
   "source": [
    "module = MLP(config.embed_dim, config.mlp_multiplier, config.dropout_rate)\n",
    "module.enable_logging()\n",
    "output = module(torch.randn(32,config.max_seq_len,config.embed_dim))\n",
    "del module, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9be355-a811-41de-a761-822cd1cf5ad8",
   "metadata": {},
   "source": [
    "# Norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "548c6663-41ce-4715-b138-30fe7dfebd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Norm(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.eps = config.eps\n",
    "        self.affine = config.norm_affine\n",
    "        self.dropout_rate = config.dropout_rate\n",
    "        self.type = config.norm_type\n",
    "\n",
    "        # Initialize weight and bias parameters for affine transformation\n",
    "        # We start with ones for weight to keep the original scale initially, and zeros for bias.\n",
    "        self.w = nn.Parameter(torch.ones(config.embed_dim))\n",
    "        self.b = nn.Parameter(torch.zeros(config.embed_dim))\n",
    "        \n",
    "        self.logging_enabled = False\n",
    "        self.disabled_logging_functions = set()\n",
    "    def enable_logging(self):\n",
    "        self.logging_enabled = True\n",
    "    def disable_logging(self):\n",
    "        self.logging_enabled = False\n",
    "    def disable_function_logging(self, func_name):\n",
    "        self.disabled_logging_functions.add(func_name)\n",
    "    def enable_function_logging(self, func_name):\n",
    "        self.disabled_logging_functions.discard(func_name)\n",
    "\n",
    "    @log_io\n",
    "    def forward(self, x: torch.Tensor, training: bool = False) -> torch.Tensor:\n",
    "        # Normalize the input tensor\n",
    "        if self.type == \"CosineNorm\":\n",
    "            x = self.CosineNorm(x)\n",
    "        elif self.type == \"LayerNorm\":\n",
    "            x = self.LayerNorm(x)\n",
    "        else: # defaults to RMSNorm bc that's the most commonly used nowadays\n",
    "            x = self.RMSNorm(x)\n",
    "\n",
    "        if self.affine: # Optionally apply the affine transformation\n",
    "            x = x * self.w + self.b\n",
    "            \n",
    "        return F.dropout(x, p=self.dropout_rate, training=training) # and dropout if we're training\n",
    "\n",
    "    @log_io\n",
    "    def CosineNorm(self, x):\n",
    "        # normalize x by dividing by its L2 norm along the last dimension.\n",
    "        # this places x on the unit hypersphere centered at the origin\n",
    "        # Add a small constant to the denominator to avoid division by zero.\n",
    "        return x / torch.norm(x, p=2, dim=-1, keepdim=True).clamp(min=self.eps)\n",
    "\n",
    "    @log_io\n",
    "    def LayerNorm(self, x): # nn.LayerNorm() exists but might as well make it from scratch if we have to do the other two\n",
    "        # normalize x by subtracting by its mean then dividing by its variance\n",
    "        # this places x on a hypersphere of radius sqrt(dimension) centered at the origin\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        return (x - mean) / torch.sqrt(var + self.eps)\n",
    "\n",
    "    @log_io\n",
    "    def RMSNorm(self, x):\n",
    "        # normalize x by dividing by its root-mean-square along the last dimension\n",
    "        # this places x on a hypersphere of radius sqrt(dimension) with no certain center\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5e1ddc-f4a0-42ea-b2a0-629e0115848f",
   "metadata": {},
   "source": [
    "### demonstration/debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6ca306ab-41fb-497c-b1a1-4154c141aa22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering Norm.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 256, 256])\n",
      "\n",
      "==========Entering Norm.RMSNorm==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 256, 256])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 256, 256])\n",
      "==========Exiting Norm.RMSNorm==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 256, 256])\n",
      "==========Exiting Norm.forward==========\n"
     ]
    }
   ],
   "source": [
    "module = Norm(config)\n",
    "module.enable_logging()\n",
    "\n",
    "### disabling printing for sub-functions\n",
    "#module.disable_function_logging('RMSNorm')\n",
    "#module.disable_function_logging('LayerNorm')\n",
    "#module.disable_function_logging('CosineNorm')\n",
    "\n",
    "output = module(torch.randn(32, config.max_seq_len, config.embed_dim))\n",
    "del module, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f333b374-d14d-4d5f-8913-c36c47a87df7",
   "metadata": {},
   "source": [
    "# crossMQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c3606776-86d8-4c94-8573-59babb19de59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class crossMQA(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ca_q_heads = config.ca_q_heads\n",
    "        self.ca_kv_heads = config.ca_kv_heads\n",
    "        assert self.ca_q_heads % self.ca_kv_heads == 0\n",
    "        self.num_queries_per_kv = self.ca_q_heads // self.ca_kv_heads\n",
    "        self.embed_dim = config.embed_dim\n",
    "        self.ca_head_dim = config.ca_head_dim\n",
    "        self.sa_head_dim = config.sa_head_dim # used only for an assertion to make sure sizes will fit\n",
    "        self.combo = config.combo # used only for an assertion to make sure sizes will fit\n",
    "        self.theta = config.theta\n",
    "        self.use_RoPE = config.ca_use_RoPE\n",
    "        self.dropout_rate = config.dropout_rate\n",
    "        self.predictive_mask = config.predictive_mask\n",
    "\n",
    "        self.Wqkv = nn.Parameter(torch.Tensor(self.embed_dim, (self.ca_q_heads + 2 * self.ca_kv_heads) * self.ca_head_dim))\n",
    "        nn.init.uniform_(self.Wqkv, -((1 / self.embed_dim) ** 0.5), (1 / self.embed_dim) ** 0.5)\n",
    "        \n",
    "        self.Wo = nn.Parameter(torch.Tensor(self.ca_q_heads * self.ca_head_dim, self.embed_dim))\n",
    "        nn.init.uniform_(self.Wo, -((1 / (self.ca_q_heads * self.ca_head_dim)) ** 0.5), (1 / (self.ca_q_heads * self.ca_head_dim)) ** 0.5)\n",
    "        \n",
    "        self.logging_enabled = False\n",
    "        self.disabled_logging_functions = set()\n",
    "    def enable_logging(self):\n",
    "        self.logging_enabled = True\n",
    "    def disable_logging(self):\n",
    "        self.logging_enabled = False\n",
    "    def disable_function_logging(self, func_name):\n",
    "        self.disabled_logging_functions.add(func_name)\n",
    "    def enable_function_logging(self, func_name):\n",
    "        self.disabled_logging_functions.discard(func_name)\n",
    "\n",
    "    @log_io\n",
    "    def forward(self, \n",
    "                x: torch.Tensor, # the current level tensor, sometimes a resid state full of tokens & sometimes concepts\n",
    "                c: torch.Tensor, # the upper level tensor, always a resid state full of concept vecs\n",
    "                training: bool = False\n",
    "               ) -> torch.Tensor:\n",
    "        \n",
    "        # Extracts batch size and input sequence length from the hidden states tensor.\n",
    "        batch_size, input_len_x, _ = x.shape\n",
    "        batch_size_c, input_len_c, _ = c.shape\n",
    "        assert batch_size == batch_size_c\n",
    "\n",
    "        # splicing our projection to get the correct sub-matrices\n",
    "        Wq, Wk, Wv = self.weight_splicing(self.Wqkv)\n",
    "\n",
    "        # Applies the linear projection to the hidden state to retrieve our q, k & v projections\n",
    "        xq = F.dropout(x @ Wq, p=self.dropout_rate, training=training) # also applies dropout if we're training\n",
    "        ck = F.dropout(c @ Wk, p=self.dropout_rate, training=training)\n",
    "        cv = F.dropout(c @ Wv, p=self.dropout_rate, training=training)\n",
    "\n",
    "        # Reshapes each of the Q, K, and V tensors to separate the heads and align the dimensions for attention operations.\n",
    "        xq = xq.view(batch_size, -1, self.ca_q_heads, self.ca_head_dim)\n",
    "        ck = ck.view(batch_size, -1, self.ca_kv_heads, self.ca_head_dim)\n",
    "        cv = cv.view(batch_size, -1, self.ca_kv_heads, self.ca_head_dim)\n",
    "\n",
    "        # IF we want to use RoPE (doesn't fully make sense to)\n",
    "        if self.use_RoPE:\n",
    "            expand = input_len_x // input_len_c\n",
    "            ck = ck.repeat_interleave(expand, dim=1) \n",
    "            cv = cv.repeat_interleave(expand, dim=1) # values need to be expanded for their use later on if we do this\n",
    "\n",
    "            # Applies rotary positional embeddings to queries and keys to incorporate positional information.\n",
    "            xq, ck = self.RoPE(xq, ck)\n",
    "\n",
    "        # If the number of KV heads is different from the number of query heads, adjusts keys and values to match the query heads count.\n",
    "        if self.ca_kv_heads != self.ca_q_heads:\n",
    "            ck, cv = self.match_headcount(ck, cv) # [batch_size, input_len, n_local_heads, sa_head_dim]\n",
    "\n",
    "        # Transposes Q, K, and V tensors to align them for the batch matrix multiplication in attention calculation.\n",
    "        xq = xq.transpose(1, 2) # [batch_size, n_local_heads, input_len, ca_head_dim]\n",
    "        ck = ck.transpose(1, 2)\n",
    "        cv = cv.transpose(1, 2)\n",
    "\n",
    "        # Calculates attention scores by performing a batch matrix multiplication between queries and keys, followed by scaling.\n",
    "        logits = self.attend(xq, ck) # [batch_size, n_local_heads, input_len, input_len]\n",
    "        \n",
    "        # Optionally applies the upper-triangular mask to the attention logits\n",
    "        if self.predictive_mask:\n",
    "            logits = self.apply_mask(logits, input_len_x, input_len_c)\n",
    "\n",
    "        # applies values to get final output\n",
    "        output = self.calc_output(logits, cv, batch_size, input_len_x, training)\n",
    "\n",
    "        # Applies the final linear projection to the attention output, mapping it back to d_i. \n",
    "        return F.dropout(output @ self.Wo, p=self.dropout_rate, training=training) # and dropout if we're training\n",
    "\n",
    "    @log_io\n",
    "    def weight_splicing(self, Wqkv):\n",
    "        Wq, Wk, Wv = Wqkv.split([self.ca_q_heads * self.ca_head_dim,\n",
    "                                 self.ca_kv_heads * self.ca_head_dim,\n",
    "                                 self.ca_kv_heads * self.ca_head_dim],dim = -1)\n",
    "        return Wq, Wk, Wv\n",
    "        \n",
    "    @log_io\n",
    "    def RoPE(self, xq, xk):\n",
    "        xq = RoPE(xq, self.ca_head_dim, self.theta)\n",
    "        xk = RoPE(xk, self.ca_head_dim, self.theta)\n",
    "        return xq, xk\n",
    "\n",
    "    @log_io\n",
    "    def match_headcount(self, xk, xv):\n",
    "        xk = torch.repeat_interleave(xk, self.num_queries_per_kv, dim=2)\n",
    "        xv = torch.repeat_interleave(xv, self.num_queries_per_kv, dim=2)\n",
    "        return xk, xv\n",
    "\n",
    "    @log_io\n",
    "    def attend(self, xq, ck):\n",
    "        return torch.matmul(xq, ck.transpose(2, 3)) * (self.ca_head_dim ** -0.5)\n",
    "        \n",
    "    @log_io\n",
    "    def apply_mask(self, logits, input_len_x, input_len_c):\n",
    "        self.mask = torch.triu(torch.ones((config.max_seq_len, input_len_c), dtype=torch.uint8)\n",
    "                                  ).view(1, 1, config.max_seq_len, input_len_c).to(dtype=torch.bool)\n",
    "        \n",
    "        return torch.where(self.mask[..., :input_len_x, :input_len_c].expand_as(logits),\n",
    "                           logits,\n",
    "                           torch.tensor(-1e30, device=logits.device, dtype=logits.dtype))\n",
    "    \n",
    "    @log_io\n",
    "    def calc_output(self, logits, cv, batch_size, input_len_x, training):\n",
    "        # Applies softmax to the logits to obtain attention probabilities\n",
    "        scores = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # also applies dropout if we're training\n",
    "        scores = F.dropout(scores, p=self.dropout_rate, training=training)\n",
    "        \n",
    "        # Computes the weighted sum of values based on the attention scores to obtain the output of the attention mechanism.\n",
    "        output = scores @ cv # [batch_size, n_local_heads, input_len, sa_head_dim]\n",
    "\n",
    "        # Reshapes the attention output to match the expected output dimensions, combining the heads back into the hidden dimension.\n",
    "        return output.transpose(1, 2).contiguous().view(batch_size, input_len_x, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2731e8f-023c-49de-8de8-d2b42d2833c4",
   "metadata": {},
   "source": [
    "### demonstration/debugging\n",
    "\n",
    "at one point this one had been giving me trouble so here's multiple different config.levels setups for ya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1c5f9bf9-d02c-4d46-9070-217a4bbdb1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering crossMQA.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 256, 256])\n",
      "Tensor 'c' shape: torch.Size([32, 64, 256])\n",
      "\n",
      "==========Entering crossMQA.weight_splicing==========\n",
      "Inputs:\n",
      "Tensor 'Wqkv' shape: torch.Size([256, 256])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([256, 128])\n",
      "Tensor 'output[1]' shape: torch.Size([256, 64])\n",
      "Tensor 'output[2]' shape: torch.Size([256, 64])\n",
      "==========Exiting crossMQA.weight_splicing==========\n",
      "\n",
      "==========Entering crossMQA.match_headcount==========\n",
      "Inputs:\n",
      "Tensor 'xk' shape: torch.Size([32, 64, 1, 64])\n",
      "Tensor 'xv' shape: torch.Size([32, 64, 1, 64])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([32, 64, 2, 64])\n",
      "Tensor 'output[1]' shape: torch.Size([32, 64, 2, 64])\n",
      "==========Exiting crossMQA.match_headcount==========\n",
      "\n",
      "==========Entering crossMQA.attend==========\n",
      "Inputs:\n",
      "Tensor 'xq' shape: torch.Size([32, 2, 256, 64])\n",
      "Tensor 'ck' shape: torch.Size([32, 2, 64, 64])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 2, 256, 64])\n",
      "==========Exiting crossMQA.attend==========\n",
      "\n",
      "==========Entering crossMQA.calc_output==========\n",
      "Inputs:\n",
      "Tensor 'logits' shape: torch.Size([32, 2, 256, 64])\n",
      "Tensor 'cv' shape: torch.Size([32, 2, 64, 64])\n",
      "Integer 'batch_size': Value=32\n",
      "Integer 'input_len_x': Value=256\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 256, 128])\n",
      "==========Exiting crossMQA.calc_output==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 256, 256])\n",
      "==========Exiting crossMQA.forward==========\n"
     ]
    }
   ],
   "source": [
    "hold1 = config.levels\n",
    "config.levels = 2\n",
    "hold2 = config.predictive_mask\n",
    "config.predictive_mask = False\n",
    "module = crossMQA(config)\n",
    "module.enable_logging()\n",
    "\n",
    "### Optionally disabling printing for sub-functions\n",
    "#module.disable_function_logging('weight_splicing')\n",
    "#module.disable_function_logging('RoPE')\n",
    "#module.disable_function_logging('match_headcount')\n",
    "#module.disable_function_logging('attend')\n",
    "#module.disable_function_logging('apply_mask')\n",
    "#module.disable_function_logging('calc_output')\n",
    "\n",
    "x0 = torch.randn(32, config.max_seq_len, config.embed_dim)\n",
    "c1 = torch.randn(32, config.max_seq_len // config.combo, config.embed_dim)\n",
    "output = module(x0, c1)\n",
    "config.levels = hold1\n",
    "config.predictive_mask = hold2\n",
    "del hold1, hold2, module, x0, c1, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "129b2d43-eaf2-4c87-a186-61199e76d9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering crossMQA.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 64, 256])\n",
      "Tensor 'c' shape: torch.Size([32, 16, 256])\n",
      "\n",
      "==========Entering crossMQA.weight_splicing==========\n",
      "Inputs:\n",
      "Tensor 'Wqkv' shape: torch.Size([256, 256])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([256, 128])\n",
      "Tensor 'output[1]' shape: torch.Size([256, 64])\n",
      "Tensor 'output[2]' shape: torch.Size([256, 64])\n",
      "==========Exiting crossMQA.weight_splicing==========\n",
      "\n",
      "==========Entering crossMQA.match_headcount==========\n",
      "Inputs:\n",
      "Tensor 'xk' shape: torch.Size([32, 16, 1, 64])\n",
      "Tensor 'xv' shape: torch.Size([32, 16, 1, 64])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([32, 16, 2, 64])\n",
      "Tensor 'output[1]' shape: torch.Size([32, 16, 2, 64])\n",
      "==========Exiting crossMQA.match_headcount==========\n",
      "\n",
      "==========Entering crossMQA.attend==========\n",
      "Inputs:\n",
      "Tensor 'xq' shape: torch.Size([32, 2, 64, 64])\n",
      "Tensor 'ck' shape: torch.Size([32, 2, 16, 64])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 2, 64, 16])\n",
      "==========Exiting crossMQA.attend==========\n",
      "\n",
      "==========Entering crossMQA.calc_output==========\n",
      "Inputs:\n",
      "Tensor 'logits' shape: torch.Size([32, 2, 64, 16])\n",
      "Tensor 'cv' shape: torch.Size([32, 2, 16, 64])\n",
      "Integer 'batch_size': Value=32\n",
      "Integer 'input_len_x': Value=64\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 64, 128])\n",
      "==========Exiting crossMQA.calc_output==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 64, 256])\n",
      "==========Exiting crossMQA.forward==========\n"
     ]
    }
   ],
   "source": [
    "hold1 = config.levels\n",
    "config.levels = 3\n",
    "hold2 = config.predictive_mask\n",
    "config.predictive_mask = False\n",
    "module = crossMQA(config)\n",
    "module.enable_logging()\n",
    "\n",
    "### Optionally disabling printing for sub-functions\n",
    "#module.disable_function_logging('weight_splicing')\n",
    "#module.disable_function_logging('RoPE')\n",
    "#module.disable_function_logging('match_headcount')\n",
    "#module.disable_function_logging('attend')\n",
    "#module.disable_function_logging('apply_mask')\n",
    "#module.disable_function_logging('calc_output')\n",
    "\n",
    "c1 = torch.randn(32, config.max_seq_len // (config.combo**1), config.embed_dim)\n",
    "c2 = torch.randn(32, config.max_seq_len // (config.combo**2), config.embed_dim)\n",
    "output = module(c1, c2)\n",
    "config.levels = hold1\n",
    "config.predictive_mask = hold2\n",
    "del hold1, hold2, module, c1, c2, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5af750-9441-43c2-af37-728c7323ad34",
   "metadata": {},
   "source": [
    "### working on crossMQA predictive mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bf42671a-87a7-4c0d-97b8-de0e2744fe98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [0., 1., 1.],\n",
       "        [0., 1., 1.],\n",
       "        [0., 1., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Original tensor using torch.triu on a 3x3 tensor of ones\n",
    "original_tensor = torch.triu(torch.ones(3, 3))\n",
    "\n",
    "input_len = 8\n",
    "# Expand the tensor by duplicating each row twice more\n",
    "expanded_tensor = original_tensor.repeat_interleave(3, dim=0)\n",
    "\n",
    "out = expanded_tensor[:input_len,:]\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0a693737-aa3b-4bd1-aee1-0c115eff9092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering crossMQA.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 256, 256])\n",
      "Tensor 'c' shape: torch.Size([32, 64, 256])\n",
      "\n",
      "==========Entering crossMQA.weight_splicing==========\n",
      "Inputs:\n",
      "Tensor 'Wqkv' shape: torch.Size([256, 256])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([256, 128])\n",
      "Tensor 'output[1]' shape: torch.Size([256, 64])\n",
      "Tensor 'output[2]' shape: torch.Size([256, 64])\n",
      "==========Exiting crossMQA.weight_splicing==========\n",
      "\n",
      "==========Entering crossMQA.match_headcount==========\n",
      "Inputs:\n",
      "Tensor 'xn' shape: torch.Size([32, 64, 1, 64])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 64, 2, 64])\n",
      "==========Exiting crossMQA.match_headcount==========\n",
      "\n",
      "==========Entering crossMQA.match_headcount==========\n",
      "Inputs:\n",
      "Tensor 'xn' shape: torch.Size([32, 64, 1, 64])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 64, 2, 64])\n",
      "==========Exiting crossMQA.match_headcount==========\n",
      "\n",
      "==========Entering crossMQA.attend==========\n",
      "Inputs:\n",
      "Tensor 'xq' shape: torch.Size([32, 2, 256, 64])\n",
      "Tensor 'ck' shape: torch.Size([32, 2, 64, 64])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 2, 256, 64])\n",
      "==========Exiting crossMQA.attend==========\n",
      "\n",
      "==========Entering crossMQA.apply_mask==========\n",
      "Inputs:\n",
      "Tensor 'logits' shape: torch.Size([32, 2, 256, 64])\n",
      "Integer 'input_len': Value=256\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (64) must match the existing size (256) at non-singleton dimension 3.  Target sizes: [32, 2, 256, 64].  Tensor sizes: [1, 1, 256, 256]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m x0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m32\u001b[39m, config\u001b[38;5;241m.\u001b[39mmax_seq_len, config\u001b[38;5;241m.\u001b[39membed_dim)\n\u001b[1;32m     17\u001b[0m c1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m32\u001b[39m, config\u001b[38;5;241m.\u001b[39mmax_seq_len \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m config\u001b[38;5;241m.\u001b[39mcombo, config\u001b[38;5;241m.\u001b[39membed_dim)\n\u001b[0;32m---> 18\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m config\u001b[38;5;241m.\u001b[39mlevels \u001b[38;5;241m=\u001b[39m hold1\n\u001b[1;32m     20\u001b[0m config\u001b[38;5;241m.\u001b[39mpredictive_mask \u001b[38;5;241m=\u001b[39m hold2\n",
      "File \u001b[0;32m~/Documents/next-concept-predictor/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/next-concept-predictor/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 39\u001b[0m, in \u001b[0;36mlog_io.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(arg_names, arg_values):\n\u001b[1;32m     37\u001b[0m     log_item(value, name)\n\u001b[0;32m---> 39\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOutputs:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mtuple\u001b[39m):\n",
      "Cell \u001b[0;32mIn[54], line 90\u001b[0m, in \u001b[0;36mcrossMQA.forward\u001b[0;34m(self, x, c, training)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Optionally applies the upper-triangular mask to the attention logits\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 90\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_len_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# applies values to get final output\u001b[39;00m\n\u001b[1;32m     93\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalc_output(logits, cv, batch_size, input_len_x, training)\n",
      "Cell \u001b[0;32mIn[3], line 39\u001b[0m, in \u001b[0;36mlog_io.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(arg_names, arg_values):\n\u001b[1;32m     37\u001b[0m     log_item(value, name)\n\u001b[0;32m---> 39\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOutputs:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mtuple\u001b[39m):\n",
      "Cell \u001b[0;32mIn[54], line 121\u001b[0m, in \u001b[0;36mcrossMQA.apply_mask\u001b[0;34m(self, logits, input_len)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;129m@log_io\u001b[39m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_mask\u001b[39m(\u001b[38;5;28mself\u001b[39m, logits, input_len):\n\u001b[0;32m--> 121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mwhere(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43minput_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43minput_len\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_as\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    122\u001b[0m                        logits,\n\u001b[1;32m    123\u001b[0m                        torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1e30\u001b[39m, device\u001b[38;5;241m=\u001b[39mlogits\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mlogits\u001b[38;5;241m.\u001b[39mdtype))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (64) must match the existing size (256) at non-singleton dimension 3.  Target sizes: [32, 2, 256, 64].  Tensor sizes: [1, 1, 256, 256]"
     ]
    }
   ],
   "source": [
    "hold1 = config.levels\n",
    "config.levels = 2\n",
    "hold2 = config.predictive_mask\n",
    "config.predictive_mask = True\n",
    "module = crossMQA(config)\n",
    "module.enable_logging()\n",
    "\n",
    "### Optionally disabling printing for sub-functions\n",
    "#module.disable_function_logging('weight_splicing')\n",
    "#module.disable_function_logging('RoPE')\n",
    "#module.disable_function_logging('match_headcount')\n",
    "#module.disable_function_logging('attend')\n",
    "#module.disable_function_logging('apply_mask')\n",
    "#module.disable_function_logging('calc_output')\n",
    "\n",
    "x0 = torch.randn(32, config.max_seq_len, config.embed_dim)\n",
    "c1 = torch.randn(32, config.max_seq_len // config.combo, config.embed_dim)\n",
    "output = module(x0, c1)\n",
    "config.levels = hold1\n",
    "config.predictive_mask = hold2\n",
    "del hold1, hold2, module, x0, c1, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5719f276-53b7-4c66-b4da-04231f8d6c38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc8505c5-d859-4e48-bdd9-e88746a3f88f",
   "metadata": {},
   "source": [
    "# Layer\n",
    "\n",
    "we implement cross-attention inbetween self-attention and MLP like is done in Attention is All You Need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1ca50177-b795-4a4e-8535-c5f00e161206",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pre_self_mqa_norm = Norm(config)\n",
    "        self.self_mqa = selfMQA(config)\n",
    "        self.post_self_mqa_norm = Norm(config)\n",
    "        \n",
    "        self.pre_cross_mqa_x_norm = Norm(config)\n",
    "        self.pre_cross_mqa_c_norm = Norm(config)\n",
    "        self.cross_mqa = crossMQA(config)\n",
    "        self.post_cross_mqa_norm = Norm(config)\n",
    "        \n",
    "        self.pre_mlp_norm = Norm(config)\n",
    "        self.mlp = MLP(config.embed_dim, config.mlp_multiplier, config.dropout_rate)\n",
    "        self.post_mlp_norm = Norm(config)\n",
    "        \n",
    "        self.logging_enabled = False\n",
    "        self.disabled_logging_functions = set()\n",
    "    def enable_logging(self):\n",
    "        self.logging_enabled = True\n",
    "    def disable_logging(self):\n",
    "        self.logging_enabled = False\n",
    "    def disable_function_logging(self, func_name):\n",
    "        self.disabled_logging_functions.add(func_name)\n",
    "    def enable_function_logging(self, func_name):\n",
    "        self.disabled_logging_functions.discard(func_name)\n",
    "\n",
    "    @log_io\n",
    "    def forward(self, \n",
    "                x: torch.Tensor,\n",
    "                c: torch.Tensor = None,\n",
    "                training: bool = False,\n",
    "               ) -> torch.Tensor:\n",
    "        x = x + self.self_mqa_connection(x, training)\n",
    "        if c is not None:\n",
    "            x = x + self.cross_mqa_connection(x, c, training)\n",
    "        x = x + self.mlp_connection(x, training)\n",
    "        return x\n",
    "\n",
    "    @log_io\n",
    "    def self_mqa_connection(self, x, training):\n",
    "        return self.post_self_mqa_norm(self.self_mqa(self.pre_self_mqa_norm(x, training), training), training)\n",
    "\n",
    "    @log_io\n",
    "    def cross_mqa_connection(self, x, c, training):\n",
    "        return self.post_cross_mqa_norm(self.cross_mqa(self.pre_cross_mqa_x_norm(x, training), \n",
    "                                                       self.pre_cross_mqa_c_norm(c, training), training), training)\n",
    "\n",
    "    @log_io\n",
    "    def mlp_connection(self, x, training):\n",
    "        return self.post_mlp_norm(self.mlp(self.pre_mlp_norm(x, training), training), training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711f6f6d-9997-4196-a88e-b8bf31196c85",
   "metadata": {},
   "source": [
    "### demonstration/debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d2a1bcb4-8464-4452-ac40-c6eb35fbf223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering Layer.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 256, 256])\n",
      "\n",
      "==========Entering Layer.self_mqa_connection==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 256, 256])\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 256, 256])\n",
      "==========Exiting Layer.self_mqa_connection==========\n",
      "\n",
      "==========Entering Layer.mlp_connection==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 256, 256])\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 256, 256])\n",
      "==========Exiting Layer.mlp_connection==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 256, 256])\n",
      "==========Exiting Layer.forward==========\n"
     ]
    }
   ],
   "source": [
    "module = Layer(config)\n",
    "module.enable_logging()\n",
    "\n",
    "### enabling printing for sub-modules\n",
    "#module.pre_self_mqa_norm.enable_logging()\n",
    "#module.self_mqa.enable_logging()\n",
    "#module.post_self_mqa_norm.enable_logging()\n",
    "#module.pre_cross_mqa_norm.enable_logging()\n",
    "#module.cross_mqa.enable_logging()\n",
    "#module.post_cross_mqa_norm.enable_logging()\n",
    "#module.pre_mlp_norm.enable_logging()\n",
    "#module.mlp.enable_logging()\n",
    "#module.post_mlp_norm.enable_logging()\n",
    "\n",
    "### disabling printing for sub-functions\n",
    "#module.disable_function_logging('self_mqa_connection')\n",
    "#module.disable_function_logging('cross_mqa_connection')\n",
    "#module.disable_function_logging('mlp_connection')\n",
    "\n",
    "output = module(torch.randn(32,config.max_seq_len,config.embed_dim))\n",
    "del module, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7853cc93-c55e-40fd-aa38-1a2a1f1f230f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering Layer.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 256, 256])\n",
      "Tensor 'c' shape: torch.Size([32, 64, 256])\n",
      "\n",
      "==========Entering Layer.self_mqa_connection==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 256, 256])\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 256, 256])\n",
      "==========Exiting Layer.self_mqa_connection==========\n",
      "\n",
      "==========Entering Layer.cross_mqa_connection==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 256, 256])\n",
      "Tensor 'c' shape: torch.Size([32, 64, 256])\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 256, 256])\n",
      "==========Exiting Layer.cross_mqa_connection==========\n",
      "\n",
      "==========Entering Layer.mlp_connection==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 256, 256])\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 256, 256])\n",
      "==========Exiting Layer.mlp_connection==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 256, 256])\n",
      "==========Exiting Layer.forward==========\n"
     ]
    }
   ],
   "source": [
    "hold = config.predictive_mask\n",
    "config.predictive_mask = False\n",
    "module = Layer(config)\n",
    "module.enable_logging()\n",
    "\n",
    "### enabling printing for sub-modules\n",
    "#module.pre_self_mqa_norm.enable_logging()\n",
    "#module.self_mqa.enable_logging()\n",
    "#module.post_self_mqa_norm.enable_logging()\n",
    "#module.pre_cross_mqa_norm.enable_logging()\n",
    "#module.cross_mqa.enable_logging()\n",
    "#module.post_cross_mqa_norm.enable_logging()\n",
    "#module.pre_mlp_norm.enable_logging()\n",
    "#module.mlp.enable_logging()\n",
    "#module.post_mlp_norm.enable_logging()\n",
    "\n",
    "### disabling printing for sub-functions\n",
    "#module.disable_function_logging('self_mqa_connection')\n",
    "#module.disable_function_logging('cross_mqa_connection')\n",
    "#module.disable_function_logging('mlp_connection')\n",
    "\n",
    "x = torch.randn(32, config.max_seq_len, config.embed_dim)\n",
    "c = torch.randn(32, config.max_seq_len // config.combo, config.embed_dim)\n",
    "output = module(x, c)\n",
    "config.predictive_mask = hold\n",
    "del hold, module, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f415280-2108-4a7a-a518-a59d64e98463",
   "metadata": {},
   "source": [
    "### need to do more debugging once predictive mask works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d11636-7958-4e38-91a4-00ec0d5949f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ed7034-9822-4cfa-93a6-cde136e95203",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6309588b-baa8-400e-9599-1cee9cbe3ef6",
   "metadata": {},
   "source": [
    "# Body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca9d16d-1a18-413e-a549-7b79ddebe13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to prevent the warning statement from printing hella times\n",
    "cvec_warning = False\n",
    "\n",
    "class Body(nn.Module):\n",
    "    def __init__(self, config: Config, embedder: torch.Tensor):\n",
    "        super().__init__()\n",
    "        self.ca_interval = config.ca_interval\n",
    "        self.max_seq_len = config.max_seq_len\n",
    "        self.combo = config.combo\n",
    "        self.levels = config.levels\n",
    "        self.embedding = embedder.weight\n",
    "        \n",
    "        # Initialize a sequence of Layer instances as specified by the number of hidden layers in the config\n",
    "        self.layers = nn.ModuleList(Layer(config) for _ in range(config.num_layers))\n",
    "        \n",
    "        # Initialize normalization layer to be applied after the last decoder layers, stabilizing the output\n",
    "        self.final_norm = Norm(config)\n",
    "        \n",
    "        self.logging_enabled = False\n",
    "        self.disabled_logging_functions = set()\n",
    "    def enable_logging(self):\n",
    "        self.logging_enabled = True\n",
    "    def disable_logging(self):\n",
    "        self.logging_enabled = False\n",
    "    def disable_function_logging(self, func_name):\n",
    "        self.disabled_logging_functions.add(func_name)\n",
    "    def enable_function_logging(self, func_name):\n",
    "        self.disabled_logging_functions.discard(func_name)\n",
    "        \n",
    "    @log_io\n",
    "    def forward(self,\n",
    "                x0s: Tuple[torch.Tensor], # ordered from tokens -> highest concepts\n",
    "                targets: Tuple[torch.Tensor] = None,\n",
    "                cvec_samples: int = None,\n",
    "                cvec_greedy: bool = False,\n",
    "                cvec_temp: float = 1.0,\n",
    "               ) -> Tuple[torch.Tensor]:\n",
    "        return self.forward_training(x0s, targets) if targets is not None else self.forward_inference(x0s, cvec_samples, cvec_greedy, cvec_temp)\n",
    "        \n",
    "    @log_io\n",
    "    def forward_training(self,\n",
    "                         x0s: Tuple[torch.Tensor], # ordered from tokens -> highest concepts\n",
    "                         training: bool = False,\n",
    "                        ) -> Tuple[torch.Tensor]:\n",
    "        # initiate tuple to hold final residual states\n",
    "        xfs = ()\n",
    "        # iterate through model levels, starting from highest level concepts & ending at lowest level tokens\n",
    "        for i, x in enumerate(reversed(x0s)): # reversed() makes us start at highest level\n",
    "\n",
    "        return\n",
    "        \n",
    "    @log_io\n",
    "    def forward_inference(self,\n",
    "                        x0s: Tuple[torch.Tensor], # ordered from tokens -> highest concepts\n",
    "                        cvec_samples: int = None,\n",
    "                        cvec_greedy: bool = False,\n",
    "                        cvec_temp: float = 1.0,\n",
    "                        training: bool = False,\n",
    "                       ) -> Tuple[torch.Tensor]:\n",
    "        # initiate tuple to hold final residual states\n",
    "        xfs = ()\n",
    "        # iterate through model levels, starting from highest level concepts & ending at lowest level tokens\n",
    "        for i, x in enumerate(reversed(x0s)): # reversed() makes us start at highest level\n",
    "\n",
    "            # if we're dealing with a concept level, then we need to create entire concept sequences. for token level we only want one pass\n",
    "            if i != len(x0s)-1:\n",
    "                effective_max_seq_len = self.max_seq_len // (self.combo ** (self.levels-1-i))\n",
    "                assert x.shape[1] <= effective_max_seq_len, f'somehow at level {i} a too-long sequence ({x.shape[1]} vs {effective_max_seq_len}) made it all the way to Body'\n",
    "                extra_runs = effective_max_seq_len - x.shape[1]\n",
    "                for k in range(extra_runs): # if extra_runs == 0 then this just won't do anything\n",
    "                    # run\n",
    "                    x_ = self.layers_loop(x, i, xfs[i-1], training) # xfs[i-1] is the concept embedding to pay attention to\n",
    "                    # splice out the final prediction\n",
    "                    x_ = x_[:,-1,:]\n",
    "                    # select most similar concept vectors to be appended to the sequence\n",
    "                    c = self.concept_matchup(x_, cvec_samples, cvec_greedy, cvec_temp)\n",
    "                    # append to x\n",
    "                    x = torch.concat([x, c.unsqueeze(1)], dim=1)\n",
    "\n",
    "            # the final run. this one will actually be logits to get used with crossMQA\n",
    "            x = self.layers_loop(x, i, xfs[i-1], training)\n",
    "            \n",
    "            # add the final residual state of the level to our tuple, normed\n",
    "            xfs += (self.final_norm(x, training),) # should i be using separate final norms? nah\n",
    "            \n",
    "        return xfs[-1] # for inference, we only need to return the resid state of the token level\n",
    "        \n",
    "    @log_io\n",
    "    def forward(self, \n",
    "                x0s: Tuple[torch.Tensor], # ordered from tokens -> highest concepts\n",
    "                cvec_samples: int = None,\n",
    "                cvec_greedy: bool = False,\n",
    "                cvec_temp: float = 1.0,\n",
    "                training: bool = False,\n",
    "               ) -> Tuple[torch.Tensor]:\n",
    "        # initiate tuple to hold final residual states\n",
    "        xfs = ()\n",
    "        # iterate through model levels, starting from highest level concepts & ending at lowest level tokens\n",
    "        for i, x in enumerate(reversed(x0s)): # reversed() makes us start at highest level\n",
    "\n",
    "            # if we're dealing with a concept level, then we need to create entire concept sequences. for tokens we only want one pass\n",
    "            if i != len(x0s)-1:\n",
    "                effective_max_seq_len = self.max_seq_len // (self.combo ** (self.levels-1-i))\n",
    "                assert x.shape[1] <= effective_max_seq_len, f'somehow a too-long sequence ({x.shape[1]} vs {effective_max_seq_len}) made it all the way to Body'\n",
    "                extra_runs = effective_max_seq_len - x.shape[1]\n",
    "                for k in range(extra_runs): # if extra_runs == 0 then this just won't do anything\n",
    "                    # run\n",
    "                    x_ = self.layers_loop(x, i, x0s, training)\n",
    "                    # splice out the final prediction\n",
    "                    x_ = x_[:,-1,:]\n",
    "                    # norm? no, i'm worried about affine transformations messing with it since this process wasn't present during training\n",
    "                    # select most similar concept vectors to be appended to the sequence\n",
    "                    c = self.concept_matchup(x_, cvec_samples, cvec_greedy, cvec_temp)\n",
    "                    # append to x\n",
    "                    x = torch.concat([x, c.unsqueeze(1)], dim=1)\n",
    "\n",
    "            # the final run. this one will actually be logits to get used with crossMQA\n",
    "            x = self.layers_loop(x, i, x0s, training)\n",
    "            \n",
    "            # add the final residual state of the level to our tuple, normed\n",
    "            xfs += (self.final_norm(x, training),) # should i be using separate final norms? nah\n",
    "\n",
    "        return xfs # now it's ordered from highest concepts -> token\n",
    "\n",
    "    @log_io\n",
    "    def layers_loop(self, x, i, x0s, training):\n",
    "        \n",
    "        # Iteratively process the input through each Layer of the model\n",
    "        for j in range(len(self.layers)):\n",
    "            layer = self.layers[j]\n",
    "\n",
    "            # i can't equal zero bc there'd be no higher level model to pay attention to\n",
    "            if (i != 0): \n",
    "                # run through layer while cross-attending to held outputs of upper level\n",
    "                x = layer(x, x0s[len(x0s)-i], training)\n",
    "            else:\n",
    "                # run through layer when there is no higher level to attend to\n",
    "                x = layer(x, training=training)\n",
    "        return x\n",
    "\n",
    "    @log_io\n",
    "    def concept_matchup(self,\n",
    "                        c: torch.Tensor,\n",
    "                        cvec_samples: int,\n",
    "                        cvec_greedy: bool,\n",
    "                        cvec_temp: float,\n",
    "                        ) -> torch.Tensor:\n",
    "        global cvec_warning\n",
    "        batch_size, d = c.size()\n",
    "        vocab_size = self.embedding.size(0)\n",
    "        embedding = self.embedding[:,:d]\n",
    "    \n",
    "        # Batch cosine similarity\n",
    "        # Reshape c: (batch_size x 1 x embedding_dim)\n",
    "        # Reshape embedding: (1 x vocab_size x embedding_dim)\n",
    "        # Resulting similarity: (batch_size x vocab_size)\n",
    "        token_similarities = F.cosine_similarity(c.unsqueeze(1), embedding.unsqueeze(0), dim=-1)\n",
    "        \n",
    "        # how many tokens will we sample to build up our chosen concept vector?\n",
    "        if cvec_samples is None:\n",
    "            cvec_samples = self.combo ** (self.levels-1)\n",
    "            if (cvec_warning == False) or (cvec_warning is None):\n",
    "                print(f\"cvec_samples not defined. defaulting to highest level's minimum size: combo**(levels-1) = {cvec_samples}\")\n",
    "                cvec_warning = True\n",
    "        assert cvec_samples >= self.combo ** (self.levels-1), f'cvec_samples = {cvec_samples} needs to be >= self.combo ** (self.levels-1) = {self.combo ** (self.levels-1)}'\n",
    "        \n",
    "        # Select top-k token embeddings for each concept vector\n",
    "        topk_token_indices = torch.topk(token_similarities, k=cvec_samples, dim=1).indices  # (batch_size x sample)\n",
    "    \n",
    "        # Generate concept embeddings for each set of top-k token embeddings\n",
    "        concept_embeddings_batch = []\n",
    "        X_sizes_batch = []\n",
    "        for i in range(batch_size):\n",
    "            # Pass the list of indices for each concept\n",
    "            concept_embeddings, X_sizes = self.create_concept_embeddings(embedding, \n",
    "                                                                         [topk_token_indices[i].tolist()])\n",
    "            concept_embeddings_batch.append(concept_embeddings.squeeze(0))  # Remove the extra batch dimension\n",
    "            X_sizes_batch.append(X_sizes)\n",
    "    \n",
    "        # Convert list of tensors to a tensor\n",
    "        concept_embeddings_batch = torch.stack(concept_embeddings_batch)  # (batch_size x max_X_size x d)\n",
    "    \n",
    "        # Calculate concept similarities for each concept in the batch\n",
    "        concept_similarities_batch = F.cosine_similarity(c.unsqueeze(1), concept_embeddings_batch, dim=-1)\n",
    "    \n",
    "        # Select the best matching concept embedding for each concept vector in the batch\n",
    "        if cvec_greedy:\n",
    "            best_concept_indices = concept_similarities_batch.argmax(dim=1)\n",
    "            matched_concepts = concept_embeddings_batch[torch.arange(batch_size), best_concept_indices]\n",
    "        else:\n",
    "            # Apply softmax with temperature and sample\n",
    "            topk_concept_probs = F.softmax(concept_similarities_batch / cvec_temp, dim=1)\n",
    "            concept_topk_idx = torch.multinomial(topk_concept_probs, num_samples=1).squeeze(1)\n",
    "            matched_concepts = concept_embeddings_batch[torch.arange(batch_size), concept_topk_idx]\n",
    "    \n",
    "        return matched_concepts\n",
    "\n",
    "    @log_io\n",
    "    def create_concept_embeddings(self, E: torch.Tensor, indices: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Create concept embeddings for a batch of indices.\n",
    "    \n",
    "        E: Embedding matrix (vocab_size x embedding_dim)\n",
    "        indices: A list of lists of indices (batch_size x num_indices)\n",
    "        \"\"\"\n",
    "        batch_size = len(indices)\n",
    "        d = E.size(1)\n",
    "        X_sizes = [(len(ind) - 1) * len(ind) // 2 for ind in indices]\n",
    "        max_X_size = max(X_sizes)\n",
    "        X = torch.empty((batch_size, max_X_size, d), dtype=E.dtype)\n",
    "        \n",
    "        # this could prolly be done way more efficiently with tensor operations\n",
    "        for b in range(batch_size):\n",
    "            count = 0\n",
    "            for i in range(len(indices[b])):\n",
    "                for j in range(i + 1, len(indices[b])):\n",
    "                    X[b, count] = E[indices[b][i]] + E[indices[b][j]]\n",
    "                    count += 1\n",
    "            # Padding the rest if necessary\n",
    "            if count < max_X_size:\n",
    "                X[b, count:] = torch.zeros((max_X_size - count, d))\n",
    "        \n",
    "        # X_sizes is not useful rn but i think it may be later when we switch away from TinyShakespeare\n",
    "        # and over to data that actually has variable sequence lengths\n",
    "        return X, X_sizes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
