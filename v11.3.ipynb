{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e31e531-c8f4-4641-8a87-90d7c1dd933a",
   "metadata": {},
   "source": [
    "# v11.3\n",
    "\n",
    "whereas the last version used matryoshka models, i think it makes more sense to go all-in on the self-similarity thing and simplify the whole process by using the same decoder for all levels\n",
    "\n",
    "todo's:\n",
    "- [x] copy & paste stuff that doesn't need to be changed\n",
    "- [x] copy, paste, & remove references to matryoshka embeddings for modules w/ em\n",
    "- [x] bring over todo items that haven't been completed from v11.2 and from notes app\n",
    "- [ ] ~~setup single attention mechanism module for both self & cross?~~ i feel like separate is prolly still easier. and this is kinda just unnecessary extra work\n",
    "- [x] setup single decoder layer body\n",
    "- [x] setup concept loss to use MSE and COS simultaneously\n",
    "- [ ] get predictive attention mask working in crossMQA\n",
    "- [x] add different types of pooling options to embedding combiner\n",
    "- [ ] pull concept loss out of model and into training loop so that i can progressively introduce the concept embedding prediction problem rather than deal with moving goalposts from the get-go\n",
    "- [x] make option for linear layer vs MLP in output from final concept residual states to actual concepts\n",
    "- [ ] write an inference algorithm that sacrifices quality for efficiency & see how well it works\n",
    "- [ ] ~~add multiple <|bos|> tokens to put at the beginning of each sequence, one for each level. So that way the model can know what level it's working with and have somewhere to sink attention.~~ the whole \"language is fractal\" thing means that the model shouldn't need to know. also if an MLP or linear is used for combining concepts then this would be happening implicitly anyways. better to just let a future better tokenizer do its job. \n",
    "- [x] make get_batch output values that'd result in actually using the padding token so that it gets trained\n",
    "- [ ] ~~switch to GaLore optimizer to save ram~~\n",
    "- [x] implement basic stuff i'm missing like cosine learning rate scheduling (gradient clipping?)\n",
    "- [ ] move everything into .py files without the LoggingModule parent\n",
    "- [ ] create big hyperparameter testing script to send to ben\n",
    "- [ ] would it make sense to have the RoPE theta adjust for the different sequence levels? like divide it by config.combine_factor? i really need to learn more about how RoPE works at an intuitive level rather than just \"uuuuuh trig rotations\"\n",
    "- [ ] ~~maybe add learned positional encodings to pooling?~~ nah ur doing too much\n",
    "- [ ] get the current version running & training so that i can replace TinyShakespeare with TinyStories and make corresponding edits to the dataloader\n",
    "- [x] make concepts defined recursively\n",
    "- [ ] is the mlp / linear layer / affine on the norm that gets used for each level->up combination shared across level bridges? If not it should be. or there should at least be an option to make it so.\n",
    "- [ ] setup inference concept selection\n",
    "\n",
    "      - [ ] add option to select based on cos, mae, and/or mse\n",
    "\n",
    "further ideas likely for v11.4:\n",
    "- [ ] setup effective_seq_len_mult and seq_len_list and a bunch of downstream stuff like weird batching at lower sub-levels to allow for hella long effective context lengths. i think what i do here is setup batches such that for a given context length T the concept level above will see nT before and after. i guess not really nT before and after, rather imagine if the context length of the token level model is T and the context length of the concept leel is nT then the token level will attend to a random splice of length T, and ofc it's prolly gonna be super annoying adapting the predictive mask to work with this\n",
    "- [ ] figure out a way to make concepts dynamic instead of predetermined length. I might be able to do something similar to [Quiet-STAR](https://arxiv.org/abs/2403.09629) to get this to work in a way that's not exactly what i was expecting. Like allow the model to generate multiple outputs and if those outputs help in the actual concept vector generation then keep them in the sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a84226-3000-41f2-b05c-cdbc41396595",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16628521-543c-4de8-a5aa-d830e7215bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my virtual environments are rarely properly connected to jupyter so this fixes that\n",
    "import sys\n",
    "import os\n",
    "current_dir = os.getcwd()  # Get the current working directory\n",
    "venv_dir = os.path.join(current_dir, 'venv') \n",
    "python_version = str(sys.version_info.major) + '.' + str(sys.version_info.minor)\n",
    "site_packages_path = os.path.join(venv_dir, 'lib', 'python' + python_version, 'site-packages')\n",
    "sys.path.append(site_packages_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8381842d-1256-4d00-a8a1-dd6e4fdc6192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# imports for the debugging/demonstration setup\n",
    "import functools\n",
    "import inspect\n",
    "\n",
    "# imports for the tokenizer\n",
    "from tiny_stories_tokenizer import *\n",
    "\n",
    "# Imports used for the config\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "# Imports used for the model\n",
    "import re\n",
    "from typing import Any, List, Sequence, Tuple, Union\n",
    "\n",
    "# used for training\n",
    "import random\n",
    "import time\n",
    "\n",
    "# used to save & load models\n",
    "import json\n",
    "from dataclasses import asdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdeda5ff-46cc-49f5-b496-6d316bda3835",
   "metadata": {},
   "source": [
    "# Demonstration/Debugging wrapper & Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02b4aea8-31e9-4fa9-8b62-187a042b33d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will be used throughout for debugging/demonstration purposes\n",
    "# using this is way cleaner than cluttering up our code with print statements\n",
    "def log_io(func):\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(self, *args, **kwargs):\n",
    "        # Check if logging is enabled globally and for the specific function\n",
    "        if not self.logging_enabled or func.__name__ in self.disabled_logging_functions:\n",
    "            return func(self, *args, **kwargs)\n",
    "        #if not self.logging_enabled:\n",
    "            #return func(self, *args, **kwargs)\n",
    "\n",
    "        def log_item(item, name, level=0, is_root=False):\n",
    "            indent = \"    \" * level\n",
    "            if isinstance(item, torch.Tensor):\n",
    "                print(f\"{indent}Tensor '{name}' shape: {item.shape}\")\n",
    "            elif isinstance(item, tuple):\n",
    "                if is_root and level == 0:\n",
    "                    # Root level tuple, don't print it as a tuple unless it's a \"true\" tuple\n",
    "                    for idx, sub_item in enumerate(item):\n",
    "                        log_item(sub_item, f\"{name}[{idx}]\", level)\n",
    "                else:\n",
    "                    print(f\"{indent}Tuple '{name}':\")\n",
    "                    for idx, sub_item in enumerate(item):\n",
    "                        log_item(sub_item, f\"{name}[{idx}]\", level + 1)\n",
    "            elif isinstance(item, int):\n",
    "                print(f\"{indent}Integer '{name}': Value={item}\")\n",
    "            elif isinstance(item, float):\n",
    "                print(f\"{indent}Float '{name}': Value={item}\")\n",
    "            else:\n",
    "                print(f\"{indent}Other-type '{name}': Type={type(item).__name__}, Value={item}\")\n",
    "\n",
    "        print(f\"\\n{'='*10}Entering {self.__class__.__name__}.{func.__name__}{'='*10}\")\n",
    "        print(\"Inputs:\")\n",
    "        arg_names = inspect.getfullargspec(func).args[1:]  # Excluding 'self'\n",
    "        arg_values = args + tuple(kwargs.values())\n",
    "        for name, value in zip(arg_names, arg_values):\n",
    "            log_item(value, name)\n",
    "\n",
    "        result = func(self, *args, **kwargs)\n",
    "        print(\"\\nOutputs:\")\n",
    "        if isinstance(result, tuple):\n",
    "            log_item(result, \"output\", is_root=True)\n",
    "        else:\n",
    "            log_item(result, \"output\")\n",
    "\n",
    "        print(f\"{'='*10}Exiting {self.__class__.__name__}.{func.__name__}{'='*10}\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "class LoggingModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.logging_enabled = False\n",
    "        self.disabled_logging_functions = set()\n",
    "\n",
    "    def enable_logging(self):\n",
    "        self.logging_enabled = True\n",
    "\n",
    "    def disable_logging(self):\n",
    "        self.logging_enabled = False\n",
    "\n",
    "    def disable_function_logging(self, func_name):\n",
    "        self.disabled_logging_functions.add(func_name)\n",
    "\n",
    "    def enable_function_logging(self, func_name):\n",
    "        self.disabled_logging_functions.discard(func_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3214e4-7a9b-4e82-a5d3-3fea93f378e2",
   "metadata": {},
   "source": [
    "# Config & tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cff0f146-fcdc-4ef6-bc0b-c79e0c7c98c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "# we'll replace this with the TinyStories dataset and the corresponding funcitons below once we've got a regular running model again\n",
    "\n",
    "# and the tokenizer\n",
    "tokenizer = get_tokenizer(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b307756e-23ca-4869-adc5-224128aede9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(vocab_size=512, max_seq_len=128, embed_dim=128, num_layers=8, sa_q_heads=2, sa_kv_heads=1, sa_head_dim=32, mlp_multiplier=2, theta=100.0, dropout_rate=0.1, norm_affine=True, learning_rate_init=0.1, learning_rate_end=0.0001, weight_decay=0.01, max_iters=1000, levels=3, combine_factor=4, combine_type='reshape->mlp_post_reshape->norm', ca_q_heads=2, ca_kv_heads=1, ca_head_dim=32, ca_use_RoPE=False, predictive_mask=False, predictive_mask_noise=None, level_loss_weight=1.0, cos_loss=True, mse_loss=True, mae_loss=True)\n",
      "sequence length of each model: [128, 32, 8]\n",
      "loss discounts starting from lowest level: [1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "@dataclass # a class meant specifically to just hold data\n",
    "class Config:\n",
    "    \"\"\" \n",
    "    The default configuration & hyperparameters for my next-concept predictor\n",
    "    \"\"\"\n",
    "    ### boring hyperparameters\n",
    "    vocab_size: int = tokenizer.vocab_len\n",
    "    max_seq_len: int = 128\n",
    "    embed_dim: int =128\n",
    "    num_layers: int = 8\n",
    "    sa_q_heads: int = 2\n",
    "    sa_kv_heads: int = 1\n",
    "    #attn_bias: bool = False\n",
    "    sa_head_dim: int = 32\n",
    "    mlp_multiplier: int = 2\n",
    "    theta: float = 100.0\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    dropout_rate: float = 0.1\n",
    "    eps = 1e-6\n",
    "    norm_type = \"RMSNorm\"  # Options are RMSNorm, CosineNorm and LayerNorm. defaults to RMSNorm\n",
    "    norm_affine: bool = True # whether norms should have a linear & bias after them. likely less necessary if you're using RMSNorm\n",
    "    learning_rate_init: float = 1e-1\n",
    "    learning_rate_end: float = 1e-4\n",
    "    weight_decay: float = 0.01\n",
    "    max_iters: int = 1000\n",
    "\n",
    "    ### Concept embedding vectors\n",
    "    levels: int = 3\n",
    "    combine_factor: int = 4 # how many lower-level tokens/concepts to combine into the next level's concept\n",
    "    combine_type: str = 'reshape->mlp_post_reshape->norm' # options to combine w/ '->' are 'sum', 'mean', 'max', 'linear', 'mlp', 'reshape', 'linear_post_reshape', and 'mlp_post_reshape'\n",
    "    @property\n",
    "    def seq_len_list(self):\n",
    "        return [(self.max_seq_len // (self.combine_factor ** (i-1))) for i in range(1, self.levels + 1)]\n",
    "\n",
    "    ### Dualcoder cross-attention\n",
    "    ca_q_heads: int = sa_q_heads\n",
    "    ca_kv_heads: int = sa_kv_heads\n",
    "    ca_head_dim: int = sa_head_dim\n",
    "    ca_use_RoPE: bool = False # True: expands out k & v tensors to be usable with rope. False: leaves k & v same size but no positional encodings\n",
    "    predictive_mask: bool = False # True: upper-triangular predictive mask to focus model's attention (not currently working). False: no mask like a regular encoder\n",
    "    predictive_mask_noise: float = None # float: sd of noise to add to predictively masked concept vecs. None: don't implement noise\n",
    "\n",
    "    ### concept output\n",
    "    output_layer = 'mlp' # options are 'linear' and 'mlp' which uses the default mlp_multiplier\n",
    "    # how much to discount each higher level in the loss function compared to the last\n",
    "    level_loss_weight: float = 1.0 \n",
    "    # multiple losses can act on the concept vectors at once\n",
    "    cos_loss: bool = True\n",
    "    mse_loss: bool = True\n",
    "    mae_loss: bool = True\n",
    "    \n",
    "    ### assertions\n",
    "    assert sa_q_heads % sa_kv_heads == 0, 'the number of query heads must be divisible by the number of key-value heads in self-attention'\n",
    "    assert ca_q_heads % ca_kv_heads == 0, 'the number of query heads must be divisible by the number of key-value heads in cross-attention'     \n",
    "        \n",
    "config = Config()\n",
    "print(config)\n",
    "print(f\"sequence length of each model: {config.seq_len_list}\")\n",
    "print(f\"loss discounts starting from lowest level: {[config.level_loss_weight**i for i in range(config.levels)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3f0865-495e-4c43-bff3-e87756818725",
   "metadata": {},
   "source": [
    "# RoPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "374a3ab5-9cba-4dd3-b5e9-5d0e4d419952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RoPE(x: torch.Tensor, dim: int, theta: float = 10000.0) -> torch.Tensor:\n",
    "    \"\"\"Applies the rotary embedding to the inputted query or key tensor\"\"\"\n",
    "    # Validate that dim is even since we split it by 2 for real and imaginary parts\n",
    "    if dim % 2 != 0: raise ValueError(\"Dimension 'dim' must be an even number.\")\n",
    "            \n",
    "    # Get sequence length\n",
    "    seq_len = x.size(1)\n",
    "    device = x.device\n",
    "\n",
    "    # Dynamically compute frequency cis based on the input sequence length\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2, device=device).float() / dim))\n",
    "    t = torch.arange(seq_len, device=device)\n",
    "    freqs = torch.outer(t, freqs).float()\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    # it's important to train on a wide variety of sequence lengths within your context length so that the model learns to generalize\n",
    "\n",
    "    # Apply rotary embeddings to the input tensor\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    x_out = x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "    return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d603ec0-52ed-4d21-ad9f-fdf6ede861e9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## maybe precompute frequencies later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14adecb-9c7c-464e-8b40-d3d22ff93cc0",
   "metadata": {},
   "source": [
    "i like the idea of pre-computing RoPE embeddings but at this point i don't think it's worth the effort bc i'd have to not only use this code but also pipe the two instantiations of this class through from `Body` all the way to `selfMQA` and `crossMQA` and I'm not even sure if it matters. I really should learn more about RoPE\n",
    "\n",
    "```Python\n",
    "# this class has not been implemented nor even tested. on my todo list\n",
    "class RoPE(nn.Module):\n",
    "    def __init__(self, \n",
    "                 dim: int, \n",
    "                 max_seq_len:int = config.max_seq_len, \n",
    "                 device: str = config.device):\n",
    "        super().__init__()\n",
    "        # Validate that dim is even since we split it by 2 for real and imaginary parts\n",
    "        if dim % 2 != 0: raise ValueError(\"Dimension 'dim' must be an even number.\")\n",
    "            \n",
    "        # Precompute frequencies based on configuration\n",
    "        theta = config.theta if hasattr(config, 'theta') else 10000.0\n",
    "        \n",
    "        freqs = 1.0 / (theta ** (torch.arange(0, config.dim, 2, device=config.device).float() / config.dim))\n",
    "        t = torch.arange(config.max_seq_len, device=config.device)\n",
    "        freqs = torch.outer(t, freqs).to(config.device).float()\n",
    "        \n",
    "        # Register as buffer to prevent gradient tracking\n",
    "        self.register_buffer('freqs_cis', torch.polar(torch.ones_like(freqs), freqs)) # complex64\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply rotary embeddings to the input tensor\n",
    "        x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "        x_out = torch.view_as_real(x_ * self.freqs_cis.unsqueeze(0)).type_as(x)\n",
    "        x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "        x_out = x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "        return x_out\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851a4b6f-2dc7-4c59-b71f-0e6740101acb",
   "metadata": {},
   "source": [
    "# selfMQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e735756c-9806-4000-84fd-7008bfa0d251",
   "metadata": {},
   "outputs": [],
   "source": [
    "class selfMQA(LoggingModule): # notice thoughout we'll be inheriting from LoggingModule instead of nn.Module\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.sa_q_heads = config.sa_q_heads\n",
    "        self.sa_kv_heads = config.sa_kv_heads\n",
    "        assert self.sa_q_heads % self.sa_kv_heads == 0\n",
    "        self.num_queries_per_kv = self.sa_q_heads // self.sa_kv_heads\n",
    "\n",
    "        self.embed_dim = config.embed_dim\n",
    "        self.sa_head_dim = config.sa_head_dim\n",
    "        self.theta = config.theta\n",
    "        self.dropout_rate = config.dropout_rate\n",
    "\n",
    "        #self.Wqkv = nn.Linear(self.embed_dim, \n",
    "        #                      (self.sa_q_heads + 2 * self.sq_kv_heads) * self.sa_head_dim, \n",
    "        #                      bias = config.attn_bias)\n",
    "        #self.Wo = nn.Linear(self.sa_q_heads * self.sa_head_dim,\n",
    "        #                    self.embed_dim,\n",
    "        #                    bias = config.attn_bias)\n",
    "        \n",
    "        self.Wqkv = nn.Parameter(torch.Tensor(self.embed_dim, (self.sa_q_heads + 2 * self.sa_kv_heads) * self.sa_head_dim))\n",
    "        nn.init.uniform_(self.Wqkv, -((1 / self.embed_dim) ** 0.5), (1 / self.embed_dim) ** 0.5)\n",
    "\n",
    "        self.Wo = nn.Parameter(torch.Tensor(self.sa_q_heads * self.sa_head_dim, self.embed_dim))\n",
    "        nn.init.uniform_(self.Wo, -((1 / (self.sa_q_heads * self.sa_head_dim)) ** 0.5), (1 / (self.sa_q_heads * self.sa_head_dim)) ** 0.5)\n",
    "\n",
    "        # for our attention mask we'll create a boolean mask that'll later be turned into large negative values\n",
    "        self.mask = torch.tril(torch.ones((config.max_seq_len, config.max_seq_len), dtype=torch.uint8)\n",
    "                              ).view(1, 1, config.max_seq_len, config.max_seq_len).to(dtype=torch.bool)\n",
    "\n",
    "    @log_io\n",
    "    def forward(self, x: torch.Tensor, training: bool = False) -> torch.Tensor:\n",
    "        # Extracts batch size and input sequence length from the hidden states tensor.\n",
    "        batch_size, input_len, _ = x.shape\n",
    "\n",
    "        # splicing our primary projection to get the correct sub-matrices\n",
    "        Wq, Wk, Wv = self.weight_splicing(self.Wqkv)\n",
    "        # technically self.weight_splicing has access to self.Wqkv & Wo but this way our debugger can see them\n",
    "\n",
    "        # Applies the linear projection to the hidden state to retrieve our q, k & v projections\n",
    "        xq = F.dropout(x @ Wq, p=self.dropout_rate, training=training) # also dropout if we're training\n",
    "        xk = F.dropout(x @ Wk, p=self.dropout_rate, training=training)\n",
    "        xv = F.dropout(x @ Wv, p=self.dropout_rate, training=training)\n",
    "\n",
    "        # Reshapes each of the Q, K, and V tensors to separate the heads and align the dimensions for attention operations.\n",
    "        xq = xq.view(batch_size, -1, self.sa_q_heads, self.sa_head_dim)\n",
    "        xk = xk.view(batch_size, -1, self.sa_kv_heads, self.sa_head_dim)\n",
    "        xv = xv.view(batch_size, -1, self.sa_kv_heads, self.sa_head_dim)\n",
    "\n",
    "        # Applies rotary positional embeddings to queries and keys to incorporate positional information.\n",
    "        xq, xk = self.RoPE(xq, xk)\n",
    "\n",
    "        # If the number of KV heads is different from the number of query heads, adjusts keys and values to match the query heads count.\n",
    "        if self.sa_kv_heads != self.sa_q_heads:\n",
    "            xk, xv = self.match_headcount(xk, xv) # [batch_size, input_len, n_local_heads, sa_head_dim]\n",
    "\n",
    "        # Transposes Q, K, and V tensors to align them for the batch matrix multiplication in attention calculation.\n",
    "        xq = xq.transpose(1, 2) # [batch_size, n_local_heads, input_len, sa_head_dim]\n",
    "        xk = xk.transpose(1, 2)\n",
    "        xv = xv.transpose(1, 2)\n",
    "\n",
    "        # Calculates attention scores by performing a batch matrix multiplication between queries and keys, followed by scaling.\n",
    "        logits = self.attend(xq, xk) # [batch_size, n_local_heads, input_len, input_len]\n",
    "        \n",
    "        # Applies the lower-triangular mask to the attention logits\n",
    "        logits = self.apply_mask(logits, input_len)\n",
    "\n",
    "        # applies values to get final output\n",
    "        output = self.calc_output(logits, xv, batch_size, input_len, training) \n",
    "\n",
    "        # Applies the final linear projection to the attention output, mapping it back to self.embed_dim\n",
    "        return F.dropout(output @ self.Wo, p=self.dropout_rate, training=training) # also dropout if we're training\n",
    "\n",
    "    @log_io\n",
    "    def weight_splicing(self, Wqkv):\n",
    "        Wq, Wk, Wv = Wqkv.split([self.sa_q_heads * self.sa_head_dim,\n",
    "                                 self.sa_kv_heads * self.sa_head_dim,\n",
    "                                 self.sa_kv_heads * self.sa_head_dim],dim = -1)\n",
    "        return Wq, Wk, Wv\n",
    "\n",
    "    @log_io\n",
    "    def RoPE(self, xq, xk):\n",
    "        xq = RoPE(xq, self.sa_head_dim, self.theta)\n",
    "        xk = RoPE(xk, self.sa_head_dim, self.theta)\n",
    "        return xq, xk\n",
    "\n",
    "    @log_io\n",
    "    def match_headcount(self, xk, xv):\n",
    "        xk = torch.repeat_interleave(xk, self.num_queries_per_kv, dim=2)\n",
    "        xv = torch.repeat_interleave(xv, self.num_queries_per_kv, dim=2)\n",
    "        return xk, xv\n",
    "\n",
    "    @log_io\n",
    "    def attend(self, xq, xk):\n",
    "        return torch.matmul(xq, xk.transpose(2, 3)) * (self.sa_head_dim ** -0.5)\n",
    "        \n",
    "    @log_io\n",
    "    def apply_mask(self, logits, input_len):\n",
    "        return torch.where(self.mask[..., :input_len, :input_len].expand_as(logits),\n",
    "                           logits,\n",
    "                           torch.tensor(-1e30, device=logits.device, dtype=logits.dtype))\n",
    "    \n",
    "    @log_io\n",
    "    def calc_output(self, logits, xv, batch_size, input_len, training):\n",
    "        # Applies softmax to the logits to obtain attention probabilities\n",
    "        scores = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # also applies dropout if we're training\n",
    "        scores = F.dropout(scores, p=self.dropout_rate, training=training)\n",
    "        \n",
    "        # Computes the weighted sum of values based on the attention scores to obtain the output of the attention mechanism.\n",
    "        output = scores @ xv # [batch_size, n_local_heads, input_len, sa_head_dim]\n",
    "\n",
    "        # Reshapes the attention output to match the expected output dimensions, combining the heads back into the hidden dimension.\n",
    "        return output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5e3b57-78fc-4243-81c4-4cbb898f9e27",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## demonstration/debugging\n",
    "I've setup these little snippets after each nn.Module to help you see what's happening and for my own debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf8af24f-0fab-4457-8aca-26f1ecc479b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering selfMQA.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 256, 256])\n",
      "\n",
      "==========Entering selfMQA.weight_splicing==========\n",
      "Inputs:\n",
      "Tensor 'Wqkv' shape: torch.Size([256, 256])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([256, 128])\n",
      "Tensor 'output[1]' shape: torch.Size([256, 64])\n",
      "Tensor 'output[2]' shape: torch.Size([256, 64])\n",
      "==========Exiting selfMQA.weight_splicing==========\n",
      "\n",
      "==========Entering selfMQA.RoPE==========\n",
      "Inputs:\n",
      "Tensor 'xq' shape: torch.Size([32, 256, 2, 64])\n",
      "Tensor 'xk' shape: torch.Size([32, 256, 1, 64])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([32, 256, 2, 64])\n",
      "Tensor 'output[1]' shape: torch.Size([32, 256, 1, 64])\n",
      "==========Exiting selfMQA.RoPE==========\n",
      "\n",
      "==========Entering selfMQA.match_headcount==========\n",
      "Inputs:\n",
      "Tensor 'xk' shape: torch.Size([32, 256, 1, 64])\n",
      "Tensor 'xv' shape: torch.Size([32, 256, 1, 64])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([32, 256, 2, 64])\n",
      "Tensor 'output[1]' shape: torch.Size([32, 256, 2, 64])\n",
      "==========Exiting selfMQA.match_headcount==========\n",
      "\n",
      "==========Entering selfMQA.attend==========\n",
      "Inputs:\n",
      "Tensor 'xq' shape: torch.Size([32, 2, 256, 64])\n",
      "Tensor 'xk' shape: torch.Size([32, 2, 256, 64])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 2, 256, 256])\n",
      "==========Exiting selfMQA.attend==========\n",
      "\n",
      "==========Entering selfMQA.apply_mask==========\n",
      "Inputs:\n",
      "Tensor 'logits' shape: torch.Size([32, 2, 256, 256])\n",
      "Integer 'input_len': Value=256\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 2, 256, 256])\n",
      "==========Exiting selfMQA.apply_mask==========\n",
      "\n",
      "==========Entering selfMQA.calc_output==========\n",
      "Inputs:\n",
      "Tensor 'logits' shape: torch.Size([32, 2, 256, 256])\n",
      "Tensor 'xv' shape: torch.Size([32, 2, 256, 64])\n",
      "Integer 'batch_size': Value=32\n",
      "Integer 'input_len': Value=256\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 256, 128])\n",
      "==========Exiting selfMQA.calc_output==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 256, 256])\n",
      "==========Exiting selfMQA.forward==========\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of selfMQA\n",
    "module = selfMQA(config)\n",
    "\n",
    "# Initially, logging is disabled\n",
    "# Enable logging\n",
    "module.enable_logging()\n",
    "\n",
    "### Optionally disabling printing for sub-functions\n",
    "#module.disable_function_logging('weight_splicing')\n",
    "#module.disable_function_logging('RoPE')\n",
    "#module.disable_function_logging('match_headcount')\n",
    "#module.disable_function_logging('attend')\n",
    "#module.disable_function_logging('apply_mask')\n",
    "#module.disable_function_logging('calc_output')\n",
    "\n",
    "# Call the forward method - logging will occur\n",
    "output = module(torch.randn(32,config.max_seq_len,config.embed_dim))#, training=True)\n",
    "\n",
    "# Disable logging. \n",
    "# This isn't actually necessary since we won't be using this object again but that's how you'd do it\n",
    "module.disable_logging()\n",
    "\n",
    "# clearing up ram jic we're training later\n",
    "del module, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8dc236-465c-40a4-9430-5bcbe10789da",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fc6e8d9-3d0a-44d7-9389-b64414d086eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(LoggingModule):\n",
    "    def __init__(self,\n",
    "                 embed_dim: int,\n",
    "                 mlp_multiplier: int,\n",
    "                 output_dim: int,\n",
    "                 dropout_rate: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.mlp_multiplier = mlp_multiplier\n",
    "        self.hidden_size = embed_dim * mlp_multiplier\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # the gate, up and down projections\n",
    "        self.gate_proj = nn.Linear(embed_dim, self.hidden_size)\n",
    "        self.up_proj = nn.Linear(embed_dim, self.hidden_size)\n",
    "        self.down_proj = nn.Linear(self.hidden_size, output_dim)\n",
    "        \n",
    "    @log_io\n",
    "    def forward(self, x: torch.Tensor, training: bool = False ) -> torch.Tensor:\n",
    "        output = self.down_proj(F.gelu(self.gate_proj(x)) * self.up_proj(x))\n",
    "        return F.dropout(output, p=self.dropout_rate, training=training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c357e34-cf79-4363-a97f-d96b8a56e837",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## demonstration/debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "410acc17-013b-4a26-bc59-f5780cef9073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering MLP.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 256, 256])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 256, 256])\n",
      "==========Exiting MLP.forward==========\n"
     ]
    }
   ],
   "source": [
    "module = MLP(config.embed_dim, config.mlp_multiplier, config.embed_dim, config.dropout_rate)\n",
    "module.enable_logging()\n",
    "output = module(torch.randn(32,config.max_seq_len,config.embed_dim))\n",
    "del module, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9be355-a811-41de-a761-822cd1cf5ad8",
   "metadata": {},
   "source": [
    "# Norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "548c6663-41ce-4715-b138-30fe7dfebd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Norm(LoggingModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.eps = config.eps\n",
    "        self.affine = config.norm_affine\n",
    "        self.dropout_rate = config.dropout_rate\n",
    "        self.type = config.norm_type\n",
    "\n",
    "        # Initialize weight and bias parameters for affine transformation\n",
    "        # We start with ones for weight to keep the original scale initially, and zeros for bias.\n",
    "        self.w = nn.Parameter(torch.ones(config.embed_dim))\n",
    "        self.b = nn.Parameter(torch.zeros(config.embed_dim))\n",
    "\n",
    "        # Mapping norm types to their respective methods\n",
    "        self.norm_methods = {\n",
    "            \"CosineNorm\": self.CosineNorm,\n",
    "            \"LayerNorm\": self.LayerNorm,\n",
    "            \"RMSNorm\": self.RMSNorm}\n",
    "\n",
    "        # Ensure the specified norm type exists, default to RMSNorm if not found\n",
    "        if self.type not in self.norm_methods:\n",
    "            self.type = \"RMSNorm\"\n",
    "            print(f'{self.type} not found. defaulting to RMSNorm')\n",
    "\n",
    "    @log_io\n",
    "    def forward(self, x: torch.Tensor, training: bool = False) -> torch.Tensor:\n",
    "        # Apply normalization\n",
    "        norm_method = self.norm_methods[self.type]\n",
    "        x = norm_method(x)\n",
    "\n",
    "        if self.affine: # Optionally apply the affine transformation and dropout if we're training\n",
    "            x = F.dropout(x * self.w + self.b, p=self.dropout_rate, training=training)\n",
    "            \n",
    "        return x\n",
    "\n",
    "    @log_io\n",
    "    def CosineNorm(self, x):\n",
    "        # normalize x by dividing by its L2 norm along the last dimension.\n",
    "        # this places x on the unit hypersphere centered at the origin\n",
    "        # Add a small constant to the denominator to avoid division by zero.\n",
    "        return x / torch.norm(x, p=2, dim=-1, keepdim=True).clamp(min=self.eps)\n",
    "\n",
    "    @log_io\n",
    "    def LayerNorm(self, x): # nn.LayerNorm() exists but might as well make it from scratch if we have to do the other two\n",
    "        # normalize x by subtracting by its mean then dividing by its variance\n",
    "        # this places x on a hypersphere of radius sqrt(dimension) centered at the origin\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        return (x - mean) / torch.sqrt(var + self.eps)\n",
    "\n",
    "    @log_io\n",
    "    def RMSNorm(self, x):\n",
    "        # normalize x by dividing by its root-mean-square along the last dimension\n",
    "        # this places x on a hypersphere of radius sqrt(dimension) with no certain center\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5e1ddc-f4a0-42ea-b2a0-629e0115848f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## demonstration/debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ca306ab-41fb-497c-b1a1-4154c141aa22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering Norm.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 128, 64])\n",
      "\n",
      "==========Entering Norm.RMSNorm==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 128, 64])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 128, 64])\n",
      "==========Exiting Norm.RMSNorm==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 128, 64])\n",
      "==========Exiting Norm.forward==========\n"
     ]
    }
   ],
   "source": [
    "module = Norm(config)\n",
    "module.enable_logging()\n",
    "\n",
    "### disabling printing for sub-functions\n",
    "#module.disable_function_logging('RMSNorm')\n",
    "#module.disable_function_logging('LayerNorm')\n",
    "#module.disable_function_logging('CosineNorm')\n",
    "\n",
    "output = module(torch.randn(32, config.max_seq_len, config.embed_dim))\n",
    "del module, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f333b374-d14d-4d5f-8913-c36c47a87df7",
   "metadata": {},
   "source": [
    "# crossMQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3606776-86d8-4c94-8573-59babb19de59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class crossMQA(LoggingModule):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ca_q_heads = config.ca_q_heads\n",
    "        self.ca_kv_heads = config.ca_kv_heads\n",
    "        assert self.ca_q_heads % self.ca_kv_heads == 0\n",
    "        self.num_queries_per_kv = self.ca_q_heads // self.ca_kv_heads\n",
    "        self.embed_dim = config.embed_dim\n",
    "        self.ca_head_dim = config.ca_head_dim\n",
    "        self.sa_head_dim = config.sa_head_dim # used only for an assertion to make sure sizes will fit\n",
    "        self.combine_factor = config.combine_factor # used only for an assertion to make sure sizes will fit\n",
    "        self.theta = config.theta\n",
    "        self.use_RoPE = config.ca_use_RoPE\n",
    "        self.dropout_rate = config.dropout_rate\n",
    "        self.predictive_mask = config.predictive_mask\n",
    "\n",
    "        self.Wqkv = nn.Parameter(torch.Tensor(self.embed_dim, (self.ca_q_heads + 2 * self.ca_kv_heads) * self.ca_head_dim))\n",
    "        nn.init.uniform_(self.Wqkv, -((1 / self.embed_dim) ** 0.5), (1 / self.embed_dim) ** 0.5)\n",
    "        \n",
    "        self.Wo = nn.Parameter(torch.Tensor(self.ca_q_heads * self.ca_head_dim, self.embed_dim))\n",
    "        nn.init.uniform_(self.Wo, -((1 / (self.ca_q_heads * self.ca_head_dim)) ** 0.5), (1 / (self.ca_q_heads * self.ca_head_dim)) ** 0.5)\n",
    "\n",
    "    @log_io\n",
    "    def forward(self, \n",
    "                x: torch.Tensor, # the current level tensor, sometimes a resid state full of tokens & sometimes concepts\n",
    "                c: torch.Tensor, # the upper level tensor, always a resid state full of concept vecs\n",
    "                training: bool = False\n",
    "               ) -> torch.Tensor:\n",
    "        \n",
    "        # Extracts batch size and input sequence length from the hidden states tensor.\n",
    "        batch_size, input_len_x, _ = x.shape\n",
    "        batch_size_c, input_len_c, _ = c.shape\n",
    "        assert batch_size == batch_size_c\n",
    "\n",
    "        # splicing our projection to get the correct sub-matrices\n",
    "        Wq, Wk, Wv = self.weight_splicing(self.Wqkv)\n",
    "\n",
    "        # Applies the linear projection to the hidden state to retrieve our q, k & v projections\n",
    "        xq = F.dropout(x @ Wq, p=self.dropout_rate, training=training) # also applies dropout if we're training\n",
    "        ck = F.dropout(c @ Wk, p=self.dropout_rate, training=training)\n",
    "        cv = F.dropout(c @ Wv, p=self.dropout_rate, training=training)\n",
    "\n",
    "        # Reshapes each of the Q, K, and V tensors to separate the heads and align the dimensions for attention operations.\n",
    "        xq = xq.view(batch_size, -1, self.ca_q_heads, self.ca_head_dim)\n",
    "        ck = ck.view(batch_size, -1, self.ca_kv_heads, self.ca_head_dim)\n",
    "        cv = cv.view(batch_size, -1, self.ca_kv_heads, self.ca_head_dim)\n",
    "\n",
    "        # IF we want to use RoPE (doesn't fully make sense to i don't think)\n",
    "        if self.use_RoPE:\n",
    "            expand = input_len_x // input_len_c\n",
    "            ck = ck.repeat_interleave(expand, dim=1) \n",
    "            cv = cv.repeat_interleave(expand, dim=1) # values need to be expanded for their use later on if we do this\n",
    "\n",
    "            # Applies rotary positional embeddings to queries and keys to incorporate positional information.\n",
    "            xq, ck = self.RoPE(xq, ck)\n",
    "\n",
    "        # If the number of KV heads is different from the number of query heads, adjusts keys and values to match the query heads count.\n",
    "        if self.ca_kv_heads != self.ca_q_heads:\n",
    "            ck, cv = self.match_headcount(ck, cv) # [batch_size, input_len, n_local_heads, sa_head_dim]\n",
    "\n",
    "        # Transposes Q, K, and V tensors to align them for the batch matrix multiplication in attention calculation.\n",
    "        xq = xq.transpose(1, 2) # [batch_size, n_local_heads, input_len, ca_head_dim]\n",
    "        ck = ck.transpose(1, 2)\n",
    "        cv = cv.transpose(1, 2)\n",
    "\n",
    "        # Calculates attention scores by performing a batch matrix multiplication between queries and keys, followed by scaling.\n",
    "        logits = self.attend(xq, ck) # [batch_size, n_local_heads, input_len, input_len]\n",
    "        \n",
    "        # Optionally applies the upper-triangular mask to the attention logits\n",
    "        if self.predictive_mask:\n",
    "            logits = self.apply_mask(logits, input_len_x, input_len_c)\n",
    "\n",
    "        # applies values to get final output\n",
    "        output = self.calc_output(logits, cv, batch_size, input_len_x, training)\n",
    "\n",
    "        # Applies the final linear projection to the attention output, mapping it back to d_i. \n",
    "        return F.dropout(output @ self.Wo, p=self.dropout_rate, training=training) # and dropout if we're training\n",
    "\n",
    "    @log_io\n",
    "    def weight_splicing(self, Wqkv):\n",
    "        Wq, Wk, Wv = Wqkv.split([self.ca_q_heads * self.ca_head_dim,\n",
    "                                 self.ca_kv_heads * self.ca_head_dim,\n",
    "                                 self.ca_kv_heads * self.ca_head_dim],dim = -1)\n",
    "        return Wq, Wk, Wv\n",
    "        \n",
    "    @log_io\n",
    "    def RoPE(self, xq, xk):\n",
    "        xq = RoPE(xq, self.ca_head_dim, self.theta)\n",
    "        xk = RoPE(xk, self.ca_head_dim, self.theta)\n",
    "        return xq, xk\n",
    "\n",
    "    @log_io\n",
    "    def match_headcount(self, xk, xv):\n",
    "        xk = torch.repeat_interleave(xk, self.num_queries_per_kv, dim=2)\n",
    "        xv = torch.repeat_interleave(xv, self.num_queries_per_kv, dim=2)\n",
    "        return xk, xv\n",
    "\n",
    "    @log_io\n",
    "    def attend(self, xq, ck):\n",
    "        return torch.matmul(xq, ck.transpose(2, 3)) * (self.ca_head_dim ** -0.5)\n",
    "        \n",
    "    @log_io\n",
    "    def apply_mask(self, logits, input_len_x, input_len_c):\n",
    "        self.mask = torch.triu(torch.ones((config.max_seq_len, input_len_c), dtype=torch.uint8)\n",
    "                                  ).view(1, 1, config.max_seq_len, input_len_c).to(dtype=torch.bool)\n",
    "        \n",
    "        return torch.where(self.mask[..., :input_len_x, :input_len_c].expand_as(logits),\n",
    "                           logits,\n",
    "                           torch.tensor(-1e30, device=logits.device, dtype=logits.dtype))\n",
    "    \n",
    "    @log_io\n",
    "    def calc_output(self, logits, cv, batch_size, input_len_x, training):\n",
    "        # Applies softmax to the logits to obtain attention probabilities\n",
    "        scores = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # also applies dropout if we're training\n",
    "        scores = F.dropout(scores, p=self.dropout_rate, training=training)\n",
    "        \n",
    "        # Computes the weighted sum of values based on the attention scores to obtain the output of the attention mechanism.\n",
    "        output = scores @ cv # [batch_size, n_local_heads, input_len, sa_head_dim]\n",
    "\n",
    "        # Reshapes the attention output to match the expected output dimensions, combining the heads back into the hidden dimension.\n",
    "        return output.transpose(1, 2).contiguous().view(batch_size, input_len_x, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2731e8f-023c-49de-8de8-d2b42d2833c4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## demonstration/debugging\n",
    "\n",
    "at one point this one had been giving me trouble so here's multiple different config.levels setups for ya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c5f9bf9-d02c-4d46-9070-217a4bbdb1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering crossMQA.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 256, 256])\n",
      "Tensor 'c' shape: torch.Size([32, 64, 256])\n",
      "\n",
      "==========Entering crossMQA.weight_splicing==========\n",
      "Inputs:\n",
      "Tensor 'Wqkv' shape: torch.Size([256, 256])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([256, 128])\n",
      "Tensor 'output[1]' shape: torch.Size([256, 64])\n",
      "Tensor 'output[2]' shape: torch.Size([256, 64])\n",
      "==========Exiting crossMQA.weight_splicing==========\n",
      "\n",
      "==========Entering crossMQA.match_headcount==========\n",
      "Inputs:\n",
      "Tensor 'xk' shape: torch.Size([32, 64, 1, 64])\n",
      "Tensor 'xv' shape: torch.Size([32, 64, 1, 64])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([32, 64, 2, 64])\n",
      "Tensor 'output[1]' shape: torch.Size([32, 64, 2, 64])\n",
      "==========Exiting crossMQA.match_headcount==========\n",
      "\n",
      "==========Entering crossMQA.attend==========\n",
      "Inputs:\n",
      "Tensor 'xq' shape: torch.Size([32, 2, 256, 64])\n",
      "Tensor 'ck' shape: torch.Size([32, 2, 64, 64])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 2, 256, 64])\n",
      "==========Exiting crossMQA.attend==========\n",
      "\n",
      "==========Entering crossMQA.calc_output==========\n",
      "Inputs:\n",
      "Tensor 'logits' shape: torch.Size([32, 2, 256, 64])\n",
      "Tensor 'cv' shape: torch.Size([32, 2, 64, 64])\n",
      "Integer 'batch_size': Value=32\n",
      "Integer 'input_len_x': Value=256\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 256, 128])\n",
      "==========Exiting crossMQA.calc_output==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 256, 256])\n",
      "==========Exiting crossMQA.forward==========\n"
     ]
    }
   ],
   "source": [
    "hold1 = config.levels\n",
    "config.levels = 2\n",
    "hold2 = config.predictive_mask\n",
    "config.predictive_mask = False\n",
    "module = crossMQA(config)\n",
    "module.enable_logging()\n",
    "\n",
    "### Optionally disabling printing for sub-functions\n",
    "#module.disable_function_logging('weight_splicing')\n",
    "#module.disable_function_logging('RoPE')\n",
    "#module.disable_function_logging('match_headcount')\n",
    "#module.disable_function_logging('attend')\n",
    "#module.disable_function_logging('apply_mask')\n",
    "#module.disable_function_logging('calc_output')\n",
    "\n",
    "x0 = torch.randn(32, config.max_seq_len, config.embed_dim)\n",
    "c1 = torch.randn(32, config.max_seq_len // config.combine_factor, config.embed_dim)\n",
    "output = module(x0, c1)\n",
    "config.levels = hold1\n",
    "config.predictive_mask = hold2\n",
    "del hold1, hold2, module, x0, c1, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "129b2d43-eaf2-4c87-a186-61199e76d9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering crossMQA.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 64, 256])\n",
      "Tensor 'c' shape: torch.Size([32, 16, 256])\n",
      "\n",
      "==========Entering crossMQA.weight_splicing==========\n",
      "Inputs:\n",
      "Tensor 'Wqkv' shape: torch.Size([256, 256])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([256, 128])\n",
      "Tensor 'output[1]' shape: torch.Size([256, 64])\n",
      "Tensor 'output[2]' shape: torch.Size([256, 64])\n",
      "==========Exiting crossMQA.weight_splicing==========\n",
      "\n",
      "==========Entering crossMQA.match_headcount==========\n",
      "Inputs:\n",
      "Tensor 'xk' shape: torch.Size([32, 16, 1, 64])\n",
      "Tensor 'xv' shape: torch.Size([32, 16, 1, 64])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([32, 16, 2, 64])\n",
      "Tensor 'output[1]' shape: torch.Size([32, 16, 2, 64])\n",
      "==========Exiting crossMQA.match_headcount==========\n",
      "\n",
      "==========Entering crossMQA.attend==========\n",
      "Inputs:\n",
      "Tensor 'xq' shape: torch.Size([32, 2, 64, 64])\n",
      "Tensor 'ck' shape: torch.Size([32, 2, 16, 64])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 2, 64, 16])\n",
      "==========Exiting crossMQA.attend==========\n",
      "\n",
      "==========Entering crossMQA.calc_output==========\n",
      "Inputs:\n",
      "Tensor 'logits' shape: torch.Size([32, 2, 64, 16])\n",
      "Tensor 'cv' shape: torch.Size([32, 2, 16, 64])\n",
      "Integer 'batch_size': Value=32\n",
      "Integer 'input_len_x': Value=64\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 64, 128])\n",
      "==========Exiting crossMQA.calc_output==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 64, 256])\n",
      "==========Exiting crossMQA.forward==========\n"
     ]
    }
   ],
   "source": [
    "hold1 = config.levels\n",
    "config.levels = 3\n",
    "hold2 = config.predictive_mask\n",
    "config.predictive_mask = False\n",
    "module = crossMQA(config)\n",
    "module.enable_logging()\n",
    "\n",
    "### Optionally disabling printing for sub-functions\n",
    "#module.disable_function_logging('weight_splicing')\n",
    "#module.disable_function_logging('RoPE')\n",
    "#module.disable_function_logging('match_headcount')\n",
    "#module.disable_function_logging('attend')\n",
    "#module.disable_function_logging('apply_mask')\n",
    "#module.disable_function_logging('calc_output')\n",
    "\n",
    "c1 = torch.randn(32, config.max_seq_len // (config.combine_factor**1), config.embed_dim)\n",
    "c2 = torch.randn(32, config.max_seq_len // (config.combine_factor**2), config.embed_dim)\n",
    "output = module(c1, c2)\n",
    "config.levels = hold1\n",
    "config.predictive_mask = hold2\n",
    "del hold1, hold2, module, c1, c2, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5af750-9441-43c2-af37-728c7323ad34",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## working on crossMQA predictive mask\n",
    "\n",
    "notice how when i do the live-splicing of the mask, only the lower level actually needs live splicing since the upper level will always be witnessing the full context length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf42671a-87a7-4c0d-97b8-de0e2744fe98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_number 0\n",
      "x: torch.Size([1, 20, 1, 4])\n",
      "tensor([[[[-1.7051,  0.4757,  1.3667,  2.3230]],\n",
      "\n",
      "         [[-0.4236,  1.1244, -0.2000,  1.1232]],\n",
      "\n",
      "         [[ 1.2987, -1.2002,  0.8761,  0.3183]],\n",
      "\n",
      "         [[-0.5328,  0.0147,  1.2374,  1.2683]],\n",
      "\n",
      "         [[ 1.7717,  2.0656,  0.9818, -0.2135]],\n",
      "\n",
      "         [[ 0.2059, -0.3381, -0.3381,  0.7264]],\n",
      "\n",
      "         [[ 0.1274,  0.1418, -0.8047, -1.3830]],\n",
      "\n",
      "         [[ 2.6674, -0.1535,  0.0233,  0.0684]],\n",
      "\n",
      "         [[-1.5625,  1.1472, -1.7107, -0.6834]],\n",
      "\n",
      "         [[ 0.1548, -0.6723,  0.9502, -0.0697]],\n",
      "\n",
      "         [[-1.1599, -0.6320,  0.7380, -0.8934]],\n",
      "\n",
      "         [[ 1.0836,  0.3302, -0.7806, -0.3407]],\n",
      "\n",
      "         [[ 1.4597, -0.9352,  0.2811, -0.3279]],\n",
      "\n",
      "         [[-0.2260,  0.1323, -0.6450,  1.1478]],\n",
      "\n",
      "         [[ 0.6980, -1.5263, -0.1852,  0.4709]],\n",
      "\n",
      "         [[ 1.1317, -0.1370, -0.3752, -1.3378]],\n",
      "\n",
      "         [[-0.9330,  0.6437, -0.4607,  0.0485]],\n",
      "\n",
      "         [[-1.1409, -0.7679, -0.8752,  0.1486]],\n",
      "\n",
      "         [[ 0.3966,  0.3267,  0.1775, -0.8950]],\n",
      "\n",
      "         [[-1.0743,  0.0627, -3.1296, -1.6897]]]])\n",
      "c: torch.Size([1, 5, 1, 4])\n",
      "tensor([[[[ 1.7771, -0.7861,  1.7299,  1.1830]],\n",
      "\n",
      "         [[-0.5856,  2.6073, -1.3054, -1.2741]],\n",
      "\n",
      "         [[ 1.6421, -1.6871,  0.8962,  1.3020]],\n",
      "\n",
      "         [[-0.8004, -0.9013,  0.7939,  1.6125]],\n",
      "\n",
      "         [[ 0.6070,  0.8307, -0.4564, -1.3072]]]])\n",
      "logits: torch.Size([1, 1, 20, 5])\n",
      "tensor([[[[ 1.7082, -2.5050,  0.6469,  5.7670, -4.3002],\n",
      "          [-0.6539,  2.0098, -1.3095,  0.9780, -0.7000],\n",
      "          [ 5.1433, -5.4389,  5.3570,  1.2509, -1.0247],\n",
      "          [ 2.6825, -2.8809,  1.8606,  3.4407, -2.5338],\n",
      "          [ 2.9706,  3.3383,  0.0264, -2.8446,  2.6221],\n",
      "          [ 0.9061, -1.4863,  1.5513,  1.0428, -0.9510],\n",
      "          [-2.9131,  3.1076, -2.5518, -3.0987,  2.3702],\n",
      "          [ 4.9821, -2.0800,  4.7490, -1.8679,  1.3914],\n",
      "          [-7.4461,  7.0098, -6.9241, -2.2432,  1.6786],\n",
      "          [ 2.3648, -2.9950,  2.1492,  1.1239, -0.8071],\n",
      "          [-1.3446, -0.7937, -1.3402,  0.6433, -0.3980],\n",
      "          [-0.0874,  1.6795,  0.0790, -2.3341,  1.7337],\n",
      "          [ 3.4276, -3.2424,  3.7998, -0.6311,  0.4094],\n",
      "          [-0.2636, -0.1432,  0.3220,  1.4004, -1.2332],\n",
      "          [ 2.6769, -4.7465,  4.1683,  1.4293, -1.3753],\n",
      "          [-0.1128,  1.1743,  0.0114, -3.2374,  2.4931],\n",
      "          [-2.9035,  2.7643, -2.9678, -0.1208,  0.1153],\n",
      "          [-2.7619, -0.3810, -1.1687,  1.1501, -1.1252],\n",
      "          [-0.3037,  1.5282, -0.9061, -1.9141,  1.6010],\n",
      "          [-9.3711,  7.0308, -6.8747, -4.4057,  3.0371]]]])\n",
      "mask_init: torch.Size([1, 1, 4, 4])\n",
      "tensor([[[[1., 1., 1., 1.],\n",
      "          [0., 1., 1., 1.],\n",
      "          [0., 0., 1., 1.],\n",
      "          [0., 0., 0., 1.]]]])\n",
      "mask_init: torch.Size([1, 1, 16, 4])\n",
      "tensor([[[[1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.],\n",
      "          [0., 1., 1., 1.],\n",
      "          [0., 1., 1., 1.],\n",
      "          [0., 1., 1., 1.],\n",
      "          [0., 1., 1., 1.],\n",
      "          [0., 0., 1., 1.],\n",
      "          [0., 0., 1., 1.],\n",
      "          [0., 0., 1., 1.],\n",
      "          [0., 0., 1., 1.],\n",
      "          [0., 0., 0., 1.],\n",
      "          [0., 0., 0., 1.],\n",
      "          [0., 0., 0., 1.],\n",
      "          [0., 0., 0., 1.]]]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 1, 20, 5]' is invalid for input of size 64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m mask_init \u001b[38;5;241m=\u001b[39m mask_init\u001b[38;5;241m.\u001b[39mrepeat_interleave(combine_factor, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmask_init: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmask_init\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmask_init\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m mask \u001b[38;5;241m=\u001b[39m mask_init\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m, offset \u001b[38;5;241m+\u001b[39m input_len, (input_len \u001b[38;5;241m+\u001b[39m combine_factor) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m combine_factor)\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbool)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmask: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmask\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     32\u001b[0m masked_logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(mask\u001b[38;5;241m.\u001b[39mexpand_as(logits),\u001b[38;5;66;03m#[-1,-1,logits.shape[2],logits.shape[3]]\u001b[39;00m\n\u001b[1;32m     33\u001b[0m                             logits,\n\u001b[1;32m     34\u001b[0m                             torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1e30\u001b[39m, device\u001b[38;5;241m=\u001b[39mlogits\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mlogits\u001b[38;5;241m.\u001b[39mdtype))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 1, 20, 5]' is invalid for input of size 64"
     ]
    }
   ],
   "source": [
    "combine_factor = 4\n",
    "max_seq_len = 16\n",
    "random_number = random.randint(0, combine_factor-1)\n",
    "print(f'random_number {random_number}')\n",
    "input_len = max_seq_len - random_number\n",
    "h = 1\n",
    "head_dim = 4\n",
    "b = 1\n",
    "\n",
    "offset = combine_factor - (input_len % combine_factor)\n",
    "x = torch.randn(b, offset + input_len, h, head_dim)\n",
    "print(f'x: {x.shape}\\n{x}')\n",
    "\n",
    "c = torch.randn(b, (input_len + combine_factor) // combine_factor, h, head_dim)\n",
    "print(f'c: {c.shape}\\n{c}')\n",
    "\n",
    "x = x.transpose(1, 2)\n",
    "c = c.transpose(1, 2)\n",
    "\n",
    "logits = x @ c.transpose(2,3)\n",
    "print(f'logits: {logits.shape}\\n{logits}')\n",
    "\n",
    "mask_init = torch.triu(torch.ones(b,h, max_seq_len // combine_factor, max_seq_len // combine_factor))#, diagonal=1)\n",
    "print(f'mask_init: {mask_init.shape}\\n{mask_init}')\n",
    "\n",
    "mask_init = mask_init.repeat_interleave(combine_factor, dim=2)\n",
    "print(f'mask_init: {mask_init.shape}\\n{mask_init}')\n",
    "\n",
    "mask = mask_init.view(1,1, offset + input_len, (input_len + combine_factor) // combine_factor).to(dtype=torch.bool)\n",
    "print(f'mask: {mask.shape}\\n{mask}')\n",
    "\n",
    "masked_logits = torch.where(mask.expand_as(logits),#[-1,-1,logits.shape[2],logits.shape[3]]\n",
    "                            logits,\n",
    "                            torch.tensor(-1e30, device=logits.device, dtype=logits.dtype))\n",
    "print(f'masked_logits: {masked_logits.shape}\\n{masked_logits}')\n",
    "\n",
    "scores = F.softmax(masked_logits, dim=-1)\n",
    "print(f'scores: {scores.shape}\\n{scores}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0a693737-aa3b-4bd1-aee1-0c115eff9092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering crossMQA.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 256, 256])\n",
      "Tensor 'c' shape: torch.Size([32, 64, 256])\n",
      "\n",
      "==========Entering crossMQA.weight_splicing==========\n",
      "Inputs:\n",
      "Tensor 'Wqkv' shape: torch.Size([256, 256])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([256, 128])\n",
      "Tensor 'output[1]' shape: torch.Size([256, 64])\n",
      "Tensor 'output[2]' shape: torch.Size([256, 64])\n",
      "==========Exiting crossMQA.weight_splicing==========\n",
      "\n",
      "==========Entering crossMQA.match_headcount==========\n",
      "Inputs:\n",
      "Tensor 'xn' shape: torch.Size([32, 64, 1, 64])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 64, 2, 64])\n",
      "==========Exiting crossMQA.match_headcount==========\n",
      "\n",
      "==========Entering crossMQA.match_headcount==========\n",
      "Inputs:\n",
      "Tensor 'xn' shape: torch.Size([32, 64, 1, 64])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 64, 2, 64])\n",
      "==========Exiting crossMQA.match_headcount==========\n",
      "\n",
      "==========Entering crossMQA.attend==========\n",
      "Inputs:\n",
      "Tensor 'xq' shape: torch.Size([32, 2, 256, 64])\n",
      "Tensor 'ck' shape: torch.Size([32, 2, 64, 64])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 2, 256, 64])\n",
      "==========Exiting crossMQA.attend==========\n",
      "\n",
      "==========Entering crossMQA.apply_mask==========\n",
      "Inputs:\n",
      "Tensor 'logits' shape: torch.Size([32, 2, 256, 64])\n",
      "Integer 'input_len': Value=256\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (64) must match the existing size (256) at non-singleton dimension 3.  Target sizes: [32, 2, 256, 64].  Tensor sizes: [1, 1, 256, 256]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m x0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m32\u001b[39m, config\u001b[38;5;241m.\u001b[39mmax_seq_len, config\u001b[38;5;241m.\u001b[39membed_dim)\n\u001b[1;32m     17\u001b[0m c1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m32\u001b[39m, config\u001b[38;5;241m.\u001b[39mmax_seq_len \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m config\u001b[38;5;241m.\u001b[39mcombo, config\u001b[38;5;241m.\u001b[39membed_dim)\n\u001b[0;32m---> 18\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m config\u001b[38;5;241m.\u001b[39mlevels \u001b[38;5;241m=\u001b[39m hold1\n\u001b[1;32m     20\u001b[0m config\u001b[38;5;241m.\u001b[39mpredictive_mask \u001b[38;5;241m=\u001b[39m hold2\n",
      "File \u001b[0;32m~/Documents/next-concept-predictor/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/next-concept-predictor/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 39\u001b[0m, in \u001b[0;36mlog_io.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(arg_names, arg_values):\n\u001b[1;32m     37\u001b[0m     log_item(value, name)\n\u001b[0;32m---> 39\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOutputs:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mtuple\u001b[39m):\n",
      "Cell \u001b[0;32mIn[54], line 90\u001b[0m, in \u001b[0;36mcrossMQA.forward\u001b[0;34m(self, x, c, training)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Optionally applies the upper-triangular mask to the attention logits\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 90\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_len_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# applies values to get final output\u001b[39;00m\n\u001b[1;32m     93\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalc_output(logits, cv, batch_size, input_len_x, training)\n",
      "Cell \u001b[0;32mIn[3], line 39\u001b[0m, in \u001b[0;36mlog_io.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(arg_names, arg_values):\n\u001b[1;32m     37\u001b[0m     log_item(value, name)\n\u001b[0;32m---> 39\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOutputs:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mtuple\u001b[39m):\n",
      "Cell \u001b[0;32mIn[54], line 121\u001b[0m, in \u001b[0;36mcrossMQA.apply_mask\u001b[0;34m(self, logits, input_len)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;129m@log_io\u001b[39m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_mask\u001b[39m(\u001b[38;5;28mself\u001b[39m, logits, input_len):\n\u001b[0;32m--> 121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mwhere(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43minput_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43minput_len\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_as\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    122\u001b[0m                        logits,\n\u001b[1;32m    123\u001b[0m                        torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1e30\u001b[39m, device\u001b[38;5;241m=\u001b[39mlogits\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mlogits\u001b[38;5;241m.\u001b[39mdtype))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (64) must match the existing size (256) at non-singleton dimension 3.  Target sizes: [32, 2, 256, 64].  Tensor sizes: [1, 1, 256, 256]"
     ]
    }
   ],
   "source": [
    "hold1 = config.levels\n",
    "config.levels = 2\n",
    "hold2 = config.predictive_mask\n",
    "config.predictive_mask = True\n",
    "module = crossMQA(config)\n",
    "module.enable_logging()\n",
    "\n",
    "### Optionally disabling printing for sub-functions\n",
    "#module.disable_function_logging('weight_splicing')\n",
    "#module.disable_function_logging('RoPE')\n",
    "#module.disable_function_logging('match_headcount')\n",
    "#module.disable_function_logging('attend')\n",
    "#module.disable_function_logging('apply_mask')\n",
    "#module.disable_function_logging('calc_output')\n",
    "\n",
    "x0 = torch.randn(32, config.max_seq_len, config.embed_dim)\n",
    "c1 = torch.randn(32, config.max_seq_len // config.combine_factor, config.embed_dim)\n",
    "output = module(x0, c1)\n",
    "config.levels = hold1\n",
    "config.predictive_mask = hold2\n",
    "del hold1, hold2, module, x0, c1, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5719f276-53b7-4c66-b4da-04231f8d6c38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc8505c5-d859-4e48-bdd9-e88746a3f88f",
   "metadata": {},
   "source": [
    "# Layer\n",
    "\n",
    "we implement cross-attention inbetween self-attention and MLP like is done in Attention is All You Need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ca50177-b795-4a4e-8535-c5f00e161206",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(LoggingModule):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pre_self_mqa_norm = Norm(config)\n",
    "        self.self_mqa = selfMQA(config)\n",
    "        \n",
    "        self.pre_cross_mqa_x_norm = Norm(config)\n",
    "        self.pre_cross_mqa_c_norm = Norm(config)\n",
    "        self.cross_mqa = crossMQA(config)\n",
    "        \n",
    "        self.pre_mlp_norm = Norm(config)\n",
    "        self.mlp = MLP(config.embed_dim, config.mlp_multiplier, config.embed_dim, config.dropout_rate)\n",
    "\n",
    "    @log_io\n",
    "    def forward(self, \n",
    "                x: torch.Tensor,\n",
    "                c: torch.Tensor = None,\n",
    "                training: bool = False,\n",
    "               ) -> torch.Tensor:\n",
    "        x = x + self.self_mqa_connection(x, training)\n",
    "        if c is not None:\n",
    "            x = x + self.cross_mqa_connection(x, c, training)\n",
    "        x = x + self.mlp_connection(x, training)\n",
    "        return x\n",
    "\n",
    "    @log_io\n",
    "    def self_mqa_connection(self, x, training):\n",
    "        return self.self_mqa(self.pre_self_mqa_norm(x, training), training)\n",
    "\n",
    "    @log_io\n",
    "    def cross_mqa_connection(self, x, c, training):\n",
    "        return self.cross_mqa(self.pre_cross_mqa_x_norm(x, training), \n",
    "                              self.pre_cross_mqa_c_norm(c, training), training)\n",
    "    @log_io\n",
    "    def mlp_connection(self, x, training):\n",
    "        return self.mlp(self.pre_mlp_norm(x, training), training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711f6f6d-9997-4196-a88e-b8bf31196c85",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## demonstration/debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6a6444-0211-4adb-9a17-7cfe21d140ec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### no predictive mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d2a1bcb4-8464-4452-ac40-c6eb35fbf223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering Layer.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 128, 64])\n",
      "\n",
      "==========Entering Layer.self_mqa_connection==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 128, 64])\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 128, 64])\n",
      "==========Exiting Layer.self_mqa_connection==========\n",
      "\n",
      "==========Entering Layer.mlp_connection==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 128, 64])\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 128, 64])\n",
      "==========Exiting Layer.mlp_connection==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 128, 64])\n",
      "==========Exiting Layer.forward==========\n"
     ]
    }
   ],
   "source": [
    "module = Layer(config)\n",
    "module.enable_logging()\n",
    "\n",
    "### enabling printing for sub-modules\n",
    "#module.pre_self_mqa_norm.enable_logging()\n",
    "#module.self_mqa.enable_logging()\n",
    "#module.post_self_mqa_norm.enable_logging()\n",
    "#module.pre_cross_mqa_norm.enable_logging()\n",
    "#module.cross_mqa.enable_logging()\n",
    "#module.post_cross_mqa_norm.enable_logging()\n",
    "#module.pre_mlp_norm.enable_logging()\n",
    "#module.mlp.enable_logging()\n",
    "#module.post_mlp_norm.enable_logging()\n",
    "\n",
    "### disabling printing for sub-functions\n",
    "#module.disable_function_logging('self_mqa_connection')\n",
    "#module.disable_function_logging('cross_mqa_connection')\n",
    "#module.disable_function_logging('mlp_connection')\n",
    "\n",
    "output = module(torch.randn(32,config.max_seq_len,config.embed_dim))\n",
    "del module, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7853cc93-c55e-40fd-aa38-1a2a1f1f230f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering Layer.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 128, 64])\n",
      "Tensor 'c' shape: torch.Size([32, 32, 64])\n",
      "\n",
      "==========Entering Layer.self_mqa_connection==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 128, 64])\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 128, 64])\n",
      "==========Exiting Layer.self_mqa_connection==========\n",
      "\n",
      "==========Entering Layer.cross_mqa_connection==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 128, 64])\n",
      "Tensor 'c' shape: torch.Size([32, 32, 64])\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 128, 64])\n",
      "==========Exiting Layer.cross_mqa_connection==========\n",
      "\n",
      "==========Entering Layer.mlp_connection==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 128, 64])\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 128, 64])\n",
      "==========Exiting Layer.mlp_connection==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 128, 64])\n",
      "==========Exiting Layer.forward==========\n"
     ]
    }
   ],
   "source": [
    "hold = config.predictive_mask\n",
    "config.predictive_mask = False\n",
    "module = Layer(config)\n",
    "module.enable_logging()\n",
    "\n",
    "### enabling printing for sub-modules\n",
    "#module.pre_self_mqa_norm.enable_logging()\n",
    "#module.self_mqa.enable_logging()\n",
    "#module.post_self_mqa_norm.enable_logging()\n",
    "#module.pre_cross_mqa_norm.enable_logging()\n",
    "#module.cross_mqa.enable_logging()\n",
    "#module.post_cross_mqa_norm.enable_logging()\n",
    "#module.pre_mlp_norm.enable_logging()\n",
    "#module.mlp.enable_logging()\n",
    "#module.post_mlp_norm.enable_logging()\n",
    "\n",
    "### disabling printing for sub-functions\n",
    "#module.disable_function_logging('self_mqa_connection')\n",
    "#module.disable_function_logging('cross_mqa_connection')\n",
    "#module.disable_function_logging('mlp_connection')\n",
    "\n",
    "x = torch.randn(32, config.max_seq_len, config.embed_dim)\n",
    "c = torch.randn(32, config.max_seq_len // config.combine_factor, config.embed_dim)\n",
    "output = module(x, c)\n",
    "config.predictive_mask = hold\n",
    "del hold, module, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f415280-2108-4a7a-a518-a59d64e98463",
   "metadata": {},
   "source": [
    "### prolly need to do more debugging once predictive mask works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d11636-7958-4e38-91a4-00ec0d5949f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ed7034-9822-4cfa-93a6-cde136e95203",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c27dbad8-dca3-4e88-a75a-7feec206a775",
   "metadata": {},
   "source": [
    "# embedding vector combination function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f090e3eb-9251-49c0-ab54-b1bcd362e69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumModule(LoggingModule): \n",
    "    @log_io\n",
    "    def forward(self, x):\n",
    "        return x.sum(dim=2)\n",
    "class MeanModule(LoggingModule):    \n",
    "    @log_io\n",
    "    def forward(self, x):\n",
    "        return x.mean(dim=2)\n",
    "class MaxModule(LoggingModule):    \n",
    "    @log_io\n",
    "    def forward(self, x):\n",
    "        return x.max(dim=2).values\n",
    "class ReshapeModule(LoggingModule):\n",
    "    @log_io\n",
    "    def forward(self, x):\n",
    "        return x.reshape(x.shape[0], x.shape[1], -1)\n",
    "\n",
    "class CombineEmbeddings(LoggingModule):\n",
    "    def __init__(self, config: Config, padding_vector: torch.Tensor):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.embed_dim\n",
    "        self.combine_factor = config.combine_factor\n",
    "        #self.padding_vector = nn.Parameter(torch.zeros(config.embed_dim), requires_grad=True)\n",
    "        self.padding_vector = padding_vector\n",
    "\n",
    "        # Initialize operation chain\n",
    "        self.operation_chain = nn.Sequential()\n",
    "        operation_map = {\n",
    "            \"sum\": SumModule(), # cannot go before or after reshape\n",
    "            \"mean\": MeanModule(), # cannot go before or after reshape\n",
    "            \"max\": MaxModule(), # cannot go before or after reshape\n",
    "            \"norm\": Norm(config), # cannot go before or after reshape\n",
    "            \"linear_post_reshape\": nn.Linear(config.embed_dim * config.combine_factor, config.embed_dim), # must go after reshape\n",
    "            \"linear\": nn.Linear(config.embed_dim, config.embed_dim), # cannot go after reshape\n",
    "            \"mlp_post_reshape\": MLP(config.embed_dim * config.combine_factor, config.mlp_multiplier, config.embed_dim, config.dropout_rate), # must go after reshape\n",
    "            \"mlp\": MLP(config.embed_dim, config.mlp_multiplier, config.embed_dim, config.dropout_rate), # cannot go after reshape\n",
    "            \"reshape\": ReshapeModule()}\n",
    "        index = 0  # Initialize a counter to create unique module names\n",
    "        for op in config.combine_type.split(\"->\"):\n",
    "            if op in operation_map:\n",
    "                unique_op_name = f\"{op}_{index}\"  # Append the index to the operation name\n",
    "                self.operation_chain.add_module(unique_op_name, operation_map[op])\n",
    "                index += 1  # Increment the index for the next operation\n",
    "            else:\n",
    "                raise ValueError(f\"Pooling operation {op} is not a valid operation\")\n",
    "\n",
    "    \n",
    "    @log_io\n",
    "    def forward(self, tensor, combine_factor):\n",
    "        # this function will apply itself recursively for higher levels\n",
    "        assert combine_factor % self.combine_factor == 0, f'combine factor={combine_factor} is not a multiple of config.combine_factor={config.combine_factor}'\n",
    "        if combine_factor // self.combine_factor != 1:\n",
    "            tensor = self.forward(tensor, combine_factor // self.combine_factor)\n",
    "            \n",
    "        b, t, d = tensor.shape\n",
    "            \n",
    "        # Calculate the necessary amount of padding\n",
    "        remainder = t % self.combine_factor\n",
    "        padding_needed = 0 if remainder == 0 else self.combine_factor - remainder\n",
    "        \n",
    "        if padding_needed > 0:\n",
    "            # Replicate the padding vector the necessary number of times\n",
    "            padding = self.padding_vector.repeat(padding_needed, 1).unsqueeze(0).expand(b, -1, -1)\n",
    "            tensor = torch.cat([padding, tensor], dim=1)\n",
    "        \n",
    "        # Update t after padding\n",
    "        t_padded = t + padding_needed\n",
    "        \n",
    "        # Reshape the tensor to group 'combine_factor' entries along the t dimension\n",
    "        reshaped_tensor = tensor.view(b, t_padded // self.combine_factor, self.combine_factor, d)\n",
    "        \n",
    "        # the actual combination operation\n",
    "        combined_tensor = self.operation_chain(reshaped_tensor)\n",
    "        assert combined_tensor.shape[0] == b, f\"b={b}; pooling operation order is invalid. output shape: {combined_tensor.shape}\"\n",
    "        assert combined_tensor.shape[1] == t_padded // self.combine_factor, f\"t={t_padded // self.combine_factor}; pooling operation order is invalid. output shape: {combined_tensor.shape}\"\n",
    "        assert combined_tensor.shape[2] == self.embed_dim, f\"d={self.embed_dim}; pooling operation order is invalid. output shape: {combined_tensor.shape}\"\n",
    "        \n",
    "        return combined_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2121c0df-211e-4e88-a590-c37a8d0c369d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## demonstration/debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa9bdab-75e5-41aa-8a53-1427eb9c82af",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### testing different pooling types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ffa025f0-b379-4e56-b7d4-d9833a354a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering CombineEmbeddings.forward==========\n",
      "Inputs:\n",
      "Tensor 'tensor' shape: torch.Size([32, 128, 128])\n",
      "Integer 'combine_factor': Value=4\n",
      "\n",
      "==========Entering MLP.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 32, 4, 128])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 32, 4, 128])\n",
      "==========Exiting MLP.forward==========\n",
      "\n",
      "==========Entering SumModule.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 32, 4, 128])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 32, 128])\n",
      "==========Exiting SumModule.forward==========\n",
      "\n",
      "==========Entering MLP.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 32, 128])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 32, 128])\n",
      "==========Exiting MLP.forward==========\n",
      "\n",
      "==========Entering Norm.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 32, 128])\n",
      "\n",
      "==========Entering Norm.RMSNorm==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 32, 128])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 32, 128])\n",
      "==========Exiting Norm.RMSNorm==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 32, 128])\n",
      "==========Exiting Norm.forward==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 32, 128])\n",
      "==========Exiting CombineEmbeddings.forward==========\n"
     ]
    }
   ],
   "source": [
    "hold1 = config.combine_type\n",
    "hold2 = config.combine_factor\n",
    "config.combine_type = 'mlp->sum->mlp->norm'\n",
    "config.combine_factor = 4\n",
    "padding_vector = nn.Parameter(torch.zeros(config.embed_dim), requires_grad=True)\n",
    "module = CombineEmbeddings(config, padding_vector)\n",
    "\n",
    "module.enable_logging()\n",
    "module.operation_chain.mlp_0.enable_logging()\n",
    "module.operation_chain.sum_1.enable_logging()\n",
    "module.operation_chain.mlp_2.enable_logging()\n",
    "module.operation_chain.norm_3.enable_logging()\n",
    "\n",
    "x = torch.randn(32, config.max_seq_len, config.embed_dim)\n",
    "output = module(x, config.combine_factor)\n",
    "config.combine_type = hold1\n",
    "config.combine_factor = hold2\n",
    "del padding_vector, module, hold1, hold2, x, output\n",
    "module = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c8d33d20-820f-4044-a42e-636f638add1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering CombineEmbeddings.forward==========\n",
      "Inputs:\n",
      "Tensor 'tensor' shape: torch.Size([32, 128, 128])\n",
      "Integer 'combine_factor': Value=2\n",
      "\n",
      "==========Entering ReshapeModule.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 64, 2, 128])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 64, 256])\n",
      "==========Exiting ReshapeModule.forward==========\n",
      "\n",
      "==========Entering MLP.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 64, 256])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 64, 128])\n",
      "==========Exiting MLP.forward==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 64, 128])\n",
      "==========Exiting CombineEmbeddings.forward==========\n"
     ]
    }
   ],
   "source": [
    "hold1 = config.combine_type\n",
    "hold2 = config.combine_factor\n",
    "config.combine_type = 'reshape->mlp_post_reshape'\n",
    "config.combine_factor = 2\n",
    "padding_vector = nn.Parameter(torch.zeros(config.embed_dim), requires_grad=True)\n",
    "module = CombineEmbeddings(config, padding_vector)\n",
    "\n",
    "module.enable_logging()\n",
    "module.operation_chain.reshape_0.enable_logging()\n",
    "module.operation_chain.mlp_post_reshape_1.enable_logging()\n",
    "\n",
    "x = torch.randn(32, config.max_seq_len, config.embed_dim)\n",
    "output = module(x, config.combine_factor)\n",
    "config.combine_type = hold1\n",
    "config.combine_factor = hold2\n",
    "del padding_vector, module, hold1, hold2, x, output\n",
    "module = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16e0a66-4760-4ef1-ae85-704c5a741e16",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### off-lengths that'll require padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d4fea04e-c127-4285-97b2-821ea9c5d206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering CombineEmbeddings.forward==========\n",
      "Inputs:\n",
      "Tensor 'tensor' shape: torch.Size([32, 127, 128])\n",
      "Integer 'combine_factor': Value=2\n",
      "\n",
      "==========Entering ReshapeModule.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 64, 2, 128])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 64, 256])\n",
      "==========Exiting ReshapeModule.forward==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 64, 128])\n",
      "==========Exiting CombineEmbeddings.forward==========\n"
     ]
    }
   ],
   "source": [
    "hold1 = config.combine_type\n",
    "hold2 = config.combine_factor\n",
    "config.combine_type = 'reshape->linear_post_reshape'\n",
    "config.combine_factor = 2\n",
    "padding_vector = nn.Parameter(torch.zeros(config.embed_dim), requires_grad=True)\n",
    "module = CombineEmbeddings(config, padding_vector)\n",
    "\n",
    "module.enable_logging()\n",
    "module.operation_chain.reshape_0.enable_logging()\n",
    "# the linear is here it's just not compatible with our printing and i'm too lazy to build a wrapper for it\n",
    "\n",
    "x = torch.randn(32, config.max_seq_len-1, config.embed_dim)\n",
    "output = module(x, config.combine_factor)\n",
    "config.combine_type = hold1\n",
    "config.combine_factor = hold2\n",
    "del padding_vector, module, hold1, hold2, x, output\n",
    "module = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5feb9312-5f35-4837-ba14-3c2aaee12a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.2387, -1.1476,  0.0282, -1.5950, -1.6658,  0.6434, -0.7313,  0.3435])\n",
      "\n",
      "==========Entering CombineEmbeddings.forward==========\n",
      "Inputs:\n",
      "Tensor 'tensor' shape: torch.Size([32, 125, 128])\n",
      "Integer 'combine_factor': Value=4\n",
      "\n",
      "==========Entering MaxModule.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 32, 4, 128])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 32, 128])\n",
      "==========Exiting MaxModule.forward==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 32, 128])\n",
      "==========Exiting CombineEmbeddings.forward==========\n",
      "tensor([0.2387, 0.0000, 0.0282, 0.0000, 0.0000, 0.6434, 0.0000, 0.3435],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "hold1 = config.combine_type\n",
    "hold2 = config.combine_factor\n",
    "config.combine_type = 'max'\n",
    "config.combine_factor = 4\n",
    "padding_vector = nn.Parameter(torch.zeros(config.embed_dim), requires_grad=True)\n",
    "module = CombineEmbeddings(config, padding_vector)\n",
    "\n",
    "module.enable_logging()\n",
    "module.operation_chain.max_0.enable_logging()\n",
    "\n",
    "x = torch.randn(32, config.max_seq_len-3, config.embed_dim)\n",
    "print(x[0,0,:8])\n",
    "output = module(x, config.combine_factor)\n",
    "print(output[0,0,:8])\n",
    "config.combine_type = hold1\n",
    "config.combine_factor = hold2\n",
    "del padding_vector, module, hold1, hold2, x, output\n",
    "module = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aaf67558-5fa5-42a9-9642-5bb358db7c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering CombineEmbeddings.forward==========\n",
      "Inputs:\n",
      "Tensor 'tensor' shape: torch.Size([32, 124, 128])\n",
      "Integer 'combine_factor': Value=4\n",
      "\n",
      "==========Entering MeanModule.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 31, 4, 128])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 31, 128])\n",
      "==========Exiting MeanModule.forward==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 31, 128])\n",
      "==========Exiting CombineEmbeddings.forward==========\n"
     ]
    }
   ],
   "source": [
    "hold1 = config.combine_type\n",
    "hold2 = config.combine_factor\n",
    "config.combine_type = 'mean'\n",
    "config.combine_factor = 4\n",
    "padding_vector = nn.Parameter(torch.zeros(config.embed_dim), requires_grad=True)\n",
    "module = CombineEmbeddings(config, padding_vector)\n",
    "\n",
    "module.enable_logging()\n",
    "module.operation_chain.mean_0.enable_logging()\n",
    "\n",
    "x = torch.randn(32, config.max_seq_len-4, config.embed_dim)\n",
    "output = module(x, config.combine_factor)\n",
    "config.combine_type = hold1\n",
    "config.combine_factor = hold2\n",
    "del padding_vector, module, hold1, hold2, x, output\n",
    "module = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cafecd-e03a-42fb-af92-87c392cbdda4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### it has to calculate recursively for multiple levels whenever reshape is involved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "94b47de9-8ab3-40a7-8f93-9ec38bcc3dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering CombineEmbeddings.forward==========\n",
      "Inputs:\n",
      "Tensor 'tensor' shape: torch.Size([32, 128, 128])\n",
      "Integer 'combine_factor': Value=16\n",
      "combine_factor: 16, combine_factor // self.combine_factor: 4\n",
      "\n",
      "==========Entering CombineEmbeddings.forward==========\n",
      "Inputs:\n",
      "Tensor 'tensor' shape: torch.Size([32, 128, 128])\n",
      "Integer 'combine_factor': Value=4\n",
      "combine_factor: 4, combine_factor // self.combine_factor: 1\n",
      "\n",
      "==========Entering MLP.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 32, 4, 128])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 32, 4, 128])\n",
      "==========Exiting MLP.forward==========\n",
      "\n",
      "==========Entering SumModule.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 32, 4, 128])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 32, 128])\n",
      "==========Exiting SumModule.forward==========\n",
      "\n",
      "==========Entering MLP.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 32, 128])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 32, 128])\n",
      "==========Exiting MLP.forward==========\n",
      "\n",
      "==========Entering Norm.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 32, 128])\n",
      "\n",
      "==========Entering Norm.RMSNorm==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 32, 128])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 32, 128])\n",
      "==========Exiting Norm.RMSNorm==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 32, 128])\n",
      "==========Exiting Norm.forward==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 32, 128])\n",
      "==========Exiting CombineEmbeddings.forward==========\n",
      "\n",
      "==========Entering MLP.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 8, 4, 128])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 8, 4, 128])\n",
      "==========Exiting MLP.forward==========\n",
      "\n",
      "==========Entering SumModule.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 8, 4, 128])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 8, 128])\n",
      "==========Exiting SumModule.forward==========\n",
      "\n",
      "==========Entering MLP.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 8, 128])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 8, 128])\n",
      "==========Exiting MLP.forward==========\n",
      "\n",
      "==========Entering Norm.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 8, 128])\n",
      "\n",
      "==========Entering Norm.RMSNorm==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 8, 128])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 8, 128])\n",
      "==========Exiting Norm.RMSNorm==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 8, 128])\n",
      "==========Exiting Norm.forward==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 8, 128])\n",
      "==========Exiting CombineEmbeddings.forward==========\n"
     ]
    }
   ],
   "source": [
    "hold1 = config.combine_type\n",
    "hold2 = config.combine_factor\n",
    "hold3 = config.levels\n",
    "config.combine_type = 'mlp->sum->mlp->norm'\n",
    "config.combine_factor = 4\n",
    "config.levels = 3\n",
    "padding_vector = nn.Parameter(torch.zeros(config.embed_dim), requires_grad=True)\n",
    "module = CombineEmbeddings(config, padding_vector)\n",
    "\n",
    "module.enable_logging()\n",
    "module.operation_chain.mlp_0.enable_logging()\n",
    "module.operation_chain.sum_1.enable_logging()\n",
    "module.operation_chain.mlp_2.enable_logging()\n",
    "module.operation_chain.norm_3.enable_logging()\n",
    "\n",
    "x = torch.randn(32, config.max_seq_len, config.embed_dim)\n",
    "output = module(x, config.combine_factor**(config.levels-1))\n",
    "config.combine_type = hold1\n",
    "config.combine_factor = hold2\n",
    "del padding_vector, module, hold1, hold2, hold3, x, output\n",
    "module = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a431e76-b9df-4742-966e-eb0176f22623",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### recursive and off-length sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9a5ccb1e-fabb-4ee2-9a95-d69ef943b59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering CombineEmbeddings.forward==========\n",
      "Inputs:\n",
      "Tensor 'tensor' shape: torch.Size([32, 127, 128])\n",
      "Integer 'combine_factor': Value=4\n",
      "combine_factor: 4, combine_factor // self.combine_factor: 2\n",
      "\n",
      "==========Entering CombineEmbeddings.forward==========\n",
      "Inputs:\n",
      "Tensor 'tensor' shape: torch.Size([32, 127, 128])\n",
      "Integer 'combine_factor': Value=2\n",
      "combine_factor: 2, combine_factor // self.combine_factor: 1\n",
      "\n",
      "==========Entering ReshapeModule.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 64, 2, 128])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 64, 256])\n",
      "==========Exiting ReshapeModule.forward==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 64, 128])\n",
      "==========Exiting CombineEmbeddings.forward==========\n",
      "\n",
      "==========Entering ReshapeModule.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 32, 2, 128])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 32, 256])\n",
      "==========Exiting ReshapeModule.forward==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 32, 128])\n",
      "==========Exiting CombineEmbeddings.forward==========\n"
     ]
    }
   ],
   "source": [
    "hold1 = config.combine_type\n",
    "hold2 = config.combine_factor\n",
    "hold3 = config.levels\n",
    "config.combine_type = 'reshape->linear_post_reshape'\n",
    "config.combine_factor = 2\n",
    "config.levels = 3\n",
    "padding_vector = nn.Parameter(torch.zeros(config.embed_dim), requires_grad=True)\n",
    "module = CombineEmbeddings(config, padding_vector)\n",
    "\n",
    "module.enable_logging()\n",
    "module.operation_chain.reshape_0.enable_logging()\n",
    "\n",
    "x = torch.randn(32, config.max_seq_len - 1, config.embed_dim)\n",
    "output = module(x, config.combine_factor**(config.levels-1))\n",
    "config.combine_type = hold1\n",
    "config.combine_factor = hold2\n",
    "del padding_vector, module, hold1, hold2, hold3, x, output\n",
    "module = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "76b22d5d-d0ff-4a1a-8634-b1f24ef75f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering CombineEmbeddings.forward==========\n",
      "Inputs:\n",
      "Tensor 'tensor' shape: torch.Size([32, 125, 128])\n",
      "Integer 'combine_factor': Value=4\n",
      "combine_factor: 4, combine_factor // self.combine_factor: 2\n",
      "\n",
      "==========Entering CombineEmbeddings.forward==========\n",
      "Inputs:\n",
      "Tensor 'tensor' shape: torch.Size([32, 125, 128])\n",
      "Integer 'combine_factor': Value=2\n",
      "combine_factor: 2, combine_factor // self.combine_factor: 1\n",
      "\n",
      "==========Entering ReshapeModule.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 63, 2, 128])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 63, 256])\n",
      "==========Exiting ReshapeModule.forward==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 63, 128])\n",
      "==========Exiting CombineEmbeddings.forward==========\n",
      "\n",
      "==========Entering ReshapeModule.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 32, 2, 128])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 32, 256])\n",
      "==========Exiting ReshapeModule.forward==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 32, 128])\n",
      "==========Exiting CombineEmbeddings.forward==========\n"
     ]
    }
   ],
   "source": [
    "hold1 = config.combine_type\n",
    "hold2 = config.combine_factor\n",
    "hold3 = config.levels\n",
    "config.combine_type = 'reshape->linear_post_reshape'\n",
    "config.combine_factor = 2\n",
    "config.levels = 3\n",
    "padding_vector = nn.Parameter(torch.zeros(config.embed_dim), requires_grad=True)\n",
    "module = CombineEmbeddings(config, padding_vector)\n",
    "\n",
    "module.enable_logging()\n",
    "module.operation_chain.reshape_0.enable_logging()\n",
    "\n",
    "x = torch.randn(32, config.max_seq_len - 3, config.embed_dim)\n",
    "output = module(x, config.combine_factor**(config.levels-1))\n",
    "config.combine_type = hold1\n",
    "config.combine_factor = hold2\n",
    "del padding_vector, module, hold1, hold2, hold3, x, output\n",
    "module = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31560096-350b-4353-b18c-c16218afc305",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### This is how Body.concept_matchup() will use this class once it's up and working"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca4db5a-21cd-48da-b7fd-ef64e4ffe1f1",
   "metadata": {},
   "source": [
    "basically it just wants a single vector and there definitely won't be any padding to be done so it can skip most of the logic & get right to the operation chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "827f22a6-6821-41e1-bae2-588b2f031eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering CombineEmbeddings.forward==========\n",
      "Inputs:\n",
      "Tensor 'tensor' shape: torch.Size([32, 4, 64])\n",
      "Integer 'combine_factor': Value=4\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 1, 64])\n",
      "==========Exiting CombineEmbeddings.forward==========\n"
     ]
    }
   ],
   "source": [
    "hold1 = config.combine_type\n",
    "hold2 = config.combine_factor\n",
    "config.combine_type = 'mlp->sum->mlp->norm'\n",
    "config.combine_factor = 4\n",
    "padding_vector = nn.Parameter(torch.zeros(config.embed_dim), requires_grad=True)\n",
    "module = CombineEmbeddings(config, padding_vector)\n",
    "\n",
    "module.enable_logging()\n",
    "#module.operation_chain.mlp_0.enable_logging()\n",
    "#module.operation_chain.sum_1.enable_logging()\n",
    "#module.operation_chain.mlp_2.enable_logging()\n",
    "#module.operation_chain.norm_3.enable_logging()\n",
    "\n",
    "x = torch.randn(32, config.combine_factor, config.embed_dim)\n",
    "output = module(x, config.combine_factor)\n",
    "config.combine_type = hold1\n",
    "config.combine_factor = hold2\n",
    "del padding_vector, module, hold1, hold2, x, output\n",
    "module = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f908eab-e8bc-483d-b15f-fd49f9ae9b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering CombineEmbeddings.forward==========\n",
      "Inputs:\n",
      "Tensor 'tensor' shape: torch.Size([32, 4, 64])\n",
      "Integer 'combine_factor': Value=4\n",
      "\n",
      "==========Entering CombineEmbeddings.forward==========\n",
      "Inputs:\n",
      "Tensor 'tensor' shape: torch.Size([32, 4, 64])\n",
      "Integer 'combine_factor': Value=2\n",
      "\n",
      "==========Entering ReshapeModule.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 2, 2, 64])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 2, 128])\n",
      "==========Exiting ReshapeModule.forward==========\n",
      "\n",
      "==========Entering MLP.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 2, 128])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 2, 64])\n",
      "==========Exiting MLP.forward==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 2, 64])\n",
      "==========Exiting CombineEmbeddings.forward==========\n",
      "\n",
      "==========Entering ReshapeModule.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 1, 2, 64])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 1, 128])\n",
      "==========Exiting ReshapeModule.forward==========\n",
      "\n",
      "==========Entering MLP.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 1, 128])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 1, 64])\n",
      "==========Exiting MLP.forward==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 1, 64])\n",
      "==========Exiting CombineEmbeddings.forward==========\n"
     ]
    }
   ],
   "source": [
    "hold1 = config.combine_type\n",
    "hold2 = config.combine_factor\n",
    "hold3 = config.levels\n",
    "config.combine_type = 'reshape->mlp_post_reshape'\n",
    "config.combine_factor = 2\n",
    "config.levels = 3\n",
    "padding_vector = nn.Parameter(torch.zeros(config.embed_dim), requires_grad=True)\n",
    "module = CombineEmbeddings(config, padding_vector)\n",
    "\n",
    "module.enable_logging()\n",
    "module.operation_chain.reshape_0.enable_logging()\n",
    "module.operation_chain.mlp_post_reshape_1.enable_logging()\n",
    "\n",
    "x = torch.randn(32, config.combine_factor**(config.levels-1), config.embed_dim)\n",
    "output = module(x, config.combine_factor**(config.levels-1))\n",
    "config.combine_type = hold1\n",
    "config.combine_factor = hold2\n",
    "config.levels = hold3\n",
    "del padding_vector, module, hold1, hold2, hold3, x, output\n",
    "module = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6309588b-baa8-400e-9599-1cee9cbe3ef6",
   "metadata": {},
   "source": [
    "# Body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aca9d16d-1a18-413e-a549-7b79ddebe13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to prevent the warning statement from printing hella times\n",
    "cvec_warning = False\n",
    "\n",
    "class Body(LoggingModule):\n",
    "    def __init__(self, config: Config, embedding: torch.Tensor, embedding_combiner: LoggingModule):\n",
    "        super().__init__()\n",
    "        self.max_seq_len = config.max_seq_len\n",
    "        self.combine_factor = config.combine_factor\n",
    "        self.levels = config.levels\n",
    "        self.embedding = embedding\n",
    "        \n",
    "        # Initialize a sequence of Layer instances as specified by the number of hidden layers in the config\n",
    "        self.layers = nn.ModuleList(Layer(config) for _ in range(config.num_layers))\n",
    "\n",
    "        # initialize the final normalizations to stabilize residual outputs before output layer\n",
    "        self.final_norms = nn.ModuleList(Norm(config) for _ in range(config.levels))\n",
    "\n",
    "        # initialize the concept output layers as either full MLPs or simple linear layers. maybe later make an option to make them shared?\n",
    "        if config.output_layer == 'mlp':\n",
    "            self.concept_output_layers = nn.ModuleList(MLP(config.embed_dim,\n",
    "                                                           config.mlp_multiplier,\n",
    "                                                           config.embed_dim,\n",
    "                                                           config.dropout_rate) for _ in range(config.levels-1))\n",
    "        else: # defaults to a linear layer\n",
    "            self.concept_output_layers = nn.ModuleList(nn.Linear(config.embed_dim, config.embed_dim) for _ in range(config.levels-1))\n",
    "\n",
    "        # gets used during inference for the matchup to concept embedding vectors\n",
    "        self.concept_creator = embedding_combiner\n",
    "        \n",
    "    @log_io\n",
    "    def forward(self,\n",
    "                x0s: Tuple[torch.Tensor], # ordered from tokens -> highest concepts\n",
    "                targets: Tuple[torch.Tensor] = None,\n",
    "                cvec_topk: int = None,\n",
    "                cvec_greedy: bool = False,\n",
    "                cvec_temp: float = 1.0,\n",
    "               ) -> Tuple[torch.Tensor]:\n",
    "        return self.forward_training(x0s, targets) if targets is not None else self.forward_inference(x0s, cvec_topk, cvec_greedy, cvec_temp)\n",
    "        \n",
    "    @log_io\n",
    "    def forward_training(self,\n",
    "                         x0s: Tuple[torch.Tensor], # ordered from tokens -> highest concepts\n",
    "                         targets: Tuple[torch.Tensor],\n",
    "                        ) -> Tuple[torch.Tensor]:\n",
    "        # initiate tuple to hold final residual states\n",
    "        xfs = ()\n",
    "        # iterate through model levels, starting from highest level concepts & ending at lowest level tokens\n",
    "        for i, x in enumerate(reversed(x0s)): # reversed() makes us start at highest level\n",
    "\n",
    "            if i == 0:\n",
    "                # if we're dealing with the highest level concepts, there's nothing to cross-attend to\n",
    "                x = self.layers_loop(x, i, c = None, training = True)\n",
    "            else:\n",
    "                # the current level x will cross-attend to the higher level c\n",
    "                x = self.layers_loop(x, i, c = targets[-i], training = True)\n",
    "                \n",
    "            # every level gets its own norm? sure why not. only does anything if config.norm_affine == True\n",
    "            x = self.final_norms[i](x, training = True)\n",
    "\n",
    "            if i == len(x0s)-1:\n",
    "                # if we're dealing with the token residual state then we use the transposed embedding matrix as our output\n",
    "                x = x @ self.embedding.t()\n",
    "            else:\n",
    "                # all the concept levels get their own output layer to help with their regression goal\n",
    "                x = self.concept_output_layers[i](x) # not setting training=True because nn.Linear doesn't know what that is\n",
    "            \n",
    "            # add the final residual state of the level to our tuple\n",
    "            xfs += (x,)\n",
    "                \n",
    "        return xfs # return all final residual states\n",
    "        \n",
    "    @log_io\n",
    "    def forward_inference(self,\n",
    "                          x0s: Tuple[torch.Tensor], # ordered from tokens -> highest concepts\n",
    "                          cvec_topk: int = None,\n",
    "                          cvec_greedy: bool = False,\n",
    "                          cvec_temp: float = 1.0,\n",
    "                         ) -> Tuple[torch.Tensor]:\n",
    "        # initiate tuple to hold final residual states\n",
    "        xfs = ()\n",
    "        # iterate through model levels, starting from highest level concepts & ending at lowest level tokens\n",
    "        for i, x in enumerate(reversed(x0s)): # reversed() makes us start at highest level\n",
    "            \n",
    "            # if we're dealing with a concept level, then we need to fill in entire concept sequences for lower level to attend to. for token level we only want one pass\n",
    "            if i != len(x0s)-1:\n",
    "\n",
    "                # figuring out how many times we need to run this concept level in order for it to be usable for cross-attention to the layer below\n",
    "                effective_max_seq_len = self.max_seq_len // (self.combine_factor ** (self.levels-1-i))\n",
    "                assert x.shape[1] <= effective_max_seq_len, f'at level {i} a too-long sequence ({x.shape[1]} vs {effective_max_seq_len}) made it to Body'\n",
    "                extra_runs = effective_max_seq_len - x.shape[1]\n",
    "\n",
    "                # if extra_runs == 0 then this will only run once\n",
    "                for k in range(extra_runs+1): \n",
    "                    \n",
    "                    # run through layers. xfs[i-1] are the higher level concepts to pay attention to\n",
    "                    c_regression = self.layers_loop(x, i, training=False) if i == 0 else self.layers_loop(x, i, xfs[i-1], training=False)\n",
    "                    # splice out the final prediction\n",
    "                    c_regression = c_regression[:,-1,:]\n",
    "                    # normalize it\n",
    "                    c_regression = self.final_norms[i](c_regression, training=False)\n",
    "                    # our concept output layer\n",
    "                    c_regression = self.concept_output_layers[i](c_regression)\n",
    "                    # either select most similar concept vectors to be appended to the sequence or use the raw regression output\n",
    "                    c_vec = self.concept_matchup(c_regression, cvec_topk, cvec_greedy, cvec_temp) if cvec_topk is not None else c_regression\n",
    "                    # append to x\n",
    "                    x = torch.concat([x, c_vec.unsqueeze(1)], dim=1)\n",
    "\n",
    "                # this should only trim off one concept vector from the beginning of the sequence at most\n",
    "                x = x[:,-effective_max_seq_len:,:]\n",
    "                \n",
    "            else: # if we're dealing with the token layer rather than a concept layer, we only want to run once\n",
    "                # run the model\n",
    "                x = self.layers_loop(x, i, xfs[i-1], training=False)\n",
    "                \n",
    "                # normalize\n",
    "                x = self.final_norms[i](x, training=False)\n",
    "                # the output layer is the transposed embedding matrix\n",
    "                \n",
    "                x = x @ self.embedding.t()\n",
    "            \n",
    "            # add the final residual state of the level to our tuple\n",
    "            xfs += (x,)\n",
    "            \n",
    "        return xfs # we could return just xfs[-1] since it's inference but i have a feeling i'll want to analyze the concepts later\n",
    "\n",
    "    @log_io\n",
    "    def layers_loop(self, x: torch.Tensor,\n",
    "                    i: int,\n",
    "                    c: torch.Tensor = None,\n",
    "                    training : bool = False,\n",
    "                   ) -> torch.Tensor:\n",
    "        \n",
    "        # Iteratively process the input through each Layer of the model\n",
    "        for layer in self.layers:\n",
    "            \n",
    "            # run through layers. at i==0 there's no higher level to attend to\n",
    "            x = layer(x, training=training) if i == 0 else layer(x, c, training)\n",
    "                \n",
    "        return x\n",
    "    \n",
    "    @log_io \n",
    "    def concept_matchup(self,\n",
    "                        c_regression: torch.Tensor,\n",
    "                        cvec_topk: int,\n",
    "                        cvec_greedy: bool,\n",
    "                        cvec_temp: float,\n",
    "                        ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        NOT CURRENTLY WORKING\n",
    "        \n",
    "        1. check similarity of inputted c_regression against all tokens in vocabulary\n",
    "        2. select topk tokens where k = cvec_topk\n",
    "        3. create every order of combinations of length combine_factor of the selected tokens\n",
    "        4. run every combination through concept embedding function\n",
    "        5. check similarity of c_regression against all created concept embeddings\n",
    "        6. Repeat steps 2-4 until correct level is reached\n",
    "        7. if cvec_greedy==True then select highest similarity vector; otherwise softmax, apply cvec_temp, and sample from distribution\n",
    "        8. Return c_vec\n",
    "\n",
    "        hold up, would this fall apart above the second level? \n",
    "        like would the concepts that get created at a given level be unlikely to create the correct next-higher concepts because there'd be a lot of repetition?\n",
    "        also, wouldn't linear layer and MLP in the concept creation mess with the cosine similarity metric?\n",
    "        maybe this function should only be used when the concept construction method does NOT include MLP or linear layer, like only the normed version can use it\n",
    "        in that case then if we're restricted to using sum, max, and mean wouldn't these be the steps?:\n",
    "        1. assert that the concept combination method is either mean, sum, or max (and optionally norm can be in there)?\n",
    "        2. check similarity of inputted c_regression against all tokens in vocabulary\n",
    "        3. select topk tokens where k = cvec_topk\n",
    "        4. create every order of combinations of length combine_factor of the selected tokens\n",
    "        5. run every combination through concept embedding function\n",
    "        6. check similarity of c_regression against all created concept embeddings\n",
    "        7. if cvec_greedy==True then select highest similarity vector; otherwise softmax, apply cvec_temp, and sample from distribution\n",
    "        8. Return c_vec\n",
    "        \"\"\"\n",
    "        global cvec_warning\n",
    "        batch_size, d = c_regression.size()\n",
    "        vocab_size = self.embedding.size(0)\n",
    "    \n",
    "        # Batch cosine similarity\n",
    "        # Reshape c: (batch_size x 1 x embedding_dim)\n",
    "        # Reshape embedding: (1 x vocab_size x embedding_dim)\n",
    "        # Resulting similarity: (batch_size x vocab_size)\n",
    "        token_similarities = F.cosine_similarity(c_regression.unsqueeze(1), self.embedding.unsqueeze(0), dim=-1)\n",
    "        \n",
    "        # how many tokens will we sample to build up our chosen concept vector?\n",
    "        if cvec_topk is None:\n",
    "            cvec_topk = self.combine_factor ** (self.levels-1)\n",
    "            if (cvec_warning == False) or (cvec_warning is None):\n",
    "                print(f\"cvec_topk not defined. defaulting to highest level's minimum size: combo**(levels-1) = {cvec_topk}\")\n",
    "                cvec_warning = True\n",
    "        assert cvec_topk >= self.combine_factor ** (self.levels-1), f'cvec_topk = {cvec_topk} needs to be >= self.combine_factor ** (self.levels-1) = {self.combine_factor ** (self.levels-1)}'\n",
    "        \n",
    "        # Select top-k token embeddings for each concept vector\n",
    "        topk_token_indices = torch.topk(token_similarities, k=cvec_topk, dim=1).indices  # (batch_size x sample)\n",
    "\n",
    "        ##################\n",
    "        ### UNFINISHED ###\n",
    "        ##################\n",
    "        \n",
    "        return c_regression # change to c_vec once finished\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3ec6ea-e11d-43b7-86e6-5e12844b97d6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## demonstrations/debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2258a4-fce2-4d5b-8467-7377c955ff2b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c69f50b2-daf8-4933-af0a-1293fa6b6438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering Body.forward_training==========\n",
      "Inputs:\n",
      "Tuple 'x0s':\n",
      "    Tensor 'x0s[0]' shape: torch.Size([32, 128, 64])\n",
      "    Tensor 'x0s[1]' shape: torch.Size([32, 32, 64])\n",
      "Tuple 'targets':\n",
      "    Tensor 'targets[0]' shape: torch.Size([32, 128, 64])\n",
      "    Tensor 'targets[1]' shape: torch.Size([32, 32, 64])\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 32, 64])\n",
      "Integer 'i': Value=0\n",
      "Other-type 'c': Type=NoneType, Value=None\n",
      "Integer 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 32, 64])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 128, 64])\n",
      "Integer 'i': Value=1\n",
      "Tensor 'c' shape: torch.Size([32, 32, 64])\n",
      "Integer 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 128, 64])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([32, 32, 64])\n",
      "Tensor 'output[1]' shape: torch.Size([32, 128, 128])\n",
      "==========Exiting Body.forward_training==========\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # first let's do 2 levels training\n",
    "    embedder = nn.Embedding(config.vocab_size, config.embed_dim)\n",
    "    padding_vector = nn.Parameter(torch.zeros(config.embed_dim), requires_grad=True)\n",
    "    embedding_combiner = CombineEmbeddings(config, padding_vector)\n",
    "    hold = config.levels\n",
    "    config.levels = 2\n",
    "    module = Body(config, embedder.weight, embedding_combiner)\n",
    "    module.enable_logging()\n",
    "    \n",
    "    ### enabling logging for sub-modules\n",
    "    #module.layers[0].enable_logging()\n",
    "    #module.final_norms[0].enable_logging()\n",
    "    \n",
    "    ### disabling logging for sub-functions\n",
    "    module.disable_function_logging('forward')\n",
    "    #module.disable_function_logging('forward_training')\n",
    "    #module.disable_function_logging('forward_inference')\n",
    "    #module.disable_function_logging('layers_loop')\n",
    "    #module.disable_function_logging('concept_matchup')\n",
    "    #module.disable_function_logging('create_concept_embeddings')\n",
    "    \n",
    "    x = torch.randn(32, config.max_seq_len, config.embed_dim)\n",
    "    c = torch.randn(32, config.max_seq_len // config.combine_factor, config.embed_dim)\n",
    "    x0s = (x,c)\n",
    "    targets = (x + torch.randn_like(x), c + torch.randn_like(c))\n",
    "    output = module(x0s, targets)\n",
    "    config.levels = hold\n",
    "del embedder, embedding_combiner, hold, module, x, c, x0s, targets, output\n",
    "embedder, embedding_combiner, module = None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed95ade0-d798-42b8-bc15-6ba416239b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering Body.forward_training==========\n",
      "Inputs:\n",
      "Tuple 'x0s':\n",
      "    Tensor 'x0s[0]' shape: torch.Size([32, 128, 64])\n",
      "    Tensor 'x0s[1]' shape: torch.Size([32, 32, 64])\n",
      "    Tensor 'x0s[2]' shape: torch.Size([32, 8, 64])\n",
      "Tuple 'targets':\n",
      "    Tensor 'targets[0]' shape: torch.Size([32, 128, 64])\n",
      "    Tensor 'targets[1]' shape: torch.Size([32, 32, 64])\n",
      "    Tensor 'targets[2]' shape: torch.Size([32, 8, 64])\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 8, 64])\n",
      "Integer 'i': Value=0\n",
      "Other-type 'c': Type=NoneType, Value=None\n",
      "Integer 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 8, 64])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 32, 64])\n",
      "Integer 'i': Value=1\n",
      "Tensor 'c' shape: torch.Size([32, 8, 64])\n",
      "Integer 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 32, 64])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 128, 64])\n",
      "Integer 'i': Value=2\n",
      "Tensor 'c' shape: torch.Size([32, 32, 64])\n",
      "Integer 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 128, 64])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([32, 8, 64])\n",
      "Tensor 'output[1]' shape: torch.Size([32, 32, 64])\n",
      "Tensor 'output[2]' shape: torch.Size([32, 128, 128])\n",
      "==========Exiting Body.forward_training==========\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # 3 levels training\n",
    "    embedder = nn.Embedding(config.vocab_size, config.embed_dim)\n",
    "    padding_vector = nn.Parameter(torch.zeros(config.embed_dim), requires_grad=True)\n",
    "    embedding_combiner = CombineEmbeddings(config, padding_vector)\n",
    "    hold = config.levels\n",
    "    config.levels = 3\n",
    "    module = Body(config, embedder.weight, embedding_combiner)\n",
    "    module.enable_logging()\n",
    "    \n",
    "    ### enabling logging for sub-modules\n",
    "    #module.layers[0].enable_logging()\n",
    "    #module.final_norms[0].enable_logging()\n",
    "    \n",
    "    ### disabling logging for sub-functions\n",
    "    module.disable_function_logging('forward')\n",
    "    #module.disable_function_logging('forward_training')\n",
    "    #module.disable_function_logging('forward_inference')\n",
    "    #module.disable_function_logging('layers_loop')\n",
    "    #module.disable_function_logging('concept_matchup')\n",
    "    #module.disable_function_logging('create_concept_embeddings')\n",
    "    \n",
    "    x0 = torch.randn(32, config.max_seq_len // (config.combine_factor**0), config.embed_dim)\n",
    "    c1 = torch.randn(32, config.max_seq_len // (config.combine_factor**1), config.embed_dim)\n",
    "    c2 = torch.randn(32, config.max_seq_len // (config.combine_factor**2), config.embed_dim)\n",
    "    x0s = (x0, c1, c2)\n",
    "    targets = (x0 + torch.randn_like(x0), c1 + torch.randn_like(c1), c2 + torch.randn_like(c2))\n",
    "    output = module(x0s, targets)\n",
    "    config.levels = hold\n",
    "del embedder, embedding_combiner, hold, module, x0, c1, c2, x0s, targets, output\n",
    "embedder, embedding_combiner, module = None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea569681-7086-4569-a733-e37ba76422d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering Body.forward_training==========\n",
      "Inputs:\n",
      "Tuple 'x0s':\n",
      "    Tensor 'x0s[0]' shape: torch.Size([32, 128, 64])\n",
      "    Tensor 'x0s[1]' shape: torch.Size([32, 32, 64])\n",
      "    Tensor 'x0s[2]' shape: torch.Size([32, 8, 64])\n",
      "    Tensor 'x0s[3]' shape: torch.Size([32, 2, 64])\n",
      "Tuple 'targets':\n",
      "    Tensor 'targets[0]' shape: torch.Size([32, 128, 64])\n",
      "    Tensor 'targets[1]' shape: torch.Size([32, 32, 64])\n",
      "    Tensor 'targets[2]' shape: torch.Size([32, 8, 64])\n",
      "    Tensor 'targets[3]' shape: torch.Size([32, 2, 64])\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 2, 64])\n",
      "Integer 'i': Value=0\n",
      "Other-type 'c': Type=NoneType, Value=None\n",
      "Integer 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 2, 64])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 8, 64])\n",
      "Integer 'i': Value=1\n",
      "Tensor 'c' shape: torch.Size([32, 2, 64])\n",
      "Integer 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 8, 64])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 32, 64])\n",
      "Integer 'i': Value=2\n",
      "Tensor 'c' shape: torch.Size([32, 8, 64])\n",
      "Integer 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 32, 64])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 128, 64])\n",
      "Integer 'i': Value=3\n",
      "Tensor 'c' shape: torch.Size([32, 32, 64])\n",
      "Integer 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 128, 64])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([32, 2, 64])\n",
      "Tensor 'output[1]' shape: torch.Size([32, 8, 64])\n",
      "Tensor 'output[2]' shape: torch.Size([32, 32, 64])\n",
      "Tensor 'output[3]' shape: torch.Size([32, 128, 128])\n",
      "==========Exiting Body.forward_training==========\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # 4 levels training\n",
    "    embedder = nn.Embedding(config.vocab_size, config.embed_dim)\n",
    "    padding_vector = nn.Parameter(torch.zeros(config.embed_dim), requires_grad=True)\n",
    "    embedding_combiner = CombineEmbeddings(config, padding_vector)\n",
    "    hold = config.levels\n",
    "    config.levels = 4\n",
    "    module = Body(config, embedder.weight, embedding_combiner)\n",
    "    module.enable_logging()\n",
    "    \n",
    "    ### enabling logging for sub-modules\n",
    "    #module.layers[0].enable_logging()\n",
    "    #module.final_norms[0].enable_logging()\n",
    "    \n",
    "    ### disabling logging for sub-functions\n",
    "    module.disable_function_logging('forward')\n",
    "    #module.disable_function_logging('forward_training')\n",
    "    #module.disable_function_logging('forward_inference')\n",
    "    #module.disable_function_logging('layers_loop')\n",
    "    #module.disable_function_logging('concept_matchup')\n",
    "    #module.disable_function_logging('create_concept_embeddings')\n",
    "    \n",
    "    x0 = torch.randn(32, config.max_seq_len // (config.combine_factor**0), config.embed_dim)\n",
    "    c1 = torch.randn(32, config.max_seq_len // (config.combine_factor**1), config.embed_dim)\n",
    "    c2 = torch.randn(32, config.max_seq_len // (config.combine_factor**2), config.embed_dim)\n",
    "    c3 = torch.randn(32, config.max_seq_len // (config.combine_factor**3), config.embed_dim)\n",
    "    x0s = (x0, c1, c2, c3)\n",
    "    targets = (x0 + torch.randn_like(x0), c1 + torch.randn_like(c1), c2 + torch.randn_like(c2), c3 + torch.randn_like(c3))\n",
    "    output = module(x0s, targets)\n",
    "    config.levels = hold\n",
    "del embedder, embedding_combiner, hold, module, x0, c1, c2, c3, x0s, targets, output\n",
    "embedder, embedding_combiner, module = None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04c038c-176d-43ad-8135-9925a7eb239d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### inference w/out concept matchup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f232db0e-ab15-4477-a966-4f04ea021b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering Body.forward==========\n",
      "Inputs:\n",
      "Tuple 'x0s':\n",
      "    Tensor 'x0s[0]' shape: torch.Size([32, 112, 64])\n",
      "    Tensor 'x0s[1]' shape: torch.Size([32, 28, 64])\n",
      "Other-type 'targets': Type=NoneType, Value=None\n",
      "\n",
      "==========Entering Body.forward_inference==========\n",
      "Inputs:\n",
      "Tuple 'x0s':\n",
      "    Tensor 'x0s[0]' shape: torch.Size([32, 112, 64])\n",
      "    Tensor 'x0s[1]' shape: torch.Size([32, 28, 64])\n",
      "Other-type 'cvec_topk': Type=NoneType, Value=None\n",
      "Integer 'cvec_greedy': Value=False\n",
      "Float 'cvec_temp': Value=1.0\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 28, 64])\n",
      "Integer 'i': Value=0\n",
      "Integer 'c': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 28, 64])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 29, 64])\n",
      "Integer 'i': Value=0\n",
      "Integer 'c': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 29, 64])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 30, 64])\n",
      "Integer 'i': Value=0\n",
      "Integer 'c': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 30, 64])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 31, 64])\n",
      "Integer 'i': Value=0\n",
      "Integer 'c': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 31, 64])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 32, 64])\n",
      "Integer 'i': Value=0\n",
      "Integer 'c': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 32, 64])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 112, 64])\n",
      "Integer 'i': Value=1\n",
      "Tensor 'c' shape: torch.Size([32, 32, 64])\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 112, 64])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([32, 32, 64])\n",
      "Tensor 'output[1]' shape: torch.Size([32, 112, 128])\n",
      "==========Exiting Body.forward_inference==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([32, 32, 64])\n",
      "Tensor 'output[1]' shape: torch.Size([32, 112, 128])\n",
      "==========Exiting Body.forward==========\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # now 2 levels partial sequence length, so like we're doing inference\n",
    "    # our partial sequence will always be a clean interval of config.combine_factor because Model.create_x0s() uses CombineEmbeddings() to make the intervals clean\n",
    "    embedder = nn.Embedding(config.vocab_size, config.embed_dim)\n",
    "    padding_vector = nn.Parameter(torch.zeros(config.embed_dim), requires_grad=True)\n",
    "    embedding_combiner = CombineEmbeddings(config, padding_vector)\n",
    "    hold = config.levels\n",
    "    config.levels = 2\n",
    "    module = Body(config, embedder.weight, embedding_combiner)\n",
    "    module.enable_logging()\n",
    "    \n",
    "    ### enabling logging for sub-modules\n",
    "    #module.layers[0].enable_logging()\n",
    "    #module.final_norms[0].enable_logging()\n",
    "    \n",
    "    ### disabling logging for sub-functions\n",
    "    #module.disable_function_logging('forward')\n",
    "    #module.disable_function_logging('forward_training')\n",
    "    #module.disable_function_logging('forward_inference')\n",
    "    #module.disable_function_logging('layers_loop')\n",
    "    #module.disable_function_logging('concept_matchup')\n",
    "    module.disable_function_logging('create_concept_embeddings') # rn this one is hella inefficient using a for loop over batch so not fun to print\n",
    "    \n",
    "    x = torch.randn(32, config.max_seq_len - (config.combine_factor**2), config.embed_dim)\n",
    "    c = torch.randn(32, (config.max_seq_len // config.combine_factor) - config.combine_factor, config.embed_dim)\n",
    "    x0s = (x,c)\n",
    "    output = module(x0s, cvec_topk = None)\n",
    "    config.levels = hold\n",
    "del embedder, embedding_combiner, hold, module, x, c, x0s, output\n",
    "embedder, embedding_combiner, module = None, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca300b23-878a-43af-88ab-b0080efa186e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering Body.forward==========\n",
      "Inputs:\n",
      "Tuple 'x0s':\n",
      "    Tensor 'x0s[0]' shape: torch.Size([32, 64, 64])\n",
      "    Tensor 'x0s[1]' shape: torch.Size([32, 16, 64])\n",
      "    Tensor 'x0s[2]' shape: torch.Size([32, 4, 64])\n",
      "Other-type 'targets': Type=NoneType, Value=None\n",
      "\n",
      "==========Entering Body.forward_inference==========\n",
      "Inputs:\n",
      "Tuple 'x0s':\n",
      "    Tensor 'x0s[0]' shape: torch.Size([32, 64, 64])\n",
      "    Tensor 'x0s[1]' shape: torch.Size([32, 16, 64])\n",
      "    Tensor 'x0s[2]' shape: torch.Size([32, 4, 64])\n",
      "Other-type 'cvec_topk': Type=NoneType, Value=None\n",
      "Integer 'cvec_greedy': Value=False\n",
      "Float 'cvec_temp': Value=1.0\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 4, 64])\n",
      "Integer 'i': Value=0\n",
      "Integer 'c': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 4, 64])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 5, 64])\n",
      "Integer 'i': Value=0\n",
      "Integer 'c': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 5, 64])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 6, 64])\n",
      "Integer 'i': Value=0\n",
      "Integer 'c': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 6, 64])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 7, 64])\n",
      "Integer 'i': Value=0\n",
      "Integer 'c': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 7, 64])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 8, 64])\n",
      "Integer 'i': Value=0\n",
      "Integer 'c': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 8, 64])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 16, 64])\n",
      "Integer 'i': Value=1\n",
      "Tensor 'c' shape: torch.Size([32, 8, 64])\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 16, 64])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 17, 64])\n",
      "Integer 'i': Value=1\n",
      "Tensor 'c' shape: torch.Size([32, 8, 64])\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 17, 64])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 18, 64])\n",
      "Integer 'i': Value=1\n",
      "Tensor 'c' shape: torch.Size([32, 8, 64])\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 18, 64])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 19, 64])\n",
      "Integer 'i': Value=1\n",
      "Tensor 'c' shape: torch.Size([32, 8, 64])\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 19, 64])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 20, 64])\n",
      "Integer 'i': Value=1\n",
      "Tensor 'c' shape: torch.Size([32, 8, 64])\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 20, 64])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 21, 64])\n",
      "Integer 'i': Value=1\n",
      "Tensor 'c' shape: torch.Size([32, 8, 64])\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 21, 64])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 22, 64])\n",
      "Integer 'i': Value=1\n",
      "Tensor 'c' shape: torch.Size([32, 8, 64])\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 22, 64])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 23, 64])\n",
      "Integer 'i': Value=1\n",
      "Tensor 'c' shape: torch.Size([32, 8, 64])\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 23, 64])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 24, 64])\n",
      "Integer 'i': Value=1\n",
      "Tensor 'c' shape: torch.Size([32, 8, 64])\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 24, 64])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 25, 64])\n",
      "Integer 'i': Value=1\n",
      "Tensor 'c' shape: torch.Size([32, 8, 64])\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 25, 64])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 26, 64])\n",
      "Integer 'i': Value=1\n",
      "Tensor 'c' shape: torch.Size([32, 8, 64])\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 26, 64])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 27, 64])\n",
      "Integer 'i': Value=1\n",
      "Tensor 'c' shape: torch.Size([32, 8, 64])\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 27, 64])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 28, 64])\n",
      "Integer 'i': Value=1\n",
      "Tensor 'c' shape: torch.Size([32, 8, 64])\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 28, 64])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 29, 64])\n",
      "Integer 'i': Value=1\n",
      "Tensor 'c' shape: torch.Size([32, 8, 64])\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 29, 64])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 30, 64])\n",
      "Integer 'i': Value=1\n",
      "Tensor 'c' shape: torch.Size([32, 8, 64])\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 30, 64])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 31, 64])\n",
      "Integer 'i': Value=1\n",
      "Tensor 'c' shape: torch.Size([32, 8, 64])\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 31, 64])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 32, 64])\n",
      "Integer 'i': Value=1\n",
      "Tensor 'c' shape: torch.Size([32, 8, 64])\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 32, 64])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 64, 64])\n",
      "Integer 'i': Value=2\n",
      "Tensor 'c' shape: torch.Size([32, 32, 64])\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 64, 64])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([32, 8, 64])\n",
      "Tensor 'output[1]' shape: torch.Size([32, 32, 64])\n",
      "Tensor 'output[2]' shape: torch.Size([32, 64, 128])\n",
      "==========Exiting Body.forward_inference==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([32, 8, 64])\n",
      "Tensor 'output[1]' shape: torch.Size([32, 32, 64])\n",
      "Tensor 'output[2]' shape: torch.Size([32, 64, 128])\n",
      "==========Exiting Body.forward==========\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # now 3 levels partial sequence length, so like we're doing inference\n",
    "    # our partial sequence will always be a clean interval of config.combine_factor because Model.create_x0s() uses CombineEmbeddings() to make the intervals clean\n",
    "    embedder = nn.Embedding(config.vocab_size, config.embed_dim)\n",
    "    padding_vector = nn.Parameter(torch.zeros(config.embed_dim), requires_grad=True)\n",
    "    embedding_combiner = CombineEmbeddings(config, padding_vector)\n",
    "    hold = config.levels\n",
    "    config.levels = 3\n",
    "    module = Body(config, embedder.weight, embedding_combiner)\n",
    "    module.enable_logging()\n",
    "    \n",
    "    ### enabling logging for sub-modules\n",
    "    #module.layers[0].enable_logging()\n",
    "    #module.final_norms[0].enable_logging()\n",
    "    \n",
    "    ### disabling logging for sub-functions\n",
    "    #module.disable_function_logging('forward')\n",
    "    #module.disable_function_logging('forward_training')\n",
    "    #module.disable_function_logging('forward_inference')\n",
    "    #module.disable_function_logging('layers_loop')\n",
    "    module.disable_function_logging('concept_matchup') # this one is pretty boring and our prints are already long enough\n",
    "    module.disable_function_logging('create_concept_embeddings') # rn this one is hella inefficient using a for loop over batch so not fun to print\n",
    "    \n",
    "    x0 = torch.randn(32, config.max_seq_len // (config.combine_factor ** 0) - (config.combine_factor**3), config.embed_dim)\n",
    "    c1 = torch.randn(32, config.max_seq_len // (config.combine_factor ** 1) - (config.combine_factor**2), config.embed_dim)\n",
    "    c2 = torch.randn(32, config.max_seq_len // (config.combine_factor ** 2) - (config.combine_factor**1), config.embed_dim)\n",
    "    x0s = (x0, c1, c2)\n",
    "    output = module(x0s, cvec_topk = None)\n",
    "    config.levels = hold\n",
    "del embedder, embedding_combiner, hold, module, x0, c1, c2, x0s, output\n",
    "embedder, embedding_combiner, module = None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b424ce0-7662-4907-9a43-6209419bb09c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### infernce w/ matchup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfdc044-222c-497c-bf7e-78f0562bda72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57f3ce31-4b47-4c61-9f96-ebb901c5690a",
   "metadata": {},
   "source": [
    "# Concept Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d7bfbfc-5bd6-4b23-b54e-2601684fe1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConceptLoss(LoggingModule):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.combine_factor = config.combine_factor\n",
    "        self.levels = config.levels\n",
    "        self.level_loss_weight = config.level_loss_weight\n",
    "\n",
    "        self.MAE_loss = nn.L1Loss() if config.mae_loss else None\n",
    "        self.MSE_loss = nn.MSELoss() if config.mse_loss else None\n",
    "        self.COS_loss = nn.CosineSimilarity(dim=-1, eps=1e-6) if config.cos_loss else None\n",
    "    \n",
    "    @log_io\n",
    "    def forward(self, \n",
    "                xfs: Tuple[torch.Tensor], # xfs are ordered highest concept level -> token level\n",
    "                targets: Tuple[torch.Tensor], # targets are ordered highest concept level -> token level\n",
    "               ) -> torch.Tensor:\n",
    "        # initialize loss value\n",
    "        concept_loss = torch.tensor(0.0)\n",
    "        \n",
    "        # iterate through all concept-embedding layers and calculate loss\n",
    "        for i in range(self.levels - 1):\n",
    "            # select our relevant final residual state and target vectors\n",
    "            lvl_output = xfs[i]\n",
    "            lvl_targets = targets[i].detach().clone()\n",
    "            \n",
    "            # calculate the decay value placed on this level's total amount of loss\n",
    "            lambadada = (self.level_loss_weight ** (self.levels -1 -i))\n",
    "            \n",
    "            # setup flattening for if we're doing MAE or MSE\n",
    "            if (self.MAE_loss is not None) or (self.MSE_loss is not None):\n",
    "                # Reshape output and target_vectors to combine batch and seq_len dimensions\n",
    "                lvl_output_flat = lvl_output.view(-1, lvl_output.size(-1))\n",
    "                lvl_targets_flat = lvl_targets.view(-1, lvl_targets.size(-1))\n",
    "\n",
    "            # calculate loss values. notice multiple might occur or even none at all\n",
    "            if self.MAE_loss is not None:\n",
    "                concept_loss = concept_loss + self.MAE_loss(lvl_output_flat, lvl_targets_flat) * lambadada\n",
    "            if self.MSE_loss is not None:\n",
    "                concept_loss = concept_loss + self.MSE_loss(lvl_output_flat, lvl_targets_flat) * lambadada\n",
    "            if self.COS_loss is not None:\n",
    "                cosine_loss = (1 - self.COS_loss(lvl_output, lvl_targets)).mean()\n",
    "                concept_loss = concept_loss + cosine_loss * lambadada\n",
    "\n",
    "        return concept_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57dbe45-22a2-4bcd-b424-45dc0a8bdd57",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Demonstration/Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6c2ad6f1-9ff7-4285-a63f-9e11b79ed319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering ConceptLoss.forward==========\n",
      "Inputs:\n",
      "Tuple 'xfs':\n",
      "    Tensor 'xfs[0]' shape: torch.Size([32, 256, 128])\n",
      "    Tensor 'xfs[1]' shape: torch.Size([32, 64, 128])\n",
      "    Tensor 'xfs[2]' shape: torch.Size([32, 16, 128])\n",
      "Tuple 'targets':\n",
      "    Tensor 'targets[0]' shape: torch.Size([32, 256, 128])\n",
      "    Tensor 'targets[1]' shape: torch.Size([32, 64, 128])\n",
      "    Tensor 'targets[2]' shape: torch.Size([32, 16, 128])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([])\n",
      "==========Exiting ConceptLoss.forward==========\n",
      "tensor(0.5887)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    hold1 = config.levels\n",
    "    hold2 = config.cos_loss\n",
    "    hold3 = config.mse_loss\n",
    "    hold4 = config.mae_loss\n",
    "    config.levels = 3\n",
    "    config.cos_loss = True\n",
    "    config.mse_loss = False\n",
    "    config.mae_loss = False\n",
    "    module = ConceptLoss(config)\n",
    "    module.enable_logging()\n",
    "    \n",
    "    x0 = torch.randn(32, config.max_seq_len // (config.combine_factor**0), config.embed_dim)\n",
    "    c1 = torch.randn(32, config.max_seq_len // (config.combine_factor**1), config.embed_dim)\n",
    "    c2 = torch.randn(32, config.max_seq_len // (config.combine_factor**2), config.embed_dim)\n",
    "    xfs = (x0, c1, c2)\n",
    "    targets = (x0 + torch.randn_like(x0), c1 + torch.randn_like(c1), c2 + torch.randn_like(c2))\n",
    "    output = module(xfs, targets)\n",
    "    print(output)\n",
    "    config.levels = hold1\n",
    "    config.cos_loss = hold2\n",
    "    config.mse_loss = hold3\n",
    "    config.mae_loss = hold4\n",
    "del hold1, hold2, hold3, hold4, module, x0, c1, c2, xfs, targets, output\n",
    "module = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b499ffb0-c43c-4995-b59c-eaf4729823a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering ConceptLoss.forward==========\n",
      "Inputs:\n",
      "Tuple 'xfs':\n",
      "    Tensor 'xfs[0]' shape: torch.Size([32, 256, 128])\n",
      "    Tensor 'xfs[1]' shape: torch.Size([32, 64, 128])\n",
      "    Tensor 'xfs[2]' shape: torch.Size([32, 16, 128])\n",
      "Tuple 'targets':\n",
      "    Tensor 'targets[0]' shape: torch.Size([32, 256, 128])\n",
      "    Tensor 'targets[1]' shape: torch.Size([32, 64, 128])\n",
      "    Tensor 'targets[2]' shape: torch.Size([32, 16, 128])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([])\n",
      "==========Exiting ConceptLoss.forward==========\n",
      "tensor(2.0027)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    hold1 = config.levels\n",
    "    hold2 = config.cos_loss\n",
    "    hold3 = config.mse_loss\n",
    "    hold4 = config.mae_loss\n",
    "    config.levels = 3\n",
    "    config.cos_loss = False\n",
    "    config.mse_loss = True\n",
    "    config.mae_loss = False\n",
    "    module = ConceptLoss(config)\n",
    "    module.enable_logging()\n",
    "    \n",
    "    x0 = torch.randn(32, config.max_seq_len // (config.combine_factor**0), config.embed_dim)\n",
    "    c1 = torch.randn(32, config.max_seq_len // (config.combine_factor**1), config.embed_dim)\n",
    "    c2 = torch.randn(32, config.max_seq_len // (config.combine_factor**2), config.embed_dim)\n",
    "    xfs = (x0, c1, c2)\n",
    "    targets = (x0 + torch.randn_like(x0), c1 + torch.randn_like(c1), c2 + torch.randn_like(c2))\n",
    "    output = module(xfs, targets)\n",
    "    print(output)\n",
    "    config.levels = hold1\n",
    "    config.cos_loss = hold2\n",
    "    config.mse_loss = hold3\n",
    "    config.mae_loss = hold4\n",
    "del hold1, hold2, hold3, hold4, module, x0, c1, c2, xfs, targets, output\n",
    "module = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "89170f73-4f53-4d4d-b59e-d45d83da1b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering ConceptLoss.forward==========\n",
      "Inputs:\n",
      "Tuple 'xfs':\n",
      "    Tensor 'xfs[0]' shape: torch.Size([32, 256, 128])\n",
      "    Tensor 'xfs[1]' shape: torch.Size([32, 64, 128])\n",
      "    Tensor 'xfs[2]' shape: torch.Size([32, 16, 128])\n",
      "Tuple 'targets':\n",
      "    Tensor 'targets[0]' shape: torch.Size([32, 256, 128])\n",
      "    Tensor 'targets[1]' shape: torch.Size([32, 64, 128])\n",
      "    Tensor 'targets[2]' shape: torch.Size([32, 16, 128])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([])\n",
      "==========Exiting ConceptLoss.forward==========\n",
      "tensor(1.5958)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    hold1 = config.levels\n",
    "    hold2 = config.cos_loss\n",
    "    hold3 = config.mse_loss\n",
    "    hold4 = config.mae_loss\n",
    "    config.levels = 3\n",
    "    config.cos_loss = False\n",
    "    config.mse_loss = False\n",
    "    config.mae_loss = True\n",
    "    module = ConceptLoss(config)\n",
    "    module.enable_logging()\n",
    "    \n",
    "    x0 = torch.randn(32, config.max_seq_len // (config.combine_factor**0), config.embed_dim)\n",
    "    c1 = torch.randn(32, config.max_seq_len // (config.combine_factor**1), config.embed_dim)\n",
    "    c2 = torch.randn(32, config.max_seq_len // (config.combine_factor**2), config.embed_dim)\n",
    "    xfs = (x0, c1, c2)\n",
    "    targets = (x0 + torch.randn_like(x0), c1 + torch.randn_like(c1), c2 + torch.randn_like(c2))\n",
    "    output = module(xfs, targets)\n",
    "    print(output)\n",
    "    config.levels = hold1\n",
    "    config.cos_loss = hold2\n",
    "    config.mse_loss = hold3\n",
    "    config.mae_loss = hold4\n",
    "del hold1, hold2, hold3, hold4, module, x0, c1, c2, xfs, targets, output\n",
    "module = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "55286553-43f4-4ac9-9926-5920d82b9078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering ConceptLoss.forward==========\n",
      "Inputs:\n",
      "Tuple 'xfs':\n",
      "    Tensor 'xfs[0]' shape: torch.Size([32, 256, 128])\n",
      "    Tensor 'xfs[1]' shape: torch.Size([32, 64, 128])\n",
      "    Tensor 'xfs[2]' shape: torch.Size([32, 16, 128])\n",
      "Tuple 'targets':\n",
      "    Tensor 'targets[0]' shape: torch.Size([32, 256, 128])\n",
      "    Tensor 'targets[1]' shape: torch.Size([32, 64, 128])\n",
      "    Tensor 'targets[2]' shape: torch.Size([32, 16, 128])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([])\n",
      "==========Exiting ConceptLoss.forward==========\n",
      "tensor(4.1888)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    hold1 = config.levels\n",
    "    hold2 = config.cos_loss\n",
    "    hold3 = config.mse_loss\n",
    "    hold4 = config.mae_loss\n",
    "    config.levels = 3\n",
    "    config.cos_loss = True\n",
    "    config.mse_loss = True\n",
    "    config.mae_loss = True\n",
    "    module = ConceptLoss(config)\n",
    "    module.enable_logging()\n",
    "    \n",
    "    x0 = torch.randn(32, config.max_seq_len // (config.combine_factor**0), config.embed_dim)\n",
    "    c1 = torch.randn(32, config.max_seq_len // (config.combine_factor**1), config.embed_dim)\n",
    "    c2 = torch.randn(32, config.max_seq_len // (config.combine_factor**2), config.embed_dim)\n",
    "    xfs = (x0, c1, c2)\n",
    "    targets = (x0 + torch.randn_like(x0), c1 + torch.randn_like(c1), c2 + torch.randn_like(c2))\n",
    "    output = module(xfs, targets)\n",
    "    print(output)\n",
    "    config.levels = hold1\n",
    "    config.cos_loss = hold2\n",
    "    config.mse_loss = hold3\n",
    "    config.mae_loss = hold4\n",
    "del hold1, hold2, hold3, hold4, module, x0, c1, c2, xfs, targets, output\n",
    "module = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2835a5-2186-4a0e-81a5-864cb94f1387",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d91c1b3c-19a4-4513-9460-78c244c80d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec_warning = False\n",
    "\n",
    "class Model(LoggingModule):\n",
    "    def __init__(self, config: Config, tokenizer: tokenizer):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        ### hyperparameters\n",
    "        self.max_seq_len = config.max_seq_len\n",
    "        self.sa_head_dim = config.sa_head_dim\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.embed_dim = config.embed_dim\n",
    "        self.combine_factor = config.combine_factor\n",
    "        self.levels = config.levels\n",
    "        \n",
    "        ### embedding\n",
    "        # the embedding matrix. for converting tokens to the first residual state, and the last residual state to logits\n",
    "        self.embedder = nn.Embedding(config.vocab_size, config.embed_dim)\n",
    "        # the padding vector to get used when sequence length isn't perfect\n",
    "        self.padding_vector = nn.Parameter(torch.zeros(config.embed_dim), requires_grad=True)\n",
    "        \n",
    "        # the function that combines embeddings into higher level concept residual states\n",
    "        self.embedding_combiner = CombineEmbeddings(config, self.padding_vector)\n",
    "\n",
    "        ### the actual bulk of the model\n",
    "        self.body = Body(config, self.embedder.weight, self.embedding_combiner)\n",
    "        \n",
    "        ### the loss functions\n",
    "        # lowest-level token model\n",
    "        self.ce_loss_fn = nn.CrossEntropyLoss()\n",
    "        # concept models\n",
    "        self.concept_loss_fn = ConceptLoss(config)\n",
    "        \n",
    "    @log_io\n",
    "    def forward(self,\n",
    "                input_token_ids: torch.Tensor, # a shape (batch_size, input_seq_len) list of integer token ids to run forward pass on\n",
    "                target_token_ids: torch.Tensor = None, # a shape (batch_size, input_seq_len + combo ** (levels-1)) list of token ids to train on\n",
    "                cvec_topk: int = None,\n",
    "                cvec_greedy: bool = False,\n",
    "                cvec_temp: float = 1.0,\n",
    "               ) -> torch.Tensor:\n",
    "\n",
    "        # create the tuple of initial residual states to calculate on\n",
    "        x0s = self.create_x0s(input_token_ids) # x0s are ordered token level -> highest concept level\n",
    "        \n",
    "        if target_token_ids is None: ### if we're doing inference\n",
    "            # the body of the model that iterates through the decoder & cross-attention layers\n",
    "            xfs = self.body(x0s, cvec_topk=cvec_topk, cvec_greedy=cvec_greedy, cvec_temp=cvec_temp) \n",
    "\n",
    "            # the actual token output logits we care about\n",
    "            logits = xfs[-1]\n",
    "            \n",
    "            # if we're not training, then we don't need to calculate loss\n",
    "            loss = None\n",
    "        else: ### if we're training\n",
    "            assert input_token_ids.shape[1] == target_token_ids.shape[1] - (self.combine_factor ** (self.levels - 1)), f'inputs:{input_token_ids.shape[1]} and targets:{target_token_ids.shape[1]} have unexpected shapes'\n",
    "\n",
    "            # create the tuple of target embedding vectors\n",
    "            targets = self.create_targets(target_token_ids, input_token_ids.shape[1]) # targets are ordered token level -> highest concept level\n",
    "\n",
    "            # the body of the model that iterates through the decoder & cross-attention layers\n",
    "            xfs = self.body(x0s, targets) # xfs are ordered highest concept level -> token level\n",
    "\n",
    "            ### first up is regular CE token loss\n",
    "            logits = xfs[-1]\n",
    "            batch_size, input_len, vocab_size = logits.shape\n",
    "            # splice target tokens to exclude the ones that were only to be used by concept levels\n",
    "            target_token_ids_spliced = target_token_ids[:,:input_len]\n",
    "            # we reshape our logits & targets before calculating cross-entropy loss\n",
    "            ce_loss = self.ce_loss_fn(logits.view(batch_size*input_len, vocab_size),\n",
    "                                      target_token_ids_spliced.reshape(batch_size*input_len))\n",
    "\n",
    "            ### the new thing, a regression loss for all our concept-embedding layers\n",
    "            concept_loss = self.concept_loss_fn(xfs, tuple(reversed(targets)))\n",
    "            # adding it all together\n",
    "            loss = ce_loss + concept_loss\n",
    "        \n",
    "        return logits, loss\n",
    "        \n",
    "    @log_io\n",
    "    def create_x0s(self, input_token_ids: torch.Tensor) -> Tuple[torch.Tensor]:\n",
    "        #print(f'input_token_ids: {input_token_ids.shape}\\n{input_token_ids[0,:32]}')\n",
    "        \n",
    "        # turn the input tokens into the first residual state using the embedding matrix\n",
    "        x0 = self.embedder(input_token_ids) # (batch_size, input_len, embed_dim)\n",
    "        #print(f'x0: {x0.shape}\\n{x0[0,0:(self.combine_factor**(config.levels-1))*2,:4]}')\n",
    "\n",
    "        # finding the number of padding vectos we have to use at the token level to ensure the cross-attention predictive mask will line up\n",
    "        remainder = x0.shape[1] % self.combine_factor\n",
    "        padding_needed = 0 if remainder == 0 else self.combine_factor - remainder\n",
    "\n",
    "        # do the actual padding for the token level\n",
    "        # once i get a more complicatead tokenizer would i replace this with a <|bos|> token? Would that token be unique to each level?\n",
    "        if padding_needed > 0:\n",
    "            # Replicate the padding vector the necessary number of times\n",
    "            padding = self.padding_vector.repeat(padding_needed, 1).unsqueeze(0).expand(x0.shape[0], -1, -1)\n",
    "            #print(f'padding: {padding.shape}')\n",
    "            \n",
    "            x0 = torch.cat([padding, x0], dim=1)\n",
    "            #print(f'x0 after padding: {x0.shape}\\n{x0[0,0:(self.combine_factor**(config.levels-1))*2,:4]}')\n",
    "        \n",
    "        # instantiate the tuple that'll hold all the residual states\n",
    "        x0s = (x0 * (self.embed_dim ** 0.5),) \n",
    "        \n",
    "        ### iterating through levels to create each higher-level concept residual state\n",
    "        for i in range(self.levels-1):\n",
    "            # combine into smaller tensor by adding token (or lower level concept) embeddings together\n",
    "            lvl_combo = self.combine_factor ** (i+1)\n",
    "            x0c = self.embedding_combiner(x0, lvl_combo) # c stands for concept\n",
    "            #print(f'x0c: {x0c.shape}\\n{x0c[0,0:self.combine_factor,:4]}')\n",
    "            \n",
    "            # finally scale & add it to the tuple of residual states\n",
    "            x0s += (x0c * (self.embed_dim ** 0.5),)\n",
    "        \n",
    "        return x0s\n",
    "\n",
    "    @log_io\n",
    "    def create_targets(self, target_token_ids: torch.Tensor, input_len: int) -> Tuple[torch.Tensor]:\n",
    "        #print(f'target_token_ids: {target_token_ids.shape}\\n{target_token_ids[0,:32]}')\n",
    "        \n",
    "        # turn the target tokens into the first residual state using the embedding matrix\n",
    "        token_lvl_target_token_ids = target_token_ids[:,1:1+input_len]\n",
    "        t0 = self.embedder(token_lvl_target_token_ids) # (batch_size, input_len, embed_dim)\n",
    "        #print(f'token_lvl_target_token_ids: {token_lvl_target_token_ids.shape}\\n{token_lvl_target_token_ids[0,:32]}')\n",
    "        #print(f't0: {t0.shape}\\n{t0[0,0:(self.combine_factor**(config.levels-1))*2,:4]}')\n",
    "        \n",
    "        # need to account for offsets in sequence length, which includes both an offset correction & padding vectors\n",
    "        remainder = t0.shape[1] % self.combine_factor\n",
    "        padding_needed = 0 if remainder == 0 else self.combine_factor - remainder\n",
    "        if padding_needed == 1:\n",
    "            t0 = torch.cat([self.embedder(target_token_ids[:,0].unsqueeze(1)), t0], dim=1)\n",
    "            #print(f't0 after padding: {t0.shape}\\n{t0[0,0:(self.combine_factor**(config.levels-1))*2,:4]}')\n",
    "        elif padding_needed > 1:\n",
    "            padding = self.padding_vector.repeat(padding_needed - 1, 1).unsqueeze(0).expand(t0.shape[0], -1, -1)\n",
    "            #print(f'padding: {padding.shape}')\n",
    "            t0 = torch.cat([padding, self.embedder(target_token_ids[:,0].unsqueeze(1)), t0], dim=1)\n",
    "            #print(f't0 after padding: {t0.shape}\\n{t0[0,0:(self.combine_factor**(config.levels-1))*2,:4]}')\n",
    "        \n",
    "        # instantiate the tuple that'll hold all the residual states\n",
    "        targets = (t0,) \n",
    "        \n",
    "        ### iterating through levels to create each higher-level concepts\n",
    "        for i in range(1, self.levels):\n",
    "            # calculate the correct combo factor for this level\n",
    "            lvl_combo = self.combine_factor ** i\n",
    "\n",
    "            # my subsetting here is all messy. doesn't properly take into account off-sequences & the padding token\n",
    "            # i think maybe i can fix this in the predictive mask once i make that part\n",
    "\n",
    "            # how many tokens off are we from a perfectly sized (multiple of lvl_combo) sequence, meaning how many padding vectors do we need?\n",
    "            remainder = input_len % self.combine_factor # will only ever be self.combine_factor -1 at most\n",
    "            offset = 0 if remainder == 0 else self.combine_factor - remainder\n",
    "            \n",
    "            # adjust input_len to ceiling the size necessary for this level\n",
    "            #input_len_adj = input_len + lvl_combo\n",
    "\n",
    "            # subset the currect targets to be predicted at this level\n",
    "            concept_lvl_target_token_ids = target_token_ids[:, lvl_combo - offset:lvl_combo + input_len]# - offset]\n",
    "            #print(f'concept_lvl_target_token_ids: {concept_lvl_target_token_ids.shape}\\n{concept_lvl_target_token_ids[0,:32]}')\n",
    "\n",
    "            # turn them into embeddings\n",
    "            t0c = self.embedder(concept_lvl_target_token_ids)\n",
    "\n",
    "            # combine the token embeddings into concepts\n",
    "            t0c = self.embedding_combiner(t0c, lvl_combo)\n",
    "            #print(f't0c: {t0c.shape}\\n{t0c[0,0:self.combine_factor,:4]}')\n",
    "            \n",
    "            # append to tuple\n",
    "            targets += (t0c,)\n",
    "        \n",
    "        return targets\n",
    "        \n",
    "    @log_io\n",
    "    def generate(self,\n",
    "                 prompt: str,\n",
    "                 output_len: int = 1, # the model will output 1 token by default\n",
    "                 temperature: float = 1.0, # 1.0 would be no effect\n",
    "                 top_p: float = 1.0,\n",
    "                 top_k: int = config.vocab_size,\n",
    "                ) -> str: \n",
    "        \"\"\" Wrapper around sampler() that deals with manipulation of the sequence \"\"\"\n",
    "        # encoding the prompt into token indices\n",
    "        tokens = self.tokenizer.encode(prompt)\n",
    "\n",
    "        # turning it into the right tensor shape\n",
    "        tokens = torch.tensor(tokens, device=config.device).unsqueeze(0)\n",
    "        \n",
    "        # we wouldn't want to go past the maximum context length we trained on\n",
    "        if len(tokens) + output_len > self.config.max_seq_len:\n",
    "            output_len = self.max_seq_len - len(tokens)\n",
    "            print(\"capping output at maximum sequence length\")\n",
    "\n",
    "        for i in range(output_len):\n",
    "            # get the model's output logits and ignore the loss, which would be a NoneType object\n",
    "            logits, _ = self(tokens[:,:self.max_seq_len])\n",
    "            \n",
    "            next_token = self.Sampler(logits, temperature, top_p, top_k)\n",
    "\n",
    "            # add our new token to the sequence\n",
    "            tokens = torch.cat((tokens, next_token), dim=1)\n",
    "\n",
    "        # resets this variable so that the corresponding warning in Body.concept_matchup can come up next time we perform inference\n",
    "        global cvec_warning \n",
    "        cvec_warning = False\n",
    "\n",
    "        # decode our list of tokens to an actual string\n",
    "        return self.tokenizer.decode(tokens.squeeze(0).tolist())\n",
    "\n",
    "    @torch.no_grad() # no need to keep track of gradients during inference\n",
    "    @log_io\n",
    "    def Sampler(\n",
    "        self,\n",
    "        logits: torch.Tensor, # shape (batch_size, input_len, vocab_size)\n",
    "        temperature: float, # controls how boring vs random the outputs should be\n",
    "        top_p: float, # the maximum cumulative probability of output options we're willing to consider\n",
    "        top_k: int, # the maximum number of output options we're willing to consider\n",
    "    ) -> torch.Tensor:\n",
    "        \n",
    "        # Select the last element for each sequence & apply temperature scaling\n",
    "        logits = logits[:,-1,:].div_(temperature) # -> (batch_size, vocab_size)\n",
    "        \n",
    "        # Calculate probabilities with softmax.\n",
    "        probs = torch.softmax(logits, dim=-1, dtype=torch.float) # dim=-1 is the vocab_size dimension that we calculate along\n",
    "        #print('first probs: ', probs)\n",
    "        \n",
    "        # sort the probabilities to for use in top-p & top-k. both are (batch_size, vocab_size)\n",
    "        probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
    "\n",
    "        ### calculating top-p\n",
    "        # creates same-size tensor of cumulatve probabilities instead of indivdiual probs\n",
    "        probs_sum = torch.cumsum(probs_sort, dim=-1) \n",
    "        # mask where 0's are top-p selections & 1's are to be excluded\n",
    "        top_ps_mask = (probs_sum - probs_sort) > top_p\n",
    "        # the original probabilities with excluded tokens changed to 0.0\n",
    "        probs_sort = torch.where(top_ps_mask, 0, probs_sort) \n",
    "\n",
    "        ### calculating top_k\n",
    "        # create a shape (vocab_size) tensor that just iterates up by 1's\n",
    "        top_ks_mask = torch.arange(probs_idx.shape[-1], device=probs_idx.device) \n",
    "        # expand our mask along the batch_size dimension to become size (batch_size, vocab_size)\n",
    "        top_ks_mask = top_ks_mask.expand(probs_idx.shape[0], -1)\n",
    "        # top_ks is a list of integers. we keep whichever entries in top_ks_mask are greater than their corresponding entries in top_ks\n",
    "        top_ks_mask = top_ks_mask >= top_k\n",
    "\n",
    "        # we'll be combining top-p with top-k and using whichever gives us fewer tokens. a very conservative approach\n",
    "        # this trims probs_sort to also fit within our top_k requirement\n",
    "        probs_sort = torch.where(top_ks_mask, 0, probs_sort)\n",
    "\n",
    "        # Re-normalization so that total probabilities add up to 1\n",
    "        probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
    "        \n",
    "        # now we rearrange the modified probabilities in probs_sort back to their original order according to probs_idx\n",
    "        probs = torch.gather(probs_sort, dim=-1, index=torch.argsort(probs_idx, dim=-1))\n",
    "        #print('probs after topp & k: ', probs)\n",
    "        \n",
    "        # samples from the distribution\n",
    "        next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        return next_token_id # returns the predicted token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce987b4a-ccae-4a8c-b05c-90a7db54a273",
   "metadata": {},
   "source": [
    "## demonstration/debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99219cd7-e131-4f8c-9f63-ee51b606fcbd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### training w/ regular length sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c8699759-c377-4f48-9e4f-b81bf0fc71a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering Model.forward==========\n",
      "Inputs:\n",
      "Tensor 'input_token_ids' shape: torch.Size([32, 128])\n",
      "Tensor 'target_token_ids' shape: torch.Size([32, 132])\n",
      "\n",
      "==========Entering Model.create_x0s==========\n",
      "Inputs:\n",
      "Tensor 'input_token_ids' shape: torch.Size([32, 128])\n",
      "\n",
      "==========Entering CombineEmbeddings.forward==========\n",
      "Inputs:\n",
      "Tensor 'tensor' shape: torch.Size([32, 128, 32])\n",
      "Integer 'combine_factor': Value=4\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 32, 32])\n",
      "==========Exiting CombineEmbeddings.forward==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([32, 128, 32])\n",
      "Tensor 'output[1]' shape: torch.Size([32, 32, 32])\n",
      "==========Exiting Model.create_x0s==========\n",
      "\n",
      "==========Entering Model.create_targets==========\n",
      "Inputs:\n",
      "Tensor 'target_token_ids' shape: torch.Size([32, 132])\n",
      "Integer 'input_len': Value=128\n",
      "target_token_ids: torch.Size([32, 132])\n",
      "tensor([399, 323,  78, 360, 263,  85, 353,  57,  82, 347, 143,  58, 158, 195,\n",
      "        357, 271, 478, 187, 283, 458, 342,  11, 390, 427, 160, 114, 317, 251,\n",
      "        459, 355, 210, 391])\n",
      "token_lvl_target_token_ids: torch.Size([32, 128])\n",
      "tensor([323,  78, 360, 263,  85, 353,  57,  82, 347, 143,  58, 158, 195, 357,\n",
      "        271, 478, 187, 283, 458, 342,  11, 390, 427, 160, 114, 317, 251, 459,\n",
      "        355, 210, 391, 397])\n",
      "t0: torch.Size([32, 128, 32])\n",
      "tensor([[-1.7506, -0.9634, -0.2178, -0.0687],\n",
      "        [ 0.0207, -1.3398, -0.4307, -0.2780],\n",
      "        [ 1.1901, -1.0175, -0.0489,  2.3864],\n",
      "        [ 0.3816, -1.4317, -0.2792,  0.3650],\n",
      "        [ 1.2356,  2.3866,  1.8080, -0.1694],\n",
      "        [ 1.4509,  1.0165,  1.6164,  0.8893],\n",
      "        [-0.0945, -0.1836, -1.0756,  0.9430],\n",
      "        [-0.9042,  0.3172,  0.3724, -0.2828]], grad_fn=<SliceBackward0>)\n",
      "concept_lvl_target_token_ids: torch.Size([32, 128])\n",
      "tensor([263,  85, 353,  57,  82, 347, 143,  58, 158, 195, 357, 271, 478, 187,\n",
      "        283, 458, 342,  11, 390, 427, 160, 114, 317, 251, 459, 355, 210, 391,\n",
      "        397, 414, 374, 188])\n",
      "\n",
      "==========Entering CombineEmbeddings.forward==========\n",
      "Inputs:\n",
      "Tensor 'tensor' shape: torch.Size([32, 128, 32])\n",
      "Integer 'combine_factor': Value=4\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 32, 32])\n",
      "==========Exiting CombineEmbeddings.forward==========\n",
      "t0c: torch.Size([32, 32, 32])\n",
      "tensor([[-1.2011e-01,  6.8683e-01, -4.4339e-01, -4.8020e-01],\n",
      "        [-5.8942e-01,  5.4070e-01,  8.7527e-02, -6.8751e-01],\n",
      "        [ 9.0611e-01,  2.1288e+00,  1.8055e+00, -2.3488e-01],\n",
      "        [ 1.7217e-01,  1.4094e+00, -1.2368e+00, -1.0036e-03]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([32, 128, 32])\n",
      "Tensor 'output[1]' shape: torch.Size([32, 32, 32])\n",
      "==========Exiting Model.create_targets==========\n",
      "\n",
      "==========Entering ConceptLoss.forward==========\n",
      "Inputs:\n",
      "Tuple 'xfs':\n",
      "    Tensor 'xfs[0]' shape: torch.Size([32, 32, 32])\n",
      "    Tensor 'xfs[1]' shape: torch.Size([32, 128, 512])\n",
      "Tuple 'targets':\n",
      "    Tensor 'targets[0]' shape: torch.Size([32, 32, 32])\n",
      "    Tensor 'targets[1]' shape: torch.Size([32, 128, 32])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([])\n",
      "==========Exiting ConceptLoss.forward==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([32, 128, 512])\n",
      "Tensor 'output[1]' shape: torch.Size([])\n",
      "==========Exiting Model.forward==========\n"
     ]
    }
   ],
   "source": [
    "hold = config.levels\n",
    "config.levels = 2\n",
    "module = Model(config, tokenizer)\n",
    "module.enable_logging()\n",
    "\n",
    "### enabling logging for sub-modules\n",
    "module.embedding_combiner.enable_logging()\n",
    "#module.body.enable_logging()\n",
    "module.concept_loss_fn.enable_logging()\n",
    "\n",
    "### disabling logging for sub-functions\n",
    "#module.disable_function_logging('create_x0s')\n",
    "#module.disable_function_logging('create_targets')\n",
    "#module.disable_function_logging('generate')\n",
    "#module.disable_function_logging('sampler')\n",
    "\n",
    "token_ids = torch.randint(config.vocab_size, (32, config.max_seq_len + config.combine_factor))\n",
    "input_token_ids = token_ids[:,:config.max_seq_len]\n",
    "target_token_ids = token_ids\n",
    "output, loss = module(input_token_ids, target_token_ids)\n",
    "config.levels = hold\n",
    "del hold, module, token_ids, input_token_ids, target_token_ids, output, loss\n",
    "module = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6bee464f-eaaf-4834-ab8d-5906536170d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering Model.forward==========\n",
      "Inputs:\n",
      "Tensor 'input_token_ids' shape: torch.Size([32, 128])\n",
      "Tensor 'target_token_ids' shape: torch.Size([32, 144])\n",
      "\n",
      "==========Entering Model.create_x0s==========\n",
      "Inputs:\n",
      "Tensor 'input_token_ids' shape: torch.Size([32, 128])\n",
      "\n",
      "==========Entering CombineEmbeddings.forward==========\n",
      "Inputs:\n",
      "Tensor 'tensor' shape: torch.Size([32, 128, 32])\n",
      "Integer 'combine_factor': Value=4\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 32, 32])\n",
      "==========Exiting CombineEmbeddings.forward==========\n",
      "\n",
      "==========Entering CombineEmbeddings.forward==========\n",
      "Inputs:\n",
      "Tensor 'tensor' shape: torch.Size([32, 128, 32])\n",
      "Integer 'combine_factor': Value=16\n",
      "\n",
      "==========Entering CombineEmbeddings.forward==========\n",
      "Inputs:\n",
      "Tensor 'tensor' shape: torch.Size([32, 128, 32])\n",
      "Integer 'combine_factor': Value=4\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 32, 32])\n",
      "==========Exiting CombineEmbeddings.forward==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 8, 32])\n",
      "==========Exiting CombineEmbeddings.forward==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([32, 128, 32])\n",
      "Tensor 'output[1]' shape: torch.Size([32, 32, 32])\n",
      "Tensor 'output[2]' shape: torch.Size([32, 8, 32])\n",
      "==========Exiting Model.create_x0s==========\n",
      "\n",
      "==========Entering Model.create_targets==========\n",
      "Inputs:\n",
      "Tensor 'target_token_ids' shape: torch.Size([32, 144])\n",
      "Integer 'input_len': Value=128\n",
      "target_token_ids: torch.Size([32, 144])\n",
      "tensor([ 21, 476, 421, 280, 382, 385, 111, 199, 434, 413, 330,  94, 455, 102,\n",
      "        141,  15, 101, 234, 386, 174,   2, 285, 363, 356, 491,  62, 288,  14,\n",
      "        133, 403,  97, 500])\n",
      "token_lvl_target_token_ids: torch.Size([32, 128])\n",
      "tensor([476, 421, 280, 382, 385, 111, 199, 434, 413, 330,  94, 455, 102, 141,\n",
      "         15, 101, 234, 386, 174,   2, 285, 363, 356, 491,  62, 288,  14, 133,\n",
      "        403,  97, 500,  82])\n",
      "t0: torch.Size([32, 128, 32])\n",
      "tensor([[-0.4823, -0.5245, -0.7526, -0.2982],\n",
      "        [-0.4767, -0.5211, -1.7428, -0.3853],\n",
      "        [ 0.3630,  1.0131,  0.1923,  0.0151],\n",
      "        [ 0.9731,  0.2591,  0.5129,  0.1124],\n",
      "        [-0.3098, -0.0223,  0.1517,  0.4681],\n",
      "        [ 0.1428, -0.1676,  1.5998, -0.1614],\n",
      "        [-0.7239,  0.9829,  1.3072, -0.4454],\n",
      "        [-0.5595, -0.8299, -0.3000,  1.5081],\n",
      "        [ 0.0435, -1.5505,  0.1719,  0.2398],\n",
      "        [ 0.0322, -1.4259, -1.8984, -2.1609],\n",
      "        [-0.2000,  0.6550, -0.5279,  0.0330],\n",
      "        [ 0.9946,  1.6962,  1.1829, -0.7620],\n",
      "        [ 1.0348, -0.0389, -0.8945,  0.8631],\n",
      "        [-0.3585,  0.4509,  0.7236, -1.1337],\n",
      "        [ 0.0949,  0.5177,  0.4478,  0.3386],\n",
      "        [-1.3074, -0.1430,  0.3130,  1.4881],\n",
      "        [-0.2736, -0.1099, -0.9630,  1.3959],\n",
      "        [ 0.1186, -0.9685, -0.1921, -1.0689],\n",
      "        [ 3.1854, -0.9211, -0.2653,  0.4628],\n",
      "        [-0.3155,  0.5415, -0.2551,  0.9559],\n",
      "        [ 1.7433,  0.1455, -2.0331,  0.7904],\n",
      "        [ 0.3564,  2.3714, -0.2054,  1.4852],\n",
      "        [-0.3273,  0.6659, -1.3880, -1.7468],\n",
      "        [ 0.4750, -0.4867, -0.2119, -0.0379],\n",
      "        [ 0.3329, -1.6566, -0.9522,  0.7216],\n",
      "        [-0.3920,  1.2738,  1.0490, -0.6500],\n",
      "        [ 0.7971, -0.6192,  1.2357, -0.3751],\n",
      "        [ 1.5439,  0.5115,  0.4421,  1.6461],\n",
      "        [-1.0142,  2.0341,  0.4124,  1.4244],\n",
      "        [ 1.1360,  0.6279, -0.4706,  0.1589],\n",
      "        [-0.7522, -1.0721, -0.1138,  0.8793],\n",
      "        [ 0.4113,  0.6890, -0.2701, -1.0519]], grad_fn=<SliceBackward0>)\n",
      "concept_lvl_target_token_ids: torch.Size([32, 128])\n",
      "tensor([382, 385, 111, 199, 434, 413, 330,  94, 455, 102, 141,  15, 101, 234,\n",
      "        386, 174,   2, 285, 363, 356, 491,  62, 288,  14, 133, 403,  97, 500,\n",
      "         82, 108, 311,  62])\n",
      "\n",
      "==========Entering CombineEmbeddings.forward==========\n",
      "Inputs:\n",
      "Tensor 'tensor' shape: torch.Size([32, 128, 32])\n",
      "Integer 'combine_factor': Value=4\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 32, 32])\n",
      "==========Exiting CombineEmbeddings.forward==========\n",
      "t0c: torch.Size([32, 32, 32])\n",
      "tensor([[ 0.2501, -0.0648,  0.9436,  0.0809],\n",
      "        [ 1.3797, -0.9560,  0.0285,  0.1081],\n",
      "        [-0.2897, -0.9299,  0.6365, -0.0154],\n",
      "        [ 1.1937,  1.4533,  1.4118, -1.0362]], grad_fn=<SliceBackward0>)\n",
      "concept_lvl_target_token_ids: torch.Size([32, 128])\n",
      "tensor([101, 234, 386, 174,   2, 285, 363, 356, 491,  62, 288,  14, 133, 403,\n",
      "         97, 500,  82, 108, 311,  62, 403, 455, 301, 121,  41, 321, 353,  48,\n",
      "        244, 218, 304, 301])\n",
      "\n",
      "==========Entering CombineEmbeddings.forward==========\n",
      "Inputs:\n",
      "Tensor 'tensor' shape: torch.Size([32, 128, 32])\n",
      "Integer 'combine_factor': Value=16\n",
      "\n",
      "==========Entering CombineEmbeddings.forward==========\n",
      "Inputs:\n",
      "Tensor 'tensor' shape: torch.Size([32, 128, 32])\n",
      "Integer 'combine_factor': Value=4\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 32, 32])\n",
      "==========Exiting CombineEmbeddings.forward==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 8, 32])\n",
      "==========Exiting CombineEmbeddings.forward==========\n",
      "t0c: torch.Size([32, 8, 32])\n",
      "tensor([[ 0.2221,  1.0339,  0.4152, -1.7610],\n",
      "        [-0.0879,  0.9385,  1.6901,  1.3073],\n",
      "        [-1.6925, -0.2953,  1.3703, -0.2781],\n",
      "        [-0.9400,  1.4207,  0.8212, -0.7253]], grad_fn=<SliceBackward0>)\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([32, 128, 32])\n",
      "Tensor 'output[1]' shape: torch.Size([32, 32, 32])\n",
      "Tensor 'output[2]' shape: torch.Size([32, 8, 32])\n",
      "==========Exiting Model.create_targets==========\n",
      "\n",
      "==========Entering ConceptLoss.forward==========\n",
      "Inputs:\n",
      "Tuple 'xfs':\n",
      "    Tensor 'xfs[0]' shape: torch.Size([32, 8, 32])\n",
      "    Tensor 'xfs[1]' shape: torch.Size([32, 32, 32])\n",
      "    Tensor 'xfs[2]' shape: torch.Size([32, 128, 512])\n",
      "Tuple 'targets':\n",
      "    Tensor 'targets[0]' shape: torch.Size([32, 8, 32])\n",
      "    Tensor 'targets[1]' shape: torch.Size([32, 32, 32])\n",
      "    Tensor 'targets[2]' shape: torch.Size([32, 128, 32])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([])\n",
      "==========Exiting ConceptLoss.forward==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([32, 128, 512])\n",
      "Tensor 'output[1]' shape: torch.Size([])\n",
      "==========Exiting Model.forward==========\n"
     ]
    }
   ],
   "source": [
    "hold = config.levels\n",
    "config.levels = 3\n",
    "module = Model(config, tokenizer)\n",
    "module.enable_logging()\n",
    "\n",
    "### enabling logging for sub-modules\n",
    "module.embedding_combiner.enable_logging()\n",
    "#module.body.enable_logging()\n",
    "module.concept_loss_fn.enable_logging()\n",
    "\n",
    "### disabling logging for sub-functions\n",
    "#module.disable_function_logging('create_x0s')\n",
    "#module.disable_function_logging('create_targets')\n",
    "#module.disable_function_logging('generate')\n",
    "#module.disable_function_logging('sampler')\n",
    "\n",
    "token_ids = torch.randint(config.vocab_size, (32, config.max_seq_len + (config.combine_factor ** (config.levels-1))))\n",
    "input_token_ids = token_ids[:,:config.max_seq_len]\n",
    "target_token_ids = token_ids\n",
    "output, loss = module(input_token_ids, target_token_ids)\n",
    "config.levels = hold\n",
    "del hold, module, token_ids, input_token_ids, target_token_ids, output, loss\n",
    "module = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "09e7e2ec-5797-4132-b83a-293829c643ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering Model.forward==========\n",
      "Inputs:\n",
      "Tensor 'input_token_ids' shape: torch.Size([32, 128])\n",
      "Tensor 'target_token_ids' shape: torch.Size([32, 192])\n",
      "\n",
      "==========Entering Model.create_x0s==========\n",
      "Inputs:\n",
      "Tensor 'input_token_ids' shape: torch.Size([32, 128])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([32, 128, 32])\n",
      "Tensor 'output[1]' shape: torch.Size([32, 32, 32])\n",
      "Tensor 'output[2]' shape: torch.Size([32, 8, 32])\n",
      "Tensor 'output[3]' shape: torch.Size([32, 2, 32])\n",
      "==========Exiting Model.create_x0s==========\n",
      "\n",
      "==========Entering Model.create_targets==========\n",
      "Inputs:\n",
      "Tensor 'target_token_ids' shape: torch.Size([32, 192])\n",
      "Integer 'input_len': Value=128\n",
      "target_token_ids: torch.Size([32, 192])\n",
      "tensor([366, 358, 297, 222, 158,  23, 247, 358, 313, 279, 504, 393, 387, 370,\n",
      "        425, 215, 107, 163,  14,  78, 350, 155, 260, 115, 128, 166, 125, 385,\n",
      "        235, 311,  55, 138])\n",
      "token_lvl_target_token_ids: torch.Size([32, 128])\n",
      "tensor([358, 297, 222, 158,  23, 247, 358, 313, 279, 504, 393, 387, 370, 425,\n",
      "        215, 107, 163,  14,  78, 350, 155, 260, 115, 128, 166, 125, 385, 235,\n",
      "        311,  55, 138, 156])\n",
      "t0: torch.Size([32, 128, 32])\n",
      "tensor([[ 1.8047e+00,  6.9837e-01, -1.1603e-01,  6.1935e-01],\n",
      "        [-4.1588e-01, -8.9018e-01,  7.2190e-01,  1.2412e+00],\n",
      "        [-6.1823e-01, -2.2285e-01, -4.2172e-01,  9.2846e-01],\n",
      "        [-9.6139e-01, -2.9086e-02, -1.4018e+00,  1.8248e-01],\n",
      "        [-7.0424e-01, -1.6249e-01, -1.0273e+00, -1.1398e-01],\n",
      "        [-1.6865e+00,  6.7737e-01,  1.0587e+00,  7.2156e-01],\n",
      "        [ 1.8047e+00,  6.9837e-01, -1.1603e-01,  6.1935e-01],\n",
      "        [-7.6573e-01, -1.6523e+00,  1.8602e+00,  1.5517e+00],\n",
      "        [ 2.0906e+00, -5.7504e-01, -1.2361e+00, -4.8851e-01],\n",
      "        [ 2.3956e-01, -6.7845e-01,  4.8357e-01, -2.9357e-01],\n",
      "        [-1.6025e+00, -1.7080e+00, -9.2591e-01,  1.9772e+00],\n",
      "        [-1.9212e+00, -1.4245e+00,  2.8747e+00,  2.1760e+00],\n",
      "        [ 9.0193e-01, -8.5408e-01, -9.9729e-02, -7.6329e-01],\n",
      "        [-1.3388e+00, -1.9802e+00, -3.7572e-01,  2.3622e-01],\n",
      "        [ 2.2525e-01,  1.3858e+00, -2.1925e-01,  1.0466e+00],\n",
      "        [ 6.1011e-01, -4.9144e-01,  2.9234e-01, -1.0164e+00],\n",
      "        [ 8.4113e-02, -2.2093e-01,  9.9620e-01, -1.5740e-02],\n",
      "        [-1.5612e+00,  1.5324e+00,  2.3550e-01,  8.0710e-01],\n",
      "        [ 5.8238e-01,  1.3480e+00,  9.0861e-01, -2.0045e+00],\n",
      "        [-8.0148e-02,  3.5495e-02,  7.7813e-02,  9.2970e-01],\n",
      "        [ 2.4530e-01, -1.7094e-03, -4.7792e-02, -4.5210e-01],\n",
      "        [ 1.3049e+00, -9.4557e-01, -1.1565e+00,  7.9126e-01],\n",
      "        [ 1.0856e+00, -4.7231e-01,  5.7842e-02, -1.6199e+00],\n",
      "        [ 2.6505e+00,  5.8937e-01, -1.3994e+00, -1.6865e+00],\n",
      "        [-3.9224e-01, -1.4682e+00,  2.6344e-01,  7.3580e-02],\n",
      "        [-1.7937e+00, -6.0424e-02, -1.0045e+00, -3.1982e-01],\n",
      "        [ 5.0152e-01,  8.1713e-01,  1.4561e+00,  1.0984e+00],\n",
      "        [ 7.3889e-01,  2.3877e-01, -1.8498e+00, -1.5205e+00],\n",
      "        [ 1.9869e-01, -6.8958e-01,  4.9058e-02, -2.9869e-01],\n",
      "        [ 4.1520e-01, -5.9774e-01, -5.3849e-01,  6.6477e-01],\n",
      "        [-4.4353e-01, -8.8237e-02,  1.1278e+00,  5.1891e-01],\n",
      "        [ 7.7662e-01, -8.1324e-01, -4.3339e-01, -5.8096e-01],\n",
      "        [ 8.5855e-01, -1.3001e-01, -7.5823e-01,  4.1430e-01],\n",
      "        [ 1.8811e+00,  4.6845e-01, -6.2287e-01, -9.9645e-01],\n",
      "        [-7.5949e-01,  1.4794e+00,  1.0713e+00,  3.4774e-01],\n",
      "        [-1.5692e+00,  1.0098e+00, -8.1478e-01, -1.6688e+00],\n",
      "        [-1.2040e-01,  1.7691e-01,  1.6247e+00,  1.0805e+00],\n",
      "        [ 1.2018e+00, -6.6514e-01, -2.6544e-01, -1.4529e+00],\n",
      "        [ 3.8707e-01, -1.8349e-01, -6.7539e-01,  1.4594e-01],\n",
      "        [-1.5287e-01,  5.4244e-01,  2.3928e-01, -1.5480e+00],\n",
      "        [ 2.2479e+00, -3.7953e-01,  1.4765e+00,  1.1392e+00],\n",
      "        [ 7.2466e-01, -1.8480e+00,  2.5913e-01, -5.1820e-01],\n",
      "        [-4.4260e-01, -4.3624e-01,  3.1837e-01, -5.2260e-01],\n",
      "        [-1.0220e-01, -4.5015e-02,  1.7293e+00,  5.4061e-01],\n",
      "        [ 1.2072e+00, -7.5498e-01,  6.1891e-01,  8.1052e-01],\n",
      "        [ 3.0680e-01, -4.0393e-02, -8.2057e-01,  1.2754e+00],\n",
      "        [-4.1267e-01,  8.2652e-01, -4.8248e-01,  3.7065e-01],\n",
      "        [ 6.0194e-01,  9.5922e-01,  1.3543e+00, -1.1270e+00],\n",
      "        [-3.6339e-01,  7.3495e-01, -3.6549e-01, -3.6305e-01],\n",
      "        [-3.9415e-01,  2.0593e-01,  2.2968e+00,  6.8489e-01],\n",
      "        [-1.0220e-01, -4.5015e-02,  1.7293e+00,  5.4061e-01],\n",
      "        [-1.7804e-01,  7.5983e-01,  1.1983e-01,  5.7220e-01],\n",
      "        [-9.8705e-01,  8.7160e-03,  3.1596e-01, -6.7602e-01],\n",
      "        [-8.7748e-01,  1.6446e+00, -1.1857e+00,  1.5811e+00],\n",
      "        [ 7.2369e-02,  8.9275e-01,  1.1087e+00,  1.2391e+00],\n",
      "        [-1.4546e+00, -1.5373e+00,  8.0765e-01, -9.9325e-01],\n",
      "        [ 2.0454e-01,  2.5219e+00, -1.8310e+00, -6.7326e-01],\n",
      "        [-9.0433e-01, -6.3602e-01, -3.0907e-01,  4.8686e-01],\n",
      "        [-2.0771e-01, -1.0426e+00, -1.7589e+00,  1.0386e+00],\n",
      "        [-9.1933e-01,  5.9761e-01,  6.3376e-01,  1.7648e+00],\n",
      "        [ 1.2816e+00,  1.1936e+00,  3.0108e+00,  8.9331e-01],\n",
      "        [ 1.6557e+00,  1.0682e-01, -5.6833e-01,  9.3215e-02],\n",
      "        [-2.0506e+00, -8.7251e-01, -8.1597e-02,  2.1233e+00],\n",
      "        [ 6.5913e-01,  3.9692e-01, -2.5207e+00, -9.0295e-01],\n",
      "        [-1.1897e+00, -4.7251e-01, -1.5952e+00, -7.3262e-01],\n",
      "        [-1.5958e+00, -2.2005e+00, -1.6908e-01, -1.7529e+00],\n",
      "        [ 6.6543e-01, -3.5239e-01,  2.1788e-01,  5.3735e-01],\n",
      "        [ 3.0569e-01,  7.2725e-01,  1.3851e+00, -4.5122e-01],\n",
      "        [ 8.6798e-01, -8.0450e-01,  4.7728e-01, -8.8473e-01],\n",
      "        [-1.4730e+00, -3.3307e+00, -2.7624e-01, -3.4957e-01],\n",
      "        [-4.1267e-01,  8.2652e-01, -4.8248e-01,  3.7065e-01],\n",
      "        [ 6.6543e-01, -3.5239e-01,  2.1788e-01,  5.3735e-01],\n",
      "        [ 6.1011e-01, -4.9144e-01,  2.9234e-01, -1.0164e+00],\n",
      "        [ 1.5010e+00,  6.0904e-01, -1.1070e+00,  1.2591e+00],\n",
      "        [ 2.3956e-01, -6.7845e-01,  4.8357e-01, -2.9357e-01],\n",
      "        [ 2.9910e-01, -3.0288e-01,  1.2020e+00, -5.9158e-01],\n",
      "        [-7.0424e-01, -1.6249e-01, -1.0273e+00, -1.1398e-01],\n",
      "        [-1.4207e+00,  2.3772e-01, -1.4861e+00,  1.6502e+00],\n",
      "        [-1.2381e-01, -2.0401e-01,  1.1984e+00,  7.5373e-01],\n",
      "        [-5.6375e-01,  1.7464e+00, -5.5750e-01,  1.2050e+00],\n",
      "        [-1.2158e+00,  7.5498e-01,  4.1316e-01,  8.2605e-01],\n",
      "        [-3.9415e-01,  2.0593e-01,  2.2968e+00,  6.8489e-01],\n",
      "        [ 7.2369e-02,  8.9275e-01,  1.1087e+00,  1.2391e+00],\n",
      "        [ 4.5070e-01, -4.7812e-01,  1.1668e+00, -1.9690e-01],\n",
      "        [-1.2784e+00,  9.6682e-01,  1.3056e+00, -3.8771e-01],\n",
      "        [ 1.0201e+00,  6.1496e-01,  2.0348e-01, -1.0199e+00],\n",
      "        [ 3.5130e-01,  5.8029e-01, -1.1048e+00, -4.9102e-01],\n",
      "        [-6.7786e-01,  8.0309e-01, -5.4663e-01,  1.2999e+00],\n",
      "        [ 1.2072e+00, -7.5498e-01,  6.1891e-01,  8.1052e-01],\n",
      "        [-6.2284e-01,  1.2562e-01, -3.6497e-01,  4.8346e-01],\n",
      "        [-1.5612e+00,  1.5324e+00,  2.3550e-01,  8.0710e-01],\n",
      "        [ 3.9342e-02, -9.3153e-01, -1.7009e-01,  3.5569e-01],\n",
      "        [ 7.8971e-03,  7.5556e-01,  1.0945e+00,  1.6280e+00],\n",
      "        [-3.0382e-02, -9.3182e-03, -5.3157e-01,  1.2518e-01],\n",
      "        [ 4.1520e-01, -5.9774e-01, -5.3849e-01,  6.6477e-01],\n",
      "        [-1.0186e+00,  3.6623e-01, -1.9395e-01, -1.9812e+00],\n",
      "        [ 7.5240e-02, -1.3361e+00,  8.3361e-05,  2.4569e+00],\n",
      "        [-5.7233e-01,  1.6369e-01,  4.6820e-01,  3.2968e-01],\n",
      "        [-1.0304e+00,  2.7339e-01,  6.5270e-01,  7.8487e-02],\n",
      "        [ 5.7466e-01,  9.9486e-01,  2.5688e-01,  9.4313e-01],\n",
      "        [ 5.8238e-01,  1.3480e+00,  9.0861e-01, -2.0045e+00],\n",
      "        [ 1.7604e+00, -1.3320e-01, -2.0526e+00, -1.9707e+00],\n",
      "        [ 3.9956e-01, -1.3788e-01, -3.3495e-02, -1.3019e+00],\n",
      "        [ 4.1520e-01, -5.9774e-01, -5.3849e-01,  6.6477e-01],\n",
      "        [ 7.7662e-01, -8.1324e-01, -4.3339e-01, -5.8096e-01],\n",
      "        [-1.6365e+00, -1.3261e-01,  2.2587e-01, -2.9663e-01],\n",
      "        [ 7.3363e-01,  1.4181e+00, -2.8433e+00, -3.6244e-01],\n",
      "        [ 5.3068e-01,  7.5939e-01,  1.7251e+00, -7.4827e-01],\n",
      "        [-4.3381e-02, -1.2387e+00, -7.4384e-01,  2.9871e-01],\n",
      "        [-1.2844e+00, -1.2445e+00,  2.6818e-01,  1.2054e+00],\n",
      "        [-2.8953e-01,  2.4572e+00,  5.8038e-01, -8.0767e-01],\n",
      "        [-1.2784e+00,  9.6682e-01,  1.3056e+00, -3.8771e-01],\n",
      "        [-3.6252e-01,  1.0640e+00,  2.5447e-01,  4.9877e-01],\n",
      "        [ 5.8828e-01,  6.0883e-01, -2.1217e+00, -1.8333e-01],\n",
      "        [ 1.1028e+00, -1.3127e+00,  6.9652e-01, -2.0604e-01],\n",
      "        [-9.8025e-01,  4.9831e-01, -2.5880e-01, -1.6195e+00],\n",
      "        [ 3.2268e-01,  1.3750e-01,  9.6680e-01,  4.2474e-01],\n",
      "        [ 1.8047e+00,  6.9837e-01, -1.1603e-01,  6.1935e-01],\n",
      "        [-3.1296e-01,  5.2539e-01, -3.3942e-01, -1.3686e+00],\n",
      "        [-1.3430e-01, -6.1845e-01,  4.5759e-01, -1.7761e-01],\n",
      "        [ 9.6920e-01, -1.0961e+00, -1.5482e-01, -3.4519e-01],\n",
      "        [-7.8096e-01, -6.6021e-01,  6.2460e-01, -1.4110e+00],\n",
      "        [ 3.7895e-01, -2.5417e+00,  1.1042e+00, -1.8797e+00],\n",
      "        [ 7.5240e-02, -1.3361e+00,  8.3361e-05,  2.4569e+00],\n",
      "        [-7.1320e-01,  1.2948e+00,  1.3672e-01, -1.6638e-02],\n",
      "        [ 1.8649e+00,  5.8216e-01, -1.7117e-01,  1.9408e+00],\n",
      "        [ 7.7631e-01,  3.1845e-01,  1.8065e+00,  8.9357e-01],\n",
      "        [ 5.0152e-01,  8.1713e-01,  1.4561e+00,  1.0984e+00]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "concept_lvl_target_token_ids: torch.Size([32, 128])\n",
      "tensor([158,  23, 247, 358, 313, 279, 504, 393, 387, 370, 425, 215, 107, 163,\n",
      "         14,  78, 350, 155, 260, 115, 128, 166, 125, 385, 235, 311,  55, 138,\n",
      "        156, 303,  88,  69])\n",
      "t0c: torch.Size([32, 32, 32])\n",
      "tensor([[-1.2049, -1.7445,  0.3358,  1.4805],\n",
      "        [ 0.7791, -1.6294, -0.8404, -0.5177],\n",
      "        [ 0.0805, -0.3114,  0.6975, -0.6190],\n",
      "        [ 0.3297, -0.3104, -0.1449,  1.7280]], grad_fn=<SliceBackward0>)\n",
      "concept_lvl_target_token_ids: torch.Size([32, 128])\n",
      "tensor([107, 163,  14,  78, 350, 155, 260, 115, 128, 166, 125, 385, 235, 311,\n",
      "         55, 138, 156, 303,  88,  69, 165,  92, 466, 342, 299, 164, 337, 490,\n",
      "        233, 336, 231, 269])\n",
      "t0c: torch.Size([32, 8, 32])\n",
      "tensor([[ 1.6667, -1.0336,  1.4609,  1.6556],\n",
      "        [-0.0669,  0.3061, -0.2564, -1.2561],\n",
      "        [ 0.4040, -1.8546, -0.5774, -0.9462],\n",
      "        [-0.4114,  0.2009, -1.0175,  0.4253]], grad_fn=<SliceBackward0>)\n",
      "concept_lvl_target_token_ids: torch.Size([32, 128])\n",
      "tensor([100, 471, 335, 318, 238, 295, 294, 269, 318, 107, 139, 504, 307,  23,\n",
      "        434, 361, 145, 319, 410, 230, 280,  47, 355, 116, 315, 336, 396,  14,\n",
      "        121, 478, 151,  55])\n",
      "t0c: torch.Size([32, 2, 32])\n",
      "tensor([[ 1.2880,  0.2062, -1.2742,  1.0375],\n",
      "        [-1.1033, -1.0328,  0.9946, -0.1152]], grad_fn=<SliceBackward0>)\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([32, 128, 32])\n",
      "Tensor 'output[1]' shape: torch.Size([32, 32, 32])\n",
      "Tensor 'output[2]' shape: torch.Size([32, 8, 32])\n",
      "Tensor 'output[3]' shape: torch.Size([32, 2, 32])\n",
      "==========Exiting Model.create_targets==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([32, 128, 512])\n",
      "Tensor 'output[1]' shape: torch.Size([])\n",
      "==========Exiting Model.forward==========\n"
     ]
    }
   ],
   "source": [
    "hold = config.levels\n",
    "config.levels = 4\n",
    "module = Model(config, tokenizer)\n",
    "module.enable_logging()\n",
    "\n",
    "### enabling logging for sub-modules\n",
    "#module.embedding_combiner.enable_logging()\n",
    "#module.body.enable_logging()\n",
    "#module.concept_loss_fn.enable_logging()\n",
    "\n",
    "### disabling logging for sub-functions\n",
    "#module.disable_function_logging('create_x0s')\n",
    "#module.disable_function_logging('create_targets')\n",
    "#module.disable_function_logging('generate')\n",
    "#module.disable_function_logging('sampler')\n",
    "\n",
    "token_ids = torch.randint(config.vocab_size, (32, config.max_seq_len + (config.combine_factor ** (config.levels-1))))\n",
    "input_token_ids = token_ids[:,:config.max_seq_len]\n",
    "target_token_ids = token_ids\n",
    "output, loss = module(input_token_ids, target_token_ids)\n",
    "config.levels = hold\n",
    "del hold, module, token_ids, input_token_ids, target_token_ids, output, loss\n",
    "module = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3e71f1-4cd8-46e5-b890-588dcb3e210c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### training w/ offset length sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "426e71f9-ecd2-471b-8b16-02f3aa459828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "randomly chosen offset: 3\n",
      "\n",
      "==========Entering Model.forward==========\n",
      "Inputs:\n",
      "Tensor 'input_token_ids' shape: torch.Size([32, 125])\n",
      "Tensor 'target_token_ids' shape: torch.Size([32, 129])\n",
      "\n",
      "==========Entering Model.create_x0s==========\n",
      "Inputs:\n",
      "Tensor 'input_token_ids' shape: torch.Size([32, 125])\n",
      "\n",
      "==========Entering CombineEmbeddings.forward==========\n",
      "Inputs:\n",
      "Tensor 'tensor' shape: torch.Size([32, 128, 32])\n",
      "Integer 'combine_factor': Value=4\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 32, 32])\n",
      "==========Exiting CombineEmbeddings.forward==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([32, 128, 32])\n",
      "Tensor 'output[1]' shape: torch.Size([32, 32, 32])\n",
      "==========Exiting Model.create_x0s==========\n",
      "\n",
      "==========Entering Model.create_targets==========\n",
      "Inputs:\n",
      "Tensor 'target_token_ids' shape: torch.Size([32, 129])\n",
      "Integer 'input_len': Value=125\n",
      "target_token_ids: torch.Size([32, 129])\n",
      "tensor([175, 356,  55, 126, 457, 470, 335, 151,  84, 287, 473, 126, 346, 320,\n",
      "         59, 476, 340,  35, 410, 248, 244, 170, 107, 458, 130, 240, 423, 457,\n",
      "        326,   1, 379, 494])\n",
      "token_lvl_target_token_ids: torch.Size([32, 125])\n",
      "tensor([356,  55, 126, 457, 470, 335, 151,  84, 287, 473, 126, 346, 320,  59,\n",
      "        476, 340,  35, 410, 248, 244, 170, 107, 458, 130, 240, 423, 457, 326,\n",
      "          1, 379, 494,  28])\n",
      "t0: torch.Size([32, 125, 32])\n",
      "tensor([[-2.1605, -0.3922, -0.6262, -0.4578],\n",
      "        [-1.1742, -1.2997,  0.2983,  0.6566],\n",
      "        [ 1.1779, -1.1486,  0.6999, -0.1288],\n",
      "        [-0.9687, -0.6900,  0.5415,  1.7575],\n",
      "        [-1.4651,  1.1049, -0.1644,  1.1973],\n",
      "        [-2.4041,  0.1719,  1.5909, -0.6291],\n",
      "        [ 0.3622,  2.6686, -0.5034, -0.2634],\n",
      "        [-0.0516, -0.4526, -1.5880,  0.6638]], grad_fn=<SliceBackward0>)\n",
      "padding: torch.Size([32, 2, 32])\n",
      "t0 after padding: torch.Size([32, 128, 32])\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.6856, -0.0153,  0.5878, -0.4851],\n",
      "        [-2.1605, -0.3922, -0.6262, -0.4578],\n",
      "        [-1.1742, -1.2997,  0.2983,  0.6566],\n",
      "        [ 1.1779, -1.1486,  0.6999, -0.1288],\n",
      "        [-0.9687, -0.6900,  0.5415,  1.7575],\n",
      "        [-1.4651,  1.1049, -0.1644,  1.1973]], grad_fn=<SliceBackward0>)\n",
      "concept_lvl_target_token_ids: torch.Size([32, 128])\n",
      "tensor([356,  55, 126, 457, 470, 335, 151,  84, 287, 473, 126, 346, 320,  59,\n",
      "        476, 340,  35, 410, 248, 244, 170, 107, 458, 130, 240, 423, 457, 326,\n",
      "          1, 379, 494,  28])\n",
      "\n",
      "==========Entering CombineEmbeddings.forward==========\n",
      "Inputs:\n",
      "Tensor 'tensor' shape: torch.Size([32, 128, 32])\n",
      "Integer 'combine_factor': Value=4\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 32, 32])\n",
      "==========Exiting CombineEmbeddings.forward==========\n",
      "t0c: torch.Size([32, 32, 32])\n",
      "tensor([[-0.2885,  0.8811,  0.2050,  0.9346],\n",
      "        [-1.5093,  0.8795, -0.2377,  0.4881],\n",
      "        [-1.0704, -1.7613, -0.0554,  0.0512],\n",
      "        [ 0.8070,  0.8294, -1.6465, -0.8362]], grad_fn=<SliceBackward0>)\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([32, 128, 32])\n",
      "Tensor 'output[1]' shape: torch.Size([32, 32, 32])\n",
      "==========Exiting Model.create_targets==========\n",
      "\n",
      "==========Entering Body.forward==========\n",
      "Inputs:\n",
      "Tuple 'x0s':\n",
      "    Tensor 'x0s[0]' shape: torch.Size([32, 128, 32])\n",
      "    Tensor 'x0s[1]' shape: torch.Size([32, 32, 32])\n",
      "Tuple 'targets':\n",
      "    Tensor 'targets[0]' shape: torch.Size([32, 128, 32])\n",
      "    Tensor 'targets[1]' shape: torch.Size([32, 32, 32])\n",
      "\n",
      "==========Entering Body.forward_training==========\n",
      "Inputs:\n",
      "Tuple 'x0s':\n",
      "    Tensor 'x0s[0]' shape: torch.Size([32, 128, 32])\n",
      "    Tensor 'x0s[1]' shape: torch.Size([32, 32, 32])\n",
      "Tuple 'targets':\n",
      "    Tensor 'targets[0]' shape: torch.Size([32, 128, 32])\n",
      "    Tensor 'targets[1]' shape: torch.Size([32, 32, 32])\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 32, 32])\n",
      "Integer 'i': Value=0\n",
      "Other-type 'c': Type=NoneType, Value=None\n",
      "Integer 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 32, 32])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 128, 32])\n",
      "Integer 'i': Value=1\n",
      "Tensor 'c' shape: torch.Size([32, 32, 32])\n",
      "Integer 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 128, 32])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([32, 32, 32])\n",
      "Tensor 'output[1]' shape: torch.Size([32, 128, 512])\n",
      "==========Exiting Body.forward_training==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([32, 32, 32])\n",
      "Tensor 'output[1]' shape: torch.Size([32, 128, 512])\n",
      "==========Exiting Body.forward==========\n",
      "\n",
      "==========Entering ConceptLoss.forward==========\n",
      "Inputs:\n",
      "Tuple 'xfs':\n",
      "    Tensor 'xfs[0]' shape: torch.Size([32, 32, 32])\n",
      "    Tensor 'xfs[1]' shape: torch.Size([32, 128, 512])\n",
      "Tuple 'targets':\n",
      "    Tensor 'targets[0]' shape: torch.Size([32, 32, 32])\n",
      "    Tensor 'targets[1]' shape: torch.Size([32, 128, 32])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([])\n",
      "==========Exiting ConceptLoss.forward==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([32, 128, 512])\n",
      "Tensor 'output[1]' shape: torch.Size([])\n",
      "==========Exiting Model.forward==========\n"
     ]
    }
   ],
   "source": [
    "hold = config.levels\n",
    "config.levels = 2\n",
    "module = Model(config, tokenizer)\n",
    "module.enable_logging()\n",
    "\n",
    "### enabling logging for sub-modules\n",
    "module.embedding_combiner.enable_logging()\n",
    "module.body.enable_logging()\n",
    "module.concept_loss_fn.enable_logging()\n",
    "\n",
    "### disabling logging for sub-functions\n",
    "#module.disable_function_logging('create_x0s')\n",
    "#module.disable_function_logging('create_targets')\n",
    "#module.disable_function_logging('generate')\n",
    "#module.disable_function_logging('sampler')\n",
    "\n",
    "offset = random.randint(1, config.combine_factor-1)\n",
    "print(f'randomly chosen offset: {offset}')\n",
    "token_ids = torch.randint(config.vocab_size, (32, config.max_seq_len + config.combine_factor - offset))\n",
    "input_token_ids = token_ids[:,:config.max_seq_len - offset]\n",
    "target_token_ids = token_ids\n",
    "output, loss = module(input_token_ids, target_token_ids)\n",
    "config.levels = hold\n",
    "del hold, module, offset, token_ids, input_token_ids, target_token_ids, output, loss\n",
    "module = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c1a30932-f8ce-495c-94e0-9e2993328df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "randomly chosen offset: 3\n",
      "\n",
      "==========Entering Model.forward==========\n",
      "Inputs:\n",
      "Tensor 'input_token_ids' shape: torch.Size([32, 125])\n",
      "Tensor 'target_token_ids' shape: torch.Size([32, 141])\n",
      "\n",
      "==========Entering Model.create_x0s==========\n",
      "Inputs:\n",
      "Tensor 'input_token_ids' shape: torch.Size([32, 125])\n",
      "\n",
      "==========Entering CombineEmbeddings.forward==========\n",
      "Inputs:\n",
      "Tensor 'tensor' shape: torch.Size([32, 128, 32])\n",
      "Integer 'combine_factor': Value=4\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 32, 32])\n",
      "==========Exiting CombineEmbeddings.forward==========\n",
      "\n",
      "==========Entering CombineEmbeddings.forward==========\n",
      "Inputs:\n",
      "Tensor 'tensor' shape: torch.Size([32, 128, 32])\n",
      "Integer 'combine_factor': Value=16\n",
      "\n",
      "==========Entering CombineEmbeddings.forward==========\n",
      "Inputs:\n",
      "Tensor 'tensor' shape: torch.Size([32, 128, 32])\n",
      "Integer 'combine_factor': Value=4\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 32, 32])\n",
      "==========Exiting CombineEmbeddings.forward==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 8, 32])\n",
      "==========Exiting CombineEmbeddings.forward==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([32, 128, 32])\n",
      "Tensor 'output[1]' shape: torch.Size([32, 32, 32])\n",
      "Tensor 'output[2]' shape: torch.Size([32, 8, 32])\n",
      "==========Exiting Model.create_x0s==========\n",
      "\n",
      "==========Entering Model.create_targets==========\n",
      "Inputs:\n",
      "Tensor 'target_token_ids' shape: torch.Size([32, 141])\n",
      "Integer 'input_len': Value=125\n",
      "target_token_ids: torch.Size([32, 141])\n",
      "tensor([498, 279,  63, 196, 416,  72, 256, 359, 458, 203, 303, 274, 152,  13,\n",
      "        446, 101,  72, 501,  85, 140, 380, 384,  36, 389, 177, 482, 197,  81,\n",
      "        383, 191,  97, 133])\n",
      "token_lvl_target_token_ids: torch.Size([32, 125])\n",
      "tensor([279,  63, 196, 416,  72, 256, 359, 458, 203, 303, 274, 152,  13, 446,\n",
      "        101,  72, 501,  85, 140, 380, 384,  36, 389, 177, 482, 197,  81, 383,\n",
      "        191,  97, 133, 183])\n",
      "t0: torch.Size([32, 125, 32])\n",
      "tensor([[-0.2882,  1.4133,  1.1632, -0.2365],\n",
      "        [ 1.0777,  0.3895,  0.1164,  2.8203],\n",
      "        [-0.9046, -1.1301,  0.5117,  0.5612],\n",
      "        [ 0.1135, -0.1708,  0.7878, -1.1178],\n",
      "        [-1.7608, -1.2408,  0.2922,  0.0647],\n",
      "        [ 1.7264, -0.4562,  1.6235, -0.0561],\n",
      "        [-1.3106, -0.2121,  0.7273,  0.3277],\n",
      "        [-0.4048, -2.0171,  1.2460,  0.7704],\n",
      "        [-0.6007,  0.0603, -0.3411,  1.1453],\n",
      "        [ 0.1906, -0.2663, -0.6551, -1.9676],\n",
      "        [ 0.8836, -0.3190, -1.2708,  0.1424],\n",
      "        [-0.5694, -0.7517, -0.7319, -0.6652],\n",
      "        [ 0.1124, -0.9175, -0.6112,  0.1294],\n",
      "        [ 1.7567,  0.5152, -1.6373, -0.2401],\n",
      "        [ 1.8506,  1.2818,  0.6872,  0.9336],\n",
      "        [-1.7608, -1.2408,  0.2922,  0.0647],\n",
      "        [-0.5622, -0.7117, -0.3337, -2.9881],\n",
      "        [-1.0329,  1.1383, -0.2175,  0.3477],\n",
      "        [ 1.1407,  0.0460, -1.0642, -0.9652],\n",
      "        [-1.1925, -2.3089, -1.2507, -0.1906],\n",
      "        [ 0.1775, -1.0658, -1.6086,  0.0060],\n",
      "        [ 1.4061,  1.5154, -0.0993,  0.3689],\n",
      "        [ 0.3451, -1.7876, -0.4511,  0.5199],\n",
      "        [ 0.6998,  0.6072, -1.1697,  0.0196],\n",
      "        [ 0.3781,  0.7523,  0.0798,  0.1276],\n",
      "        [ 0.4703, -1.2506, -1.2421, -0.0247],\n",
      "        [ 0.2345, -1.0351,  0.5350, -0.6490],\n",
      "        [-0.3463, -0.8188, -0.6738,  1.0933],\n",
      "        [ 0.0959,  0.7781,  0.2425,  0.3242],\n",
      "        [ 0.1323,  1.7864,  0.4470,  1.6441],\n",
      "        [ 0.5382,  0.6210,  0.0259, -0.1902],\n",
      "        [ 1.0458,  1.3723, -0.5228, -0.0346]], grad_fn=<SliceBackward0>)\n",
      "padding: torch.Size([32, 2, 32])\n",
      "t0 after padding: torch.Size([32, 128, 32])\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1163,  1.5399, -1.3356,  0.0306],\n",
      "        [-0.2882,  1.4133,  1.1632, -0.2365],\n",
      "        [ 1.0777,  0.3895,  0.1164,  2.8203],\n",
      "        [-0.9046, -1.1301,  0.5117,  0.5612],\n",
      "        [ 0.1135, -0.1708,  0.7878, -1.1178],\n",
      "        [-1.7608, -1.2408,  0.2922,  0.0647],\n",
      "        [ 1.7264, -0.4562,  1.6235, -0.0561],\n",
      "        [-1.3106, -0.2121,  0.7273,  0.3277],\n",
      "        [-0.4048, -2.0171,  1.2460,  0.7704],\n",
      "        [-0.6007,  0.0603, -0.3411,  1.1453],\n",
      "        [ 0.1906, -0.2663, -0.6551, -1.9676],\n",
      "        [ 0.8836, -0.3190, -1.2708,  0.1424],\n",
      "        [-0.5694, -0.7517, -0.7319, -0.6652],\n",
      "        [ 0.1124, -0.9175, -0.6112,  0.1294],\n",
      "        [ 1.7567,  0.5152, -1.6373, -0.2401],\n",
      "        [ 1.8506,  1.2818,  0.6872,  0.9336],\n",
      "        [-1.7608, -1.2408,  0.2922,  0.0647],\n",
      "        [-0.5622, -0.7117, -0.3337, -2.9881],\n",
      "        [-1.0329,  1.1383, -0.2175,  0.3477],\n",
      "        [ 1.1407,  0.0460, -1.0642, -0.9652],\n",
      "        [-1.1925, -2.3089, -1.2507, -0.1906],\n",
      "        [ 0.1775, -1.0658, -1.6086,  0.0060],\n",
      "        [ 1.4061,  1.5154, -0.0993,  0.3689],\n",
      "        [ 0.3451, -1.7876, -0.4511,  0.5199],\n",
      "        [ 0.6998,  0.6072, -1.1697,  0.0196],\n",
      "        [ 0.3781,  0.7523,  0.0798,  0.1276],\n",
      "        [ 0.4703, -1.2506, -1.2421, -0.0247],\n",
      "        [ 0.2345, -1.0351,  0.5350, -0.6490],\n",
      "        [-0.3463, -0.8188, -0.6738,  1.0933],\n",
      "        [ 0.0959,  0.7781,  0.2425,  0.3242]], grad_fn=<SliceBackward0>)\n",
      "concept_lvl_target_token_ids: torch.Size([32, 128])\n",
      "tensor([279,  63, 196, 416,  72, 256, 359, 458, 203, 303, 274, 152,  13, 446,\n",
      "        101,  72, 501,  85, 140, 380, 384,  36, 389, 177, 482, 197,  81, 383,\n",
      "        191,  97, 133, 183])\n",
      "\n",
      "==========Entering CombineEmbeddings.forward==========\n",
      "Inputs:\n",
      "Tensor 'tensor' shape: torch.Size([32, 128, 32])\n",
      "Integer 'combine_factor': Value=4\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 32, 32])\n",
      "==========Exiting CombineEmbeddings.forward==========\n",
      "t0c: torch.Size([32, 32, 32])\n",
      "tensor([[-1.1896,  0.4165,  1.2797,  0.4386],\n",
      "        [ 0.6115, -0.1726,  0.2903, -0.6787],\n",
      "        [ 0.5624,  0.6950,  0.7827,  2.2207],\n",
      "        [ 0.6352, -0.9181, -1.9913,  0.2985]], grad_fn=<SliceBackward0>)\n",
      "concept_lvl_target_token_ids: torch.Size([32, 128])\n",
      "tensor([ 13, 446, 101,  72, 501,  85, 140, 380, 384,  36, 389, 177, 482, 197,\n",
      "         81, 383, 191,  97, 133, 183, 397,  95, 138, 370, 508, 268,  74,  92,\n",
      "        469, 407, 469, 470])\n",
      "\n",
      "==========Entering CombineEmbeddings.forward==========\n",
      "Inputs:\n",
      "Tensor 'tensor' shape: torch.Size([32, 128, 32])\n",
      "Integer 'combine_factor': Value=16\n",
      "\n",
      "==========Entering CombineEmbeddings.forward==========\n",
      "Inputs:\n",
      "Tensor 'tensor' shape: torch.Size([32, 128, 32])\n",
      "Integer 'combine_factor': Value=4\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 32, 32])\n",
      "==========Exiting CombineEmbeddings.forward==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 8, 32])\n",
      "==========Exiting CombineEmbeddings.forward==========\n",
      "t0c: torch.Size([32, 8, 32])\n",
      "tensor([[-0.3332, -0.9106,  0.3895, -0.9779],\n",
      "        [-0.7401,  0.5783,  0.2626,  0.1109],\n",
      "        [-0.8514,  1.0173,  1.0788, -0.0231],\n",
      "        [ 0.6325,  0.5146,  0.0525,  1.1107]], grad_fn=<SliceBackward0>)\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([32, 128, 32])\n",
      "Tensor 'output[1]' shape: torch.Size([32, 32, 32])\n",
      "Tensor 'output[2]' shape: torch.Size([32, 8, 32])\n",
      "==========Exiting Model.create_targets==========\n",
      "\n",
      "==========Entering ConceptLoss.forward==========\n",
      "Inputs:\n",
      "Tuple 'xfs':\n",
      "    Tensor 'xfs[0]' shape: torch.Size([32, 8, 32])\n",
      "    Tensor 'xfs[1]' shape: torch.Size([32, 32, 32])\n",
      "    Tensor 'xfs[2]' shape: torch.Size([32, 128, 512])\n",
      "Tuple 'targets':\n",
      "    Tensor 'targets[0]' shape: torch.Size([32, 8, 32])\n",
      "    Tensor 'targets[1]' shape: torch.Size([32, 32, 32])\n",
      "    Tensor 'targets[2]' shape: torch.Size([32, 128, 32])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([])\n",
      "==========Exiting ConceptLoss.forward==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([32, 128, 512])\n",
      "Tensor 'output[1]' shape: torch.Size([])\n",
      "==========Exiting Model.forward==========\n"
     ]
    }
   ],
   "source": [
    "hold = config.levels\n",
    "config.levels = 3\n",
    "module = Model(config, tokenizer)\n",
    "module.enable_logging()\n",
    "\n",
    "### enabling logging for sub-modules\n",
    "module.embedding_combiner.enable_logging()\n",
    "#module.body.enable_logging()\n",
    "module.concept_loss_fn.enable_logging()\n",
    "\n",
    "### disabling logging for sub-functions\n",
    "#module.disable_function_logging('create_x0s')\n",
    "#module.disable_function_logging('create_targets')\n",
    "#module.disable_function_logging('generate')\n",
    "#module.disable_function_logging('sampler')\n",
    "\n",
    "offset = random.randint(1, config.combine_factor-1)\n",
    "print(f'randomly chosen offset: {offset}')\n",
    "token_ids = torch.randint(config.vocab_size, (32, config.max_seq_len + (config.combine_factor ** (config.levels-1)) - offset))\n",
    "input_token_ids = token_ids[:,:config.max_seq_len - offset]\n",
    "target_token_ids = token_ids\n",
    "output, loss = module(input_token_ids, target_token_ids)\n",
    "config.levels = hold\n",
    "del hold, module, offset, token_ids, input_token_ids, target_token_ids, output, loss\n",
    "module = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fe8f8f-ed60-43cc-ab4e-10f5628fb582",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9bd5e51f-4768-4a29-8929-9e60988a1e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering Model.forward==========\n",
      "Inputs:\n",
      "Tensor 'input_token_ids' shape: torch.Size([32, 117])\n",
      "\n",
      "==========Entering Model.create_x0s==========\n",
      "Inputs:\n",
      "Tensor 'input_token_ids' shape: torch.Size([32, 117])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([32, 120, 32])\n",
      "Tensor 'output[1]' shape: torch.Size([32, 30, 32])\n",
      "==========Exiting Model.create_x0s==========\n",
      "\n",
      "==========Entering Body.forward==========\n",
      "Inputs:\n",
      "Tuple 'x0s':\n",
      "    Tensor 'x0s[0]' shape: torch.Size([32, 120, 32])\n",
      "    Tensor 'x0s[1]' shape: torch.Size([32, 30, 32])\n",
      "Other-type 'targets': Type=NoneType, Value=None\n",
      "Integer 'cvec_topk': Value=False\n",
      "Float 'cvec_greedy': Value=1.0\n",
      "\n",
      "==========Entering Body.forward_inference==========\n",
      "Inputs:\n",
      "Tuple 'x0s':\n",
      "    Tensor 'x0s[0]' shape: torch.Size([32, 120, 32])\n",
      "    Tensor 'x0s[1]' shape: torch.Size([32, 30, 32])\n",
      "Other-type 'cvec_topk': Type=NoneType, Value=None\n",
      "Integer 'cvec_greedy': Value=False\n",
      "Float 'cvec_temp': Value=1.0\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 30, 32])\n",
      "Integer 'i': Value=0\n",
      "Integer 'c': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 30, 32])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 31, 32])\n",
      "Integer 'i': Value=0\n",
      "Integer 'c': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 31, 32])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 32, 32])\n",
      "Integer 'i': Value=0\n",
      "Integer 'c': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 32, 32])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 120, 32])\n",
      "Integer 'i': Value=1\n",
      "Tensor 'c' shape: torch.Size([32, 32, 32])\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 120, 32])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([32, 32, 32])\n",
      "Tensor 'output[1]' shape: torch.Size([32, 120, 512])\n",
      "==========Exiting Body.forward_inference==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([32, 32, 32])\n",
      "Tensor 'output[1]' shape: torch.Size([32, 120, 512])\n",
      "==========Exiting Body.forward==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([32, 120, 512])\n",
      "Other-type 'output[1]': Type=NoneType, Value=None\n",
      "==========Exiting Model.forward==========\n"
     ]
    }
   ],
   "source": [
    "module = Model(config, tokenizer)\n",
    "module.enable_logging()\n",
    "\n",
    "### enabling logging for sub-modules\n",
    "#module.embedding_combiner.enable_logging()\n",
    "module.body.enable_logging()\n",
    "#module.concept_loss_fn.enable_logging()\n",
    "\n",
    "### disabling logging for sub-functions\n",
    "#module.disable_function_logging('create_x0s')\n",
    "#module.disable_function_logging('create_targets')\n",
    "#module.disable_function_logging('generate')\n",
    "#module.disable_function_logging('sampler')\n",
    "\n",
    "input_token_ids = torch.randint(config.vocab_size, \n",
    "                                (32, config.max_seq_len-11))\n",
    "output, loss = module(input_token_ids)\n",
    "del module, input_token_ids, output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "67ee8927-e95a-4c5d-b688-6214ebf89435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering Model.forward==========\n",
      "Inputs:\n",
      "Tensor 'input_token_ids' shape: torch.Size([32, 113])\n",
      "\n",
      "==========Entering Model.create_x0s==========\n",
      "Inputs:\n",
      "Tensor 'input_token_ids' shape: torch.Size([32, 113])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([32, 116, 32])\n",
      "Tensor 'output[1]' shape: torch.Size([32, 29, 32])\n",
      "==========Exiting Model.create_x0s==========\n",
      "\n",
      "==========Entering Body.forward==========\n",
      "Inputs:\n",
      "Tuple 'x0s':\n",
      "    Tensor 'x0s[0]' shape: torch.Size([32, 116, 32])\n",
      "    Tensor 'x0s[1]' shape: torch.Size([32, 29, 32])\n",
      "Other-type 'targets': Type=NoneType, Value=None\n",
      "Integer 'cvec_topk': Value=False\n",
      "Float 'cvec_greedy': Value=1.0\n",
      "\n",
      "==========Entering Body.forward_inference==========\n",
      "Inputs:\n",
      "Tuple 'x0s':\n",
      "    Tensor 'x0s[0]' shape: torch.Size([32, 116, 32])\n",
      "    Tensor 'x0s[1]' shape: torch.Size([32, 29, 32])\n",
      "Other-type 'cvec_topk': Type=NoneType, Value=None\n",
      "Integer 'cvec_greedy': Value=False\n",
      "Float 'cvec_temp': Value=1.0\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 29, 32])\n",
      "Integer 'i': Value=0\n",
      "Integer 'c': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 29, 32])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 30, 32])\n",
      "Integer 'i': Value=0\n",
      "Integer 'c': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 30, 32])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 31, 32])\n",
      "Integer 'i': Value=0\n",
      "Integer 'c': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 31, 32])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 32, 32])\n",
      "Integer 'i': Value=0\n",
      "Integer 'c': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 32, 32])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 116, 32])\n",
      "Integer 'i': Value=1\n",
      "Tensor 'c' shape: torch.Size([32, 32, 32])\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 116, 32])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([32, 32, 32])\n",
      "Tensor 'output[1]' shape: torch.Size([32, 116, 512])\n",
      "==========Exiting Body.forward_inference==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([32, 32, 32])\n",
      "Tensor 'output[1]' shape: torch.Size([32, 116, 512])\n",
      "==========Exiting Body.forward==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([32, 116, 512])\n",
      "Other-type 'output[1]': Type=NoneType, Value=None\n",
      "==========Exiting Model.forward==========\n"
     ]
    }
   ],
   "source": [
    "module = Model(config, tokenizer)\n",
    "module.enable_logging()\n",
    "\n",
    "### enabling logging for sub-modules\n",
    "#module.embedding_combiner.enable_logging()\n",
    "module.body.enable_logging()\n",
    "#module.concept_loss_fn.enable_logging()\n",
    "\n",
    "### disabling logging for sub-functions\n",
    "#module.disable_function_logging('create_x0s')\n",
    "#module.disable_function_logging('create_targets')\n",
    "#module.disable_function_logging('generate')\n",
    "#module.disable_function_logging('sampler')\n",
    "\n",
    "input_token_ids = torch.randint(config.vocab_size, \n",
    "                                (32, config.max_seq_len - 15))\n",
    "output, loss = module(input_token_ids)\n",
    "del module, input_token_ids, output, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c01560-0895-49e0-aa98-b30100318433",
   "metadata": {},
   "source": [
    "# Instantiate a brand new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40e94167-e10f-45a8-9392-8d910621f2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2639.36 K parameters\n",
      "Model(\n",
      "  (embedder): Embedding(512, 128)\n",
      "  (embedding_combiner): CombineEmbeddings(\n",
      "    (operation_chain): Sequential(\n",
      "      (reshape_0): ReshapeModule()\n",
      "      (mlp_post_reshape_1): MLP(\n",
      "        (gate_proj): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        (up_proj): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        (down_proj): Linear(in_features=1024, out_features=128, bias=True)\n",
      "      )\n",
      "      (norm_2): Norm()\n",
      "    )\n",
      "  )\n",
      "  (body): Body(\n",
      "    (layers): ModuleList(\n",
      "      (0-7): 8 x Layer(\n",
      "        (pre_self_mqa_norm): Norm()\n",
      "        (self_mqa): selfMQA()\n",
      "        (pre_cross_mqa_x_norm): Norm()\n",
      "        (pre_cross_mqa_c_norm): Norm()\n",
      "        (cross_mqa): crossMQA()\n",
      "        (pre_mlp_norm): Norm()\n",
      "        (mlp): MLP(\n",
      "          (gate_proj): Linear(in_features=128, out_features=256, bias=True)\n",
      "          (up_proj): Linear(in_features=128, out_features=256, bias=True)\n",
      "          (down_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_norms): ModuleList(\n",
      "      (0-2): 3 x Norm()\n",
      "    )\n",
      "    (concept_output_layers): ModuleList(\n",
      "      (0-1): 2 x MLP(\n",
      "        (gate_proj): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (up_proj): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (down_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (concept_creator): CombineEmbeddings(\n",
      "      (operation_chain): Sequential(\n",
      "        (reshape_0): ReshapeModule()\n",
      "        (mlp_post_reshape_1): MLP(\n",
      "          (gate_proj): Linear(in_features=512, out_features=1024, bias=True)\n",
      "          (up_proj): Linear(in_features=512, out_features=1024, bias=True)\n",
      "          (down_proj): Linear(in_features=1024, out_features=128, bias=True)\n",
      "        )\n",
      "        (norm_2): Norm()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ce_loss_fn): CrossEntropyLoss()\n",
      "  (concept_loss_fn): ConceptLoss(\n",
      "    (MAE_loss): L1Loss()\n",
      "    (MSE_loss): MSELoss()\n",
      "    (COS_loss): CosineSimilarity()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Model(config, tokenizer).to(config.device)\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8efd0ad-4844-49cb-8ebc-321af1f8b892",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c40b0665-c82a-4452-b87b-588bb669778f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be our training dataset, the rest for validation\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d5d8341-b110-42de-9a09-34e468cc427d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading for training which generates a small batch of data of inputs x and targets y\n",
    "def get_batch(split, batch_size):\n",
    "    # whether we grab from our training or validation dataset\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - (config.max_seq_len + (config.combine_factor ** (config.levels-1))), (batch_size,))\n",
    "    # some training batches need to be offset so it learns how to use the padding vector\n",
    "    offset = random.randint(0, config.combine_factor-1)\n",
    "    x = torch.stack([data[i:i+config.max_seq_len - offset] for i in ix])\n",
    "    ### i actually need the y tensor to be + (config.combine_factor ** (config.levels-1)) to fit the future concepts\n",
    "    y = torch.stack([data[i+1:i+1+(config.max_seq_len + (config.combine_factor ** (config.levels-1))) - offset] for i in ix])\n",
    "    x, y = x.to(config.device), y.to(config.device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "baccce31-9fe4-46b3-b3c9-a1bd2b8007cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, batch_size, eval_iters = 5): # to estimate loss during the training loop\n",
    "    out = {}\n",
    "    model.eval() # sets model to eval mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split, batch_size)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # just resets to training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e724d95-c98a-4542-9a06-86374d268a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "# this is not what they used, but this learning rate & weight decay work for our tiny minGemma\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate_init, weight_decay=config.weight_decay)\n",
    "\n",
    "# how long we want to train for\n",
    "config.max_iters = 100\n",
    "\n",
    "# Learning rate scheduler setup\n",
    "lr_start = config.learning_rate_init\n",
    "lr_end = config.learning_rate_end  # Final learning rate after decay\n",
    "lr_lambda = lambda iter: (lr_end + (lr_start - lr_end) * (1 - iter / config.max_iters))\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# how often we want to check & see how our loss is doing\n",
    "eval_interval = 10\n",
    "\n",
    "# batch size to use\n",
    "config.batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03ddd52-9743-47b8-9d4a-aa1f1fb3003e",
   "metadata": {},
   "source": [
    "# ---- BOOKMARK -----\n",
    "so it looks like the wrong number of target concept vectors are getting created during training. once i've fixed that i'd like to confirm training & inference are working and then switch to the new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23ce596e-89a3-4de6-8dc5-a0650519cad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 112.2529, val loss 112.0210, time elapsed: 0.97 seconds\n",
      "step 10: train loss 16.2049, val loss 16.3779, time elapsed: 14.41 seconds\n",
      "step 20: train loss 9.7793, val loss 9.7198, time elapsed: 27.87 seconds\n",
      "step 30: train loss 7.8275, val loss 7.8874, time elapsed: 41.20 seconds\n",
      "step 40: train loss 6.8737, val loss 6.8584, time elapsed: 54.32 seconds\n",
      "step 50: train loss 6.1504, val loss 6.2550, time elapsed: 67.58 seconds\n",
      "step 60: train loss 5.7682, val loss 5.7188, time elapsed: 80.71 seconds\n",
      "step 70: train loss 5.3119, val loss 5.4951, time elapsed: 93.80 seconds\n",
      "step 80: train loss 5.2514, val loss 5.2149, time elapsed: 106.94 seconds\n",
      "step 90: train loss 5.1099, val loss 5.1737, time elapsed: 120.10 seconds\n",
      "step 99: train loss 5.0733, val loss 5.0451, time elapsed: 132.55 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Enable anomaly detection. uncomment these lines if you need to do extensive debugging\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "for iter in range(config.max_iters):\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train', config.batch_size)\n",
    "    \n",
    "    # train\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    # Update the learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == config.max_iters - 1:\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - start_time\n",
    "        losses = estimate_loss(model, config.batch_size)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, time elapsed: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Disable anomaly detection after the training loop\n",
    "#torch.autograd.set_detect_anomaly(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c28f086-54fe-4b38-b03a-78eacb6e7454",
   "metadata": {},
   "source": [
    "# Saving your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "588a4ae3-aa58-41d4-ba66-4c5f333943a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = f'models/{model.__class__.__name__}_{time.strftime(\"%Y-%m-%d|%H-%M-%S\")}'\n",
    "torch.save(model.state_dict(), f'{name}.pth')\n",
    "\n",
    "# Convert the dataclass object to a dictionary\n",
    "config_dict = asdict(config)\n",
    "\n",
    "# Serialize the dictionary to a JSON file\n",
    "with open(f'{name}.json', 'w') as f:\n",
    "    json.dump(config_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d406304-9dcf-4f71-b69d-e8f860d43c15",
   "metadata": {},
   "source": [
    "# Load a Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c948ec-0c43-4c64-9719-f6dea92cf04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = '?'\n",
    "\n",
    "# Deserialize the JSON file back to a dictionary\n",
    "with open(f'models/{name}.json', 'r') as f:\n",
    "    config_dict = json.load(f)\n",
    "\n",
    "# Convert the dictionary back to a dataclass object\n",
    "config = Config(**config_dict)\n",
    "\n",
    "# Initialize a blank model\n",
    "model = Model(config, tokenizer).to(config.device)  \n",
    "\n",
    "# here's the path to a minGemma model that i've trained with roughly 1m parameters\n",
    "path = f'models/{name}.pth'\n",
    "\n",
    "# Load the saved state dictionary\n",
    "model.load_state_dict(torch.load(path)) \n",
    "# REMEMBER TO CHANGE VALUES IN CONFIG TO MATCH THE MODEL YOU'VE LOADED\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')\n",
    "\n",
    "# If you only plan to do inference, switch to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# If you plan to continue training the model, switch to training mode\n",
    "#model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b5bed2-838f-4bc3-81e2-3ee4d69d7184",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "768fb9e3-185e-4a95-8e1d-229855132410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou Romeo?\n",
      "ds,  ye   es nERoron bs     n  br ningNs lth A weing\n",
      "AObird ,\n",
      " saidceheer  ararmydbca oncee t    r-look t  \n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "input_str = \"JULIET:\\nO Romeo, Romeo! wherefore art thou Romeo?\\n\" # the classic line\n",
    "max_useable_output_len = config.max_seq_len - len(input_str)\n",
    "output = model.generate(input_str, output_len = max_useable_output_len, temperature=1.0)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ae4e9e-92f2-4f6f-8d05-aad360edd056",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
