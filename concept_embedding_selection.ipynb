{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YrHovrWA_Y6t"
   },
   "outputs": [],
   "source": [
    "# my virtual environments are rarely properly connected to jupyter so this fixes that\n",
    "import sys\n",
    "import os\n",
    "current_dir = os.getcwd()  # Get the current working directory\n",
    "venv_dir = os.path.join(current_dir, 'venv') \n",
    "python_version = str(sys.version_info.major) + '.' + str(sys.version_info.minor)\n",
    "site_packages_path = os.path.join(venv_dir, 'lib', 'python' + python_version, 'site-packages')\n",
    "sys.path.append(site_packages_path) \n",
    "\n",
    "# Importing pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import random as r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sVZYlg9C_Zqg",
    "outputId": "67593490-2503-4e48-9c6c-baa788028495"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.8073,  1.1433, -0.1321,  ..., -2.6458, -0.6629, -0.1587],\n",
       "        [ 0.7613,  0.9070,  0.8021,  ...,  0.4444,  0.9084,  1.0139],\n",
       "        [ 0.1461, -1.1294,  0.2314,  ...,  0.1229, -1.9200,  1.8613],\n",
       "        ...,\n",
       "        [ 0.5094,  0.4575, -1.6483,  ...,  0.5640, -0.2304,  0.1097],\n",
       "        [ 0.3923, -0.0513,  1.3227,  ..., -0.4111, -1.8965,  1.1194],\n",
       "        [-0.1266,  1.0371,  0.1028,  ..., -0.8041,  1.1302,  0.3114]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v, d = 128, 256\n",
    "embedder = nn.Embedding(v, d)\n",
    "embedder.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "yWEApYzh_eNE"
   },
   "outputs": [],
   "source": [
    "class Norm(torch.nn.Module):\n",
    "    def __init__(self, embed_dim: int, norm_type: str = \"cos\", eps: float = 1e-6, norm_affine: bool = False, dropout_rate: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.affine = norm_affine\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.norm_type = norm_type\n",
    "\n",
    "        # Initialize weight and bias parameters for affine transformation\n",
    "        # We start with ones for weight to keep the original scale initially, and zeros for bias.\n",
    "        self.w = nn.Parameter(torch.ones(embed_dim))\n",
    "        self.b = nn.Parameter(torch.zeros(embed_dim))\n",
    "\n",
    "        self.logging_enabled = False\n",
    "    def enable_logging(self):\n",
    "        self.logging_enabled = True\n",
    "    def disable_logging(self):\n",
    "        self.logging_enabled = False\n",
    "\n",
    "    def CosineNorm(self, x):\n",
    "        # normalize x by dividing by its L2 norm along the last dimension.\n",
    "        # this places x on the unit hypersphere centered at the origin\n",
    "        # Add a small constant to the denominator to avoid division by zero.\n",
    "        return x / torch.norm(x, p=2, dim=-1, keepdim=True).clamp(min=self.eps)\n",
    "\n",
    "    def RMSNorm(self, x):\n",
    "        # normalize x by dividing by its root-mean-square along the last dimension\n",
    "        # this places x on a hypersphere of radius sqrt(dimension) with no certain center\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def LayerNorm(self, x):\n",
    "        # normalize x by subtracting by its mean then dividing by its variance\n",
    "        # this places x on a hypersphere of radius sqrt(dimension) centered at the origin\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        return (x - mean) / torch.sqrt(var + self.eps)\n",
    "\n",
    "    def splice_affine(self, weight, bias, d_i):\n",
    "        return weight[:d_i], bias[:d_i]\n",
    "\n",
    "    def forward(self, x: torch.Tensor, training: bool = False) -> torch.Tensor:\n",
    "        # Normalize the input tensor\n",
    "        if self.norm_type == \"CosineNorm\":\n",
    "            x = self.CosineNorm(x)\n",
    "        elif self.norm_type == \"LayerNorm\":\n",
    "            x = self.LayerNorm(x)\n",
    "        else: # defaults to RMSNorm bc that's the most commonly used nowadays\n",
    "            x = self.RMSNorm(x)\n",
    "\n",
    "        # Optionally apply the affine transformation with splicing\n",
    "        if self.affine:\n",
    "            w, b = self.splice_affine(self.w, self.b, x.shape[-1])\n",
    "            x = x * w + b\n",
    "            x = F.dropout(x, p=self.dropout_rate, training=training) # and dropout if we're training\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "XvRlpAfB_fWc"
   },
   "outputs": [],
   "source": [
    "# prompt: grab 2 random vectors from the embed and sum them\n",
    "\n",
    "# Get two random indices within the vocabulary size\n",
    "index1 = r.randint(0, v - 1)\n",
    "index2 = r.randint(0, v - 1)\n",
    "\n",
    "# Extract the corresponding vectors from the embedding layer\n",
    "vector1 = embedder.weight[index1]\n",
    "vector2 = embedder.weight[index2]\n",
    "\n",
    "# create some noise\n",
    "noise_sd = 0.3\n",
    "noise = torch.randn(d) * noise_sd\n",
    "\n",
    "# Sum the two vectors & add some noise\n",
    "concept = vector1 + vector2 + noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "8fT3oXEn_rxI"
   },
   "outputs": [],
   "source": [
    "# prompt: Now initialize a `Norm` and use it on our summed vector\n",
    "\n",
    "# Initialize a Norm layer with the same embedding dimension as the vectors\n",
    "norm_layer = Norm(d)\n",
    "\n",
    "# Apply the normalization to the sum vector\n",
    "normalized_concept = norm_layer(concept)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OsEtNDYxAb5g",
    "outputId": "3d9b36da-c5b5-46d5-f63a-ce8aa512db72"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7.1494e-03,  1.1527e-02,  1.3576e-02,  3.2820e-02, -3.9479e-02,\n",
       "        -9.5474e-02, -8.2890e-02,  1.5776e-02,  6.4283e-02, -2.1337e-02,\n",
       "         5.5857e-03,  1.2009e-02,  9.4453e-02,  5.1899e-02, -2.5338e-02,\n",
       "        -1.2715e-01,  8.1137e-02,  8.3458e-02, -7.1376e-03,  2.5764e-02,\n",
       "         4.1137e-02,  7.3916e-02,  5.8518e-02, -9.9237e-03,  1.2152e-02,\n",
       "        -2.6197e-03, -2.1903e-02,  1.9650e-02,  2.9031e-02,  8.6118e-03,\n",
       "        -1.0277e-01, -7.3730e-02, -8.8745e-03, -4.1543e-02,  1.1397e-02,\n",
       "         8.2652e-02, -7.3531e-02, -2.1971e-02,  7.2303e-01, -7.3195e-02,\n",
       "         2.9334e-02,  4.9980e-02,  8.1096e-02,  3.1988e-02,  6.0336e-02,\n",
       "         4.2632e-03,  2.0838e-02,  2.6068e-02,  3.0076e-02, -1.1351e-03,\n",
       "        -3.0020e-02, -6.4855e-02,  4.4605e-02,  1.4552e-01, -3.7638e-02,\n",
       "         3.0052e-02,  6.4689e-02, -4.8579e-02,  7.6461e-02,  1.1257e-01,\n",
       "         3.9186e-04,  1.1170e-01, -1.0726e-01, -5.6641e-03,  9.7479e-02,\n",
       "        -2.3225e-03, -6.9703e-02,  2.8242e-02,  5.7191e-02,  5.6471e-02,\n",
       "         2.2480e-02, -9.2584e-02, -2.6254e-02, -4.9020e-02,  5.7205e-02,\n",
       "        -3.6013e-02, -5.7123e-02,  5.5907e-02,  2.9481e-02, -3.1463e-02,\n",
       "         1.2563e-02,  5.8613e-02, -1.7618e-02, -6.3034e-02,  9.7558e-02,\n",
       "         6.0120e-02, -7.4720e-02,  3.3166e-02,  5.4167e-02, -3.2930e-02,\n",
       "         1.3913e-02, -5.6848e-02, -4.8245e-02, -5.2164e-02,  2.9689e-02,\n",
       "         1.5787e-02, -1.2871e-01,  6.7404e-03,  1.5577e-01,  5.3925e-02,\n",
       "        -3.3944e-02, -1.9637e-02,  1.2556e-02,  1.1779e-02, -6.5067e-02,\n",
       "         5.2375e-02, -1.5737e-02, -1.0784e-02,  8.5305e-03,  6.0946e-05,\n",
       "         4.0960e-03,  7.4220e-02, -3.2046e-02,  3.5128e-02,  6.1028e-02,\n",
       "         9.0543e-03,  5.2986e-02, -5.7373e-02, -8.2624e-03, -9.9348e-02,\n",
       "        -9.0855e-03,  3.6227e-02,  5.9659e-02,  5.0326e-02,  6.5396e-01,\n",
       "         4.3555e-02,  6.3478e-02, -9.8360e-02], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt: Initialize an nn.CosineSimilarity(dim=-1, eps=1e-6) and use it to compare normalized_concept to all the vectors in embedder\n",
    "\n",
    "cosine_similarity = nn.CosineSimilarity(dim=-1, eps=1e-6)\n",
    "similarity_scores = cosine_similarity(normalized_concept, embedder.weight)\n",
    "similarity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ytoOc0KiA8iE",
    "outputId": "437bce39-b532-475e-ba3a-9879dd3ef513"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7230, 0.6540, 0.1558, 0.1455, 0.1126, 0.1117, 0.0976, 0.0975],\n",
      "       grad_fn=<TopkBackward0>)\n",
      "tensor([ 38, 124,  98,  53,  59,  61,  84,  64])\n"
     ]
    }
   ],
   "source": [
    "# prompt: Grab the topk of those similarity scores with k=8 and print them for me to see\n",
    "\n",
    "topk_values, topk_indices = torch.topk(similarity_scores, k=8)\n",
    "print(topk_values)\n",
    "print(topk_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Z5Zz6uWpBLmp"
   },
   "outputs": [],
   "source": [
    "def create_concept_embeddings(E, indices):\n",
    "    k = len(indices)\n",
    "    d = E.size(1)\n",
    "    X_size = (k - 1) * k // 2\n",
    "    X = torch.empty((X_size, d), dtype=E.dtype)\n",
    "\n",
    "    count = 0\n",
    "    for i in range(k):\n",
    "        for j in range(i + 1, k):\n",
    "            X[count] = E[indices[i]] + E[indices[j]]\n",
    "            count += 1\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QniGRKRUCR9K",
    "outputId": "54ec6524-c374-4d02-aa0b-829a95e3e108"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([28, 256]),\n",
       " tensor([[-1.4138,  3.0280,  2.0697,  ..., -0.2237, -1.1991,  1.5292],\n",
       "         [-1.4550,  1.3894,  1.6838,  ..., -0.1591, -2.7218,  1.9800],\n",
       "         [ 0.5843,  3.4711,  1.2170,  ..., -0.6952, -1.3263,  3.2264],\n",
       "         ...,\n",
       "         [-1.7019, -3.3933, -0.7097,  ...,  1.0141, -0.4978,  0.9503],\n",
       "         [-4.4751, -3.0278,  0.1329,  ..., -0.7627, -0.7052,  1.9806],\n",
       "         [-3.1139, -1.1019,  1.3278,  ..., -0.2597, -1.0889,  0.2060]],\n",
       "        grad_fn=<CopySlices>))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E_comb = create_concept_embeddings(embedder.weight, topk_indices)\n",
    "E_comb.shape, E_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p_-Ce0GCCiE4",
    "outputId": "71ef10f0-db8a-413d-b396-87aceb8d6db9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9771, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "degree2_similarity_scores = cosine_similarity(normalized_concept, E_comb)\n",
    "degree2_similarity_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "3w2g9MtVCsxP"
   },
   "outputs": [],
   "source": [
    "def concept_matchup(c: torch.Tensor, embedding: torch.Tensor, combo: int, sample: int, greedy: bool = False, temp: float = 1.0):\n",
    "    assert sample >= combo, 'you need to sample at least as many token embedding vectors as what you plan to combine'\n",
    "    token_similarities = F.cosine_similarity(c, embedding, dim=-1)\n",
    "    print(token_similarities.shape, token_similarities)\n",
    "    topk_token_indices = torch.topk(token_similarities, k=sample).indices\n",
    "    print(topk_token_indices.shape, topk_token_indices)\n",
    "    concept_embeddings = create_concept_embeddings(embedding, topk_token_indices)\n",
    "    print(concept_embeddings.shape, concept_embeddings)\n",
    "    concept_similarities = F.cosine_similarity(c, concept_embeddings, dim=-1)\n",
    "    print(concept_similarities.shape, concept_similarities)\n",
    "    topk_concept_similarities, topk_concept_indices = torch.topk(concept_similarities, k=sample)\n",
    "    print(topk_concept_similarities.shape, topk_concept_similarities)\n",
    "    print(topk_concept_indices.shape, topk_concept_indices)\n",
    "    if greedy:\n",
    "        concept_idx = int(topk_concept_indices[0])\n",
    "        print(concept_idx)\n",
    "        return concept_embeddings[:,concept_idx]\n",
    "    else:\n",
    "        topk_concept_probs = F.softmax(topk_concept_similarities / temp, dim=-1)\n",
    "        print(topk_concept_probs.shape, topk_concept_probs)\n",
    "        concept_topk_idx = torch.multinomial(topk_concept_probs, num_samples = 1)\n",
    "        print(concept_topk_idx)\n",
    "        concept_idx = topk_concept_indices[concept_topk_idx]\n",
    "        print(concept_idx)\n",
    "        return concept_embeddings[concept_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5mxYgYINORoy",
    "outputId": "8acf946c-84da-4a41-b6ab-ad34aaeab1b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128]) tensor([ 7.1494e-03,  1.1527e-02,  1.3576e-02,  3.2820e-02, -3.9479e-02,\n",
      "        -9.5474e-02, -8.2890e-02,  1.5776e-02,  6.4283e-02, -2.1337e-02,\n",
      "         5.5857e-03,  1.2009e-02,  9.4453e-02,  5.1899e-02, -2.5338e-02,\n",
      "        -1.2715e-01,  8.1137e-02,  8.3458e-02, -7.1376e-03,  2.5764e-02,\n",
      "         4.1137e-02,  7.3916e-02,  5.8518e-02, -9.9237e-03,  1.2152e-02,\n",
      "        -2.6197e-03, -2.1903e-02,  1.9650e-02,  2.9031e-02,  8.6118e-03,\n",
      "        -1.0277e-01, -7.3730e-02, -8.8745e-03, -4.1543e-02,  1.1397e-02,\n",
      "         8.2652e-02, -7.3531e-02, -2.1971e-02,  7.2303e-01, -7.3195e-02,\n",
      "         2.9334e-02,  4.9980e-02,  8.1096e-02,  3.1988e-02,  6.0336e-02,\n",
      "         4.2632e-03,  2.0838e-02,  2.6068e-02,  3.0076e-02, -1.1350e-03,\n",
      "        -3.0020e-02, -6.4855e-02,  4.4605e-02,  1.4552e-01, -3.7638e-02,\n",
      "         3.0052e-02,  6.4689e-02, -4.8579e-02,  7.6461e-02,  1.1257e-01,\n",
      "         3.9186e-04,  1.1170e-01, -1.0726e-01, -5.6641e-03,  9.7479e-02,\n",
      "        -2.3225e-03, -6.9703e-02,  2.8242e-02,  5.7191e-02,  5.6471e-02,\n",
      "         2.2480e-02, -9.2584e-02, -2.6254e-02, -4.9020e-02,  5.7205e-02,\n",
      "        -3.6013e-02, -5.7123e-02,  5.5907e-02,  2.9481e-02, -3.1463e-02,\n",
      "         1.2563e-02,  5.8613e-02, -1.7618e-02, -6.3034e-02,  9.7558e-02,\n",
      "         6.0120e-02, -7.4720e-02,  3.3166e-02,  5.4167e-02, -3.2930e-02,\n",
      "         1.3913e-02, -5.6848e-02, -4.8245e-02, -5.2164e-02,  2.9689e-02,\n",
      "         1.5787e-02, -1.2871e-01,  6.7404e-03,  1.5577e-01,  5.3925e-02,\n",
      "        -3.3944e-02, -1.9637e-02,  1.2556e-02,  1.1780e-02, -6.5067e-02,\n",
      "         5.2375e-02, -1.5737e-02, -1.0784e-02,  8.5305e-03,  6.0943e-05,\n",
      "         4.0960e-03,  7.4220e-02, -3.2046e-02,  3.5128e-02,  6.1028e-02,\n",
      "         9.0543e-03,  5.2986e-02, -5.7373e-02, -8.2624e-03, -9.9348e-02,\n",
      "        -9.0855e-03,  3.6227e-02,  5.9659e-02,  5.0326e-02,  6.5396e-01,\n",
      "         4.3555e-02,  6.3477e-02, -9.8360e-02], grad_fn=<SumBackward1>)\n",
      "torch.Size([8]) tensor([ 38, 124,  98,  53,  59,  61,  84,  64])\n",
      "torch.Size([28, 256]) tensor([[-1.4138,  3.0280,  2.0697,  ..., -0.2237, -1.1991,  1.5292],\n",
      "        [-1.4550,  1.3894,  1.6838,  ..., -0.1591, -2.7218,  1.9800],\n",
      "        [ 0.5843,  3.4711,  1.2170,  ..., -0.6952, -1.3263,  3.2264],\n",
      "        ...,\n",
      "        [-1.7019, -3.3933, -0.7097,  ...,  1.0141, -0.4978,  0.9503],\n",
      "        [-4.4751, -3.0278,  0.1329,  ..., -0.7627, -0.7052,  1.9806],\n",
      "        [-3.1139, -1.1019,  1.3278,  ..., -0.2597, -1.0889,  0.2060]],\n",
      "       grad_fn=<CopySlices>)\n",
      "torch.Size([28]) tensor([0.9771, 0.6062, 0.6180, 0.5563, 0.5922, 0.5546, 0.5634, 0.5431, 0.5606,\n",
      "        0.5239, 0.5190, 0.4965, 0.4972, 0.2137, 0.1855, 0.1813, 0.1729, 0.1798,\n",
      "        0.1792, 0.1829, 0.1689, 0.1681, 0.1596, 0.1505, 0.1446, 0.1455, 0.1489,\n",
      "        0.1428], grad_fn=<SumBackward1>)\n",
      "torch.Size([8]) tensor([0.9771, 0.6180, 0.6062, 0.5922, 0.5634, 0.5606, 0.5563, 0.5546],\n",
      "       grad_fn=<TopkBackward0>)\n",
      "torch.Size([8]) tensor([0, 2, 1, 4, 6, 8, 3, 5])\n",
      "torch.Size([8]) tensor([0.5096, 0.0846, 0.0797, 0.0744, 0.0644, 0.0635, 0.0622, 0.0616],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([3])\n",
      "tensor([4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-2.2043e+00, -1.0837e+00,  1.8537e-01,  2.1764e+00,  1.4846e+00,\n",
       "          4.0453e+00,  3.8345e-02,  1.1215e+00, -1.3536e+00,  2.1843e+00,\n",
       "         -9.8436e-01,  1.0102e+00,  2.1251e+00,  1.9812e+00, -6.1911e-01,\n",
       "         -1.1599e+00, -5.7994e-01,  3.4874e-02,  8.1136e-01, -6.5791e-01,\n",
       "         -1.3594e+00,  2.1432e+00,  2.1755e+00,  1.2292e+00, -1.8886e-03,\n",
       "         -6.1713e-01,  3.9019e-01, -5.6896e-01, -1.3851e-01,  1.0843e+00,\n",
       "         -2.6903e-01,  1.9566e-01, -1.9897e-01,  1.1603e+00, -4.8639e+00,\n",
       "         -1.6356e-01, -3.4877e+00, -1.8800e+00,  2.3542e+00, -1.1411e+00,\n",
       "         -1.1248e+00,  8.9675e-01, -1.6131e+00, -9.8432e-01, -1.4025e+00,\n",
       "          7.7121e-01,  1.3408e+00, -2.4267e+00,  7.6668e-01,  1.5692e-01,\n",
       "         -6.8639e-01, -1.0739e-01, -2.8405e+00, -1.4735e-01,  2.2883e+00,\n",
       "         -4.3994e-02,  1.9398e-02, -1.4309e+00,  5.9651e-01,  6.7744e-01,\n",
       "          4.4980e-01,  5.7838e-01, -2.4775e+00,  2.1585e+00,  6.7887e-02,\n",
       "          5.7665e-01, -2.7616e-01, -3.0621e-01,  1.1714e+00, -1.0186e+00,\n",
       "         -2.0483e+00, -2.3135e+00, -6.8091e-01,  2.1577e+00, -1.2759e+00,\n",
       "         -3.2460e+00, -1.4011e+00, -8.3213e-01, -3.6852e-04,  1.7114e+00,\n",
       "         -6.3935e-01, -3.8842e-01, -1.1659e-02, -2.7424e-01, -2.9368e-01,\n",
       "         -1.2901e-01, -1.1735e+00, -6.2285e-02, -1.8709e-01,  6.8840e-01,\n",
       "         -2.3390e+00,  2.8360e+00, -1.6209e+00,  1.7019e+00,  2.3887e+00,\n",
       "         -1.9807e+00,  5.2771e-01,  8.7195e-02,  2.6770e-01,  7.1067e-01,\n",
       "         -2.6883e+00,  4.8622e-01,  1.8986e+00, -1.6516e+00,  4.3532e-01,\n",
       "          2.1769e+00, -3.3710e+00,  8.1690e-01, -1.7559e-01,  1.4472e+00,\n",
       "         -2.2884e-01, -1.6593e+00, -1.2997e+00,  2.8918e+00,  4.3618e-01,\n",
       "         -8.0170e-01, -7.4433e-01,  2.7003e-01, -1.6661e+00,  3.0870e+00,\n",
       "         -3.7784e+00,  8.6765e-02,  9.9001e-01,  1.4782e+00,  3.8905e+00,\n",
       "         -7.0400e-01, -8.2304e-01, -2.4434e+00,  2.4864e+00,  4.3484e-01,\n",
       "          8.4537e-01, -8.5154e-01,  1.8147e+00, -1.0139e+00,  1.0165e+00,\n",
       "          9.2162e-01, -2.0727e-01,  2.6829e-01, -1.3608e+00,  4.9426e-01,\n",
       "          1.2670e+00,  1.0646e+00,  2.8705e+00, -1.9162e+00, -3.2603e-01,\n",
       "          2.4106e-01,  4.3357e-01, -4.5867e-01,  2.0879e+00, -3.4849e-01,\n",
       "         -7.4900e-01,  4.4681e-01,  2.0606e+00, -2.1854e-01,  2.7736e+00,\n",
       "          4.5173e-01, -7.2431e-01,  4.0113e-01,  1.1872e+00, -3.1170e-01,\n",
       "          2.2624e+00,  8.2455e-01, -5.2862e-01,  1.6587e+00,  1.2765e+00,\n",
       "          1.1102e+00, -2.3444e+00,  1.5691e+00, -6.0151e-01,  5.2383e-01,\n",
       "          1.2541e-01,  5.0877e-01, -1.6865e+00,  1.1273e+00,  2.7643e+00,\n",
       "         -1.0231e+00,  1.3942e+00, -3.6462e-01, -6.4569e-01,  1.3406e-01,\n",
       "          3.0613e-01, -7.8213e-02, -1.3433e+00,  1.8291e-01, -1.4903e+00,\n",
       "          2.5825e+00,  2.7953e-01,  4.4754e+00, -1.3346e-01, -9.0114e-01,\n",
       "         -4.9333e-01, -4.2445e-02,  4.2714e-01,  2.7120e-02, -2.1945e+00,\n",
       "          1.8945e+00,  2.3590e+00,  2.5505e-01, -2.0286e+00, -2.9455e-01,\n",
       "         -1.2227e+00, -1.0174e+00, -5.2724e-01,  4.8900e-01, -1.8496e+00,\n",
       "         -1.2213e+00, -2.0227e-02,  2.5994e-01, -7.6342e-01, -2.2888e+00,\n",
       "         -1.2184e-01,  2.5232e+00, -5.0020e-01,  1.8996e+00, -2.3754e-02,\n",
       "          2.3353e+00, -3.4849e+00,  8.2079e-01,  3.0462e-01,  6.4338e-01,\n",
       "          6.1755e-01, -2.4296e+00,  1.1146e+00, -6.2851e-01,  6.0730e-01,\n",
       "          1.7439e+00,  2.3101e+00, -1.4349e+00,  5.0105e-01, -2.7132e+00,\n",
       "          1.3810e+00,  9.1740e-01, -1.9984e+00, -1.4812e-01, -1.1293e+00,\n",
       "          1.3386e+00, -2.1601e+00, -5.1839e-02, -2.6498e-01, -1.6531e+00,\n",
       "         -4.1830e-02,  8.7893e-01,  6.7665e-01, -4.4321e-01, -2.6491e-01,\n",
       "         -9.1928e-01,  3.4073e-01,  1.0430e+00, -2.1558e+00,  1.2489e+00,\n",
       "         -1.2975e+00, -1.3551e-01,  1.3418e+00, -7.3362e-02, -6.2321e-01,\n",
       "          2.5700e+00]], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matched_concept = concept_matchup(concept, embedder.weight, 4, 8, greedy=False, temp=0.2)\n",
    "matched_concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "58QjpSniOgph"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 256]),\n",
       " tensor([[ 0.3397, -0.6837, -0.0673,  ...,  0.9303,  1.2032,  0.7619],\n",
       "         [ 1.1240,  0.7185, -1.4923,  ...,  0.6689,  1.1480,  1.1124],\n",
       "         [-2.9146,  2.2994,  0.1993,  ..., -4.3017,  1.4652,  1.2509],\n",
       "         ...,\n",
       "         [ 0.7449, -0.5608,  0.9285,  ..., -2.0668, -0.2718,  0.5154],\n",
       "         [-1.8308,  3.0527,  1.5382,  ...,  0.1653, -1.6641,  0.5171],\n",
       "         [-0.1733,  1.5868,  2.0756,  ...,  0.0222, -2.0070,  0.3852]],\n",
       "        grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import random as r\n",
    "\n",
    "# Assuming embedder is defined and has an attribute 'weight'\n",
    "# v is the size of the vocabulary, d is the dimension of the embedding, and b is the batch size\n",
    "\n",
    "v, d = 128, 256\n",
    "embedder = nn.Embedding(v, d)\n",
    "embedder.weight\n",
    "\n",
    "b = 32  # Batch size\n",
    "\n",
    "# Get two batches of random indices within the vocabulary size\n",
    "indices1 = torch.randint(0, v, (b,))\n",
    "indices2 = torch.randint(0, v, (b,))\n",
    "\n",
    "# Extract the corresponding vectors from the embedding layer for each index in the batch\n",
    "vectors1 = embedder.weight[indices1]\n",
    "vectors2 = embedder.weight[indices2]\n",
    "\n",
    "# Create some noise for each vector in the batch\n",
    "noise_sd = 0.3\n",
    "noise = torch.randn(b, d) * noise_sd\n",
    "\n",
    "# Sum the two batches of vectors & add some noise to each\n",
    "concept_batch = vectors1 + vectors2 + noise\n",
    "concept_batch.shape, concept_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concept_matchup(c: torch.Tensor, embedding: torch.Tensor, combo: int, sample: int, greedy: bool = False, temp: float = 1.0):\n",
    "    assert sample >= combo, 'you need to sample at least as many token embedding vectors as what you plan to combine'\n",
    "    if len(c.shape) == 2:\n",
    "        embedding = embedding.unsqueeze(0)\n",
    "        print(embedding.shape)\n",
    "        embedding = embedding.expand(c.shape[0], -1, -1)\n",
    "        print(embedding.shape)\n",
    "    \n",
    "    token_similarities = F.cosine_similarity(c, embedding, dim=-1)\n",
    "    print(token_similarities.shape, token_similarities)\n",
    "    topk_token_indices = torch.topk(token_similarities, k=sample).indices\n",
    "    print(topk_token_indices.shape, topk_token_indices)\n",
    "    concept_embeddings = create_concept_embeddings(embedding, topk_token_indices)\n",
    "    print(concept_embeddings.shape, concept_embeddings)\n",
    "    concept_similarities = F.cosine_similarity(c, concept_embeddings, dim=-1)\n",
    "    print(concept_similarities.shape, concept_similarities)\n",
    "    topk_concept_similarities, topk_concept_indices = torch.topk(concept_similarities, k=sample)\n",
    "    print(topk_concept_similarities.shape, topk_concept_similarities)\n",
    "    print(topk_concept_indices.shape, topk_concept_indices)\n",
    "    if greedy:\n",
    "        concept_idx = int(topk_concept_indices[0])\n",
    "        print(concept_idx)\n",
    "        return concept_embeddings[:,concept_idx]\n",
    "    else:\n",
    "        topk_concept_probs = F.softmax(topk_concept_similarities / temp, dim=-1)\n",
    "        print(topk_concept_probs.shape, topk_concept_probs)\n",
    "        concept_topk_idx = torch.multinomial(topk_concept_probs, num_samples = 1)\n",
    "        print(concept_topk_idx)\n",
    "        concept_idx = topk_concept_indices[concept_topk_idx]\n",
    "        print(concept_idx)\n",
    "        return concept_embeddings[concept_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 256])\n",
      "torch.Size([32, 128, 256])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (32) must match the size of tensor b (128) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m matched_concepts \u001b[38;5;241m=\u001b[39m \u001b[43mconcept_matchup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconcept_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgreedy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m matched_concepts\u001b[38;5;241m.\u001b[39mshape, matched_concepts\n",
      "Cell \u001b[0;32mIn[28], line 9\u001b[0m, in \u001b[0;36mconcept_matchup\u001b[0;34m(c, embedding, combo, sample, greedy, temp)\u001b[0m\n\u001b[1;32m      6\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m embedding\u001b[38;5;241m.\u001b[39mexpand(c\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(embedding\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 9\u001b[0m token_similarities \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(token_similarities\u001b[38;5;241m.\u001b[39mshape, token_similarities)\n\u001b[1;32m     11\u001b[0m topk_token_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtopk(token_similarities, k\u001b[38;5;241m=\u001b[39msample)\u001b[38;5;241m.\u001b[39mindices\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (32) must match the size of tensor b (128) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "matched_concepts = concept_matchup(concept_batch, embedder.weight, 4, 8, greedy=False, temp=0.2)\n",
    "matched_concepts.shape, matched_concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
