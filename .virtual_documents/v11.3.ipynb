





# my virtual environments are rarely properly connected to jupyter so this fixes that
import sys
import os
current_dir = os.getcwd()  # Get the current working directory
venv_dir = os.path.join(current_dir, 'venv') 
python_version = str(sys.version_info.major) + '.' + str(sys.version_info.minor)
site_packages_path = os.path.join(venv_dir, 'lib', 'python' + python_version, 'site-packages')
sys.path.append(site_packages_path) 


# Importing pytorch
import torch
import torch.nn as nn
from torch.nn import functional as F

# imports for the debugging/demonstration setup
import functools
import inspect

# imports for the tokenizer
from tokenizer import SimpleTokenizer, loaded_stoi, loaded_merges

# Imports used for the config
from dataclasses import dataclass
from typing import Optional

# Imports used for the model
import re
from typing import Any, List, Sequence, Tuple, Union

# used for training
import random
import time

# used to save & load models
import json
from dataclasses import asdict





# this function will be used throughout for debugging/demonstration purposes
# using this is way cleaner than cluttering up our code with print statements
def log_io(func):
    @functools.wraps(func)
    def wrapper(self, *args, **kwargs):
        # Check if logging is enabled globally and for the specific function
        if not self.logging_enabled or func.__name__ in self.disabled_logging_functions:
            return func(self, *args, **kwargs)
        #if not self.logging_enabled:
            #return func(self, *args, **kwargs)

        def log_item(item, name, level=0, is_root=False):
            indent = "    " * level
            if isinstance(item, torch.Tensor):
                print(f"{indent}Tensor '{name}' shape: {item.shape}")
            elif isinstance(item, tuple):
                if is_root and level == 0:
                    # Root level tuple, don't print it as a tuple unless it's a "true" tuple
                    for idx, sub_item in enumerate(item):
                        log_item(sub_item, f"{name}[{idx}]", level)
                else:
                    print(f"{indent}Tuple '{name}':")
                    for idx, sub_item in enumerate(item):
                        log_item(sub_item, f"{name}[{idx}]", level + 1)
            elif isinstance(item, int):
                print(f"{indent}Integer '{name}': Value={item}")
            elif isinstance(item, float):
                print(f"{indent}Float '{name}': Value={item}")
            else:
                print(f"{indent}Other-type '{name}': Type={type(item).__name__}, Value={item}")

        print(f"\n{'='*10}Entering {self.__class__.__name__}.{func.__name__}{'='*10}")
        print("Inputs:")
        arg_names = inspect.getfullargspec(func).args[1:]  # Excluding 'self'
        arg_values = args + tuple(kwargs.values())
        for name, value in zip(arg_names, arg_values):
            log_item(value, name)

        result = func(self, *args, **kwargs)
        print("\nOutputs:")
        if isinstance(result, tuple):
            log_item(result, "output", is_root=True)
        else:
            log_item(result, "output")

        print(f"{'='*10}Exiting {self.__class__.__name__}.{func.__name__}{'='*10}")
        return result
    return wrapper





# load the dataset
with open('input.txt', 'r', encoding='utf-8') as f:
    text = f.read()

# and the tokenizer
tokenizer = SimpleTokenizer(loaded_stoi, loaded_merges)


@dataclass # a class meant specifically to just hold data
class Config:
    """ 
    The default configuration & hyperparameters for my next-concept predictor
    """
    ### boring hyperparameters
    vocab_size: int = tokenizer.vocab_len
    max_seq_len: int = 256
    num_layers: int = 6
    sa_q_heads: int = 2
    sa_kv_heads: int = 1
    attn_bias: bool = False
    embed_dim: int = 256
    mlp_multiplier: int = 4
    sa_head_dim: int = 64
    theta: float = 100.0
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    dropout_rate: float = 0.05
    eps = 1e-6
    norm_affine: bool = True # whether norms should have a linear & bias after them
    norm_type = "RMSNorm"  # Options are RMSNorm, CosineNorm and LayerNorm

    ### Concept embedding vectors
    levels: int = 3
    combo: int = 4 # how many lower-level tokens/concepts to combine into the next level's concept
    @property
    def seq_len_list(self):
        return [(self.max_seq_len // (self.combo ** (i-1))) for i in range(1, self.levels + 1)]

    ### Dualcoder cross-attention
    ca_q_heads: int = sa_q_heads
    ca_kv_heads: int = sa_kv_heads
    ca_head_dim: int = sa_head_dim
    ca_use_RoPE: bool = False # True: expands out k & v tensors to be usable with rope. False: leaves k & v same size but no positional encodings
    predictive_mask: bool = False # True: upper-triangular predictive mask to focus model's attention. False: no mask like a regular encoder

    ### concept output
    output_layer = 'mlp' # options are 'linear' and 'mlp' which uses the default mlp_multiplier
    # how much to discount each higher level in the loss function compared to the last
    level_loss_weight: float = 1.0 
    # multiple losses can act on the concept vectors at once
    cos_loss: bool = True
    mse_loss: bool = True
    mae_loss: bool = False

    ### assertions
    assert sa_q_heads % sa_kv_heads == 0, 'the number of query heads must be divisible by the number of key-value heads in self-attention'
    assert ca_q_heads % ca_kv_heads == 0, 'the number of query heads must be divisible by the number of key-value heads in cross-attention'     
        
config = Config()
print(config)
print(f"sequence length of each model: {config.seq_len_list}")
print(f"loss discounts starting from lowest level: {[config.level_loss_weight**i for i in range(config.levels)]}")








def RoPE(x: torch.Tensor, dim: int, theta: float = 10000.0) -> torch.Tensor:
    """Applies the rotary embedding to the inputted query or key tensor"""
    # Validate that dim is even since we split it by 2 for real and imaginary parts
    if dim % 2 != 0: raise ValueError("Dimension 'dim' must be an even number.")
            
    # Get sequence length
    seq_len = x.size(1)
    device = x.device

    # Dynamically compute frequency cis based on the input sequence length
    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2, device=device).float() / dim))
    t = torch.arange(seq_len, device=device)
    freqs = torch.outer(t, freqs).float()
    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64
    # it's important to train on a wide variety of sequence lengths within your context length so that the model learns to generalize

    # Apply rotary embeddings to the input tensor
    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))
    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled
    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)
    x_out = x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)

    return x_out





class selfMQA(nn.Module):
    def __init__(self, config: Config):
        super().__init__()

        self.sa_q_heads = config.sa_q_heads
        self.sa_kv_heads = config.sa_kv_heads
        assert self.sa_q_heads % self.sa_kv_heads == 0
        self.num_queries_per_kv = self.sa_q_heads // self.sa_kv_heads

        self.embed_dim = config.embed_dim
        self.sa_head_dim = config.sa_head_dim
        self.theta = config.theta
        self.dropout_rate = config.dropout_rate

        #self.Wqkv = nn.Linear(self.embed_dim, 
        #                      (self.sa_q_heads + 2 * self.sq_kv_heads) * self.sa_head_dim, 
        #                      bias = config.attn_bias)
        #self.Wo = nn.Linear(self.sa_q_heads * self.sa_head_dim,
        #                    self.embed_dim,
        #                    bias = config.attn_bias)
        
        self.Wqkv = nn.Parameter(torch.Tensor(self.embed_dim, (self.sa_q_heads + 2 * self.sa_kv_heads) * self.sa_head_dim))
        nn.init.uniform_(self.Wqkv, -((1 / self.embed_dim) ** 0.5), (1 / self.embed_dim) ** 0.5)

        self.Wo = nn.Parameter(torch.Tensor(self.sa_q_heads * self.sa_head_dim, self.embed_dim))
        nn.init.uniform_(self.Wo, -((1 / (self.sa_q_heads * self.sa_head_dim)) ** 0.5), (1 / (self.sa_q_heads * self.sa_head_dim)) ** 0.5)

        # for our attention mask we'll create a boolean mask that'll later be turned into large negative values
        self.mask = torch.tril(torch.ones((config.max_seq_len, config.max_seq_len), dtype=torch.uint8)
                              ).view(1, 1, config.max_seq_len, config.max_seq_len).to(dtype=torch.bool)
        
        self.logging_enabled = False
        self.disabled_logging_functions = set()
    def enable_logging(self):
        self.logging_enabled = True
    def disable_logging(self):
        self.logging_enabled = False
    def disable_function_logging(self, func_name):
        self.disabled_logging_functions.add(func_name)
    def enable_function_logging(self, func_name):
        self.disabled_logging_functions.discard(func_name)

    @log_io
    def forward(self, x: torch.Tensor, training: bool = False) -> torch.Tensor:
        # Extracts batch size and input sequence length from the hidden states tensor.
        batch_size, input_len, _ = x.shape

        # splicing our primary projection to get the correct sub-matrices
        Wq, Wk, Wv = self.weight_splicing(self.Wqkv)
        # technically self.weight_splicing has access to self.Wqkv & Wo but this way our debugger can see them

        # Applies the linear projection to the hidden state to retrieve our q, k & v projections
        xq = F.dropout(x @ Wq, p=self.dropout_rate, training=training) # also dropout if we're training
        xk = F.dropout(x @ Wk, p=self.dropout_rate, training=training)
        xv = F.dropout(x @ Wv, p=self.dropout_rate, training=training)

        # Reshapes each of the Q, K, and V tensors to separate the heads and align the dimensions for attention operations.
        xq = xq.view(batch_size, -1, self.sa_q_heads, self.sa_head_dim)
        xk = xk.view(batch_size, -1, self.sa_kv_heads, self.sa_head_dim)
        xv = xv.view(batch_size, -1, self.sa_kv_heads, self.sa_head_dim)

        # Applies rotary positional embeddings to queries and keys to incorporate positional information.
        xq, xk = self.RoPE(xq, xk)

        # If the number of KV heads is different from the number of query heads, adjusts keys and values to match the query heads count.
        if self.sa_kv_heads != self.sa_q_heads:
            xk, xv = self.match_headcount(xk, xv) # [batch_size, input_len, n_local_heads, sa_head_dim]

        # Transposes Q, K, and V tensors to align them for the batch matrix multiplication in attention calculation.
        xq = xq.transpose(1, 2) # [batch_size, n_local_heads, input_len, sa_head_dim]
        xk = xk.transpose(1, 2)
        xv = xv.transpose(1, 2)

        # Calculates attention scores by performing a batch matrix multiplication between queries and keys, followed by scaling.
        logits = self.attend(xq, xk) # [batch_size, n_local_heads, input_len, input_len]
        
        # Applies the lower-triangular mask to the attention logits
        logits = self.apply_mask(logits, input_len)

        # applies values to get final output
        output = self.calc_output(logits, xv, batch_size, input_len, training) 

        # Applies the final linear projection to the attention output, mapping it back to self.embed_dim
        return F.dropout(output @ self.Wo, p=self.dropout_rate, training=training) # also dropout if we're training

    @log_io
    def weight_splicing(self, Wqkv):
        Wq, Wk, Wv = Wqkv.split([self.sa_q_heads * self.sa_head_dim,
                                 self.sa_kv_heads * self.sa_head_dim,
                                 self.sa_kv_heads * self.sa_head_dim],dim = -1)
        return Wq, Wk, Wv

    @log_io
    def RoPE(self, xq, xk):
        xq = RoPE(xq, self.sa_head_dim, self.theta)
        xk = RoPE(xk, self.sa_head_dim, self.theta)
        return xq, xk

    @log_io
    def match_headcount(self, xk, xv):
        xk = torch.repeat_interleave(xk, self.num_queries_per_kv, dim=2)
        xv = torch.repeat_interleave(xv, self.num_queries_per_kv, dim=2)
        return xk, xv

    @log_io
    def attend(self, xq, xk):
        return torch.matmul(xq, xk.transpose(2, 3)) * (self.sa_head_dim ** -0.5)
        
    @log_io
    def apply_mask(self, logits, input_len):
        return torch.where(self.mask[..., :input_len, :input_len].expand_as(logits),
                           logits,
                           torch.tensor(-1e30, device=logits.device, dtype=logits.dtype))
    
    @log_io
    def calc_output(self, logits, xv, batch_size, input_len, training):
        # Applies softmax to the logits to obtain attention probabilities
        scores = F.softmax(logits, dim=-1)

        # also applies dropout if we're training
        scores = F.dropout(scores, p=self.dropout_rate, training=training)
        
        # Computes the weighted sum of values based on the attention scores to obtain the output of the attention mechanism.
        output = scores @ xv # [batch_size, n_local_heads, input_len, sa_head_dim]

        # Reshapes the attention output to match the expected output dimensions, combining the heads back into the hidden dimension.
        return output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)





# Create an instance of selfMQA
module = selfMQA(config)

# Initially, logging is disabled
# Enable logging
module.enable_logging()

### Optionally disabling printing for sub-functions
#module.disable_function_logging('weight_splicing')
#module.disable_function_logging('RoPE')
#module.disable_function_logging('match_headcount')
#module.disable_function_logging('attend')
#module.disable_function_logging('apply_mask')
#module.disable_function_logging('calc_output')

# Call the forward method - logging will occur
output = module(torch.randn(32,config.max_seq_len,config.embed_dim))#, training=True)

# Disable logging. 
# This isn't actually necessary since we won't be using this object again but that's how you'd do it
module.disable_logging()

# clearing up ram jic we're training later
del module, output





class MLP(nn.Module):
    def __init__(self,
                 embed_dim: int,
                 mlp_multiplier: int,
                 dropout_rate: float = 0.1):
        super().__init__()
        self.mlp_multiplier = mlp_multiplier
        self.hidden_size = embed_dim * mlp_multiplier
        self.dropout_rate = dropout_rate

        # the gate, up and down projections
        self.gate_proj = nn.Linear(embed_dim, self.hidden_size)
        self.up_proj = nn.Linear(embed_dim, self.hidden_size)
        self.down_proj = nn.Linear(self.hidden_size, embed_dim)
        
        self.logging_enabled = False
        self.disabled_logging_functions = set()
    def enable_logging(self):
        self.logging_enabled = True
    def disable_logging(self):
        self.logging_enabled = False
    def disable_function_logging(self, func_name):
        self.disabled_logging_functions.add(func_name)
    def enable_function_logging(self, func_name):
        self.disabled_logging_functions.discard(func_name)
        
    @log_io
    def forward(self, x: torch.Tensor, training: bool = False ) -> torch.Tensor:
        output = self.down_proj(F.gelu(self.gate_proj(x)) * self.up_proj(x))
        return F.dropout(output, p=self.dropout_rate, training=training)





module = MLP(config.embed_dim, config.mlp_multiplier, config.dropout_rate)
module.enable_logging()
output = module(torch.randn(32,config.max_seq_len,config.embed_dim))
del module, output





class Norm(torch.nn.Module):
    def __init__(self, config):
        super().__init__()
        self.eps = config.eps
        self.affine = config.norm_affine
        self.dropout_rate = config.dropout_rate
        self.type = config.norm_type

        # Initialize weight and bias parameters for affine transformation
        # We start with ones for weight to keep the original scale initially, and zeros for bias.
        self.w = nn.Parameter(torch.ones(config.embed_dim))
        self.b = nn.Parameter(torch.zeros(config.embed_dim))
        
        self.logging_enabled = False
        self.disabled_logging_functions = set()
    def enable_logging(self):
        self.logging_enabled = True
    def disable_logging(self):
        self.logging_enabled = False
    def disable_function_logging(self, func_name):
        self.disabled_logging_functions.add(func_name)
    def enable_function_logging(self, func_name):
        self.disabled_logging_functions.discard(func_name)

    @log_io
    def forward(self, x: torch.Tensor, training: bool = False) -> torch.Tensor:
        # Normalize the input tensor
        if self.type == "CosineNorm":
            x = self.CosineNorm(x)
        elif self.type == "LayerNorm":
            x = self.LayerNorm(x)
        else: # defaults to RMSNorm bc that's the most commonly used nowadays
            x = self.RMSNorm(x)

        if self.affine: # Optionally apply the affine transformation
            x = x * self.w + self.b
            
        return F.dropout(x, p=self.dropout_rate, training=training) # and dropout if we're training

    @log_io
    def CosineNorm(self, x):
        # normalize x by dividing by its L2 norm along the last dimension.
        # this places x on the unit hypersphere centered at the origin
        # Add a small constant to the denominator to avoid division by zero.
        return x / torch.norm(x, p=2, dim=-1, keepdim=True).clamp(min=self.eps)

    @log_io
    def LayerNorm(self, x): # nn.LayerNorm() exists but might as well make it from scratch if we have to do the other two
        # normalize x by subtracting by its mean then dividing by its variance
        # this places x on a hypersphere of radius sqrt(dimension) centered at the origin
        mean = x.mean(dim=-1, keepdim=True)
        var = x.var(dim=-1, keepdim=True, unbiased=False)
        return (x - mean) / torch.sqrt(var + self.eps)

    @log_io
    def RMSNorm(self, x):
        # normalize x by dividing by its root-mean-square along the last dimension
        # this places x on a hypersphere of radius sqrt(dimension) with no certain center
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)





module = Norm(config)
module.enable_logging()

### disabling printing for sub-functions
#module.disable_function_logging('RMSNorm')
#module.disable_function_logging('LayerNorm')
#module.disable_function_logging('CosineNorm')

output = module(torch.randn(32, config.max_seq_len, config.embed_dim))
del module, output





class crossMQA(nn.Module):
    def __init__(self, config: Config):
        super().__init__()

        self.ca_q_heads = config.ca_q_heads
        self.ca_kv_heads = config.ca_kv_heads
        assert self.ca_q_heads % self.ca_kv_heads == 0
        self.num_queries_per_kv = self.ca_q_heads // self.ca_kv_heads
        self.embed_dim = config.embed_dim
        self.ca_head_dim = config.ca_head_dim
        self.sa_head_dim = config.sa_head_dim # used only for an assertion to make sure sizes will fit
        self.combo = config.combo # used only for an assertion to make sure sizes will fit
        self.theta = config.theta
        self.use_RoPE = config.ca_use_RoPE
        self.dropout_rate = config.dropout_rate
        self.predictive_mask = config.predictive_mask

        self.Wqkv = nn.Parameter(torch.Tensor(self.embed_dim, (self.ca_q_heads + 2 * self.ca_kv_heads) * self.ca_head_dim))
        nn.init.uniform_(self.Wqkv, -((1 / self.embed_dim) ** 0.5), (1 / self.embed_dim) ** 0.5)
        
        self.Wo = nn.Parameter(torch.Tensor(self.ca_q_heads * self.ca_head_dim, self.embed_dim))
        nn.init.uniform_(self.Wo, -((1 / (self.ca_q_heads * self.ca_head_dim)) ** 0.5), (1 / (self.ca_q_heads * self.ca_head_dim)) ** 0.5)
        
        self.logging_enabled = False
        self.disabled_logging_functions = set()
    def enable_logging(self):
        self.logging_enabled = True
    def disable_logging(self):
        self.logging_enabled = False
    def disable_function_logging(self, func_name):
        self.disabled_logging_functions.add(func_name)
    def enable_function_logging(self, func_name):
        self.disabled_logging_functions.discard(func_name)

    @log_io
    def forward(self, 
                x: torch.Tensor, # the current level tensor, sometimes a resid state full of tokens & sometimes concepts
                c: torch.Tensor, # the upper level tensor, always a resid state full of concept vecs
                training: bool = False
               ) -> torch.Tensor:
        
        # Extracts batch size and input sequence length from the hidden states tensor.
        batch_size, input_len_x, _ = x.shape
        batch_size_c, input_len_c, _ = c.shape
        assert batch_size == batch_size_c

        # splicing our projection to get the correct sub-matrices
        Wq, Wk, Wv = self.weight_splicing(self.Wqkv)

        # Applies the linear projection to the hidden state to retrieve our q, k & v projections
        xq = F.dropout(x @ Wq, p=self.dropout_rate, training=training) # also applies dropout if we're training
        ck = F.dropout(c @ Wk, p=self.dropout_rate, training=training)
        cv = F.dropout(c @ Wv, p=self.dropout_rate, training=training)

        # Reshapes each of the Q, K, and V tensors to separate the heads and align the dimensions for attention operations.
        xq = xq.view(batch_size, -1, self.ca_q_heads, self.ca_head_dim)
        ck = ck.view(batch_size, -1, self.ca_kv_heads, self.ca_head_dim)
        cv = cv.view(batch_size, -1, self.ca_kv_heads, self.ca_head_dim)

        # IF we want to use RoPE (doesn't fully make sense to)
        if self.use_RoPE:
            expand = input_len_x // input_len_c
            ck = ck.repeat_interleave(expand, dim=1) 
            cv = cv.repeat_interleave(expand, dim=1) # values need to be expanded for their use later on if we do this

            # Applies rotary positional embeddings to queries and keys to incorporate positional information.
            xq, ck = self.RoPE(xq, ck)

        # If the number of KV heads is different from the number of query heads, adjusts keys and values to match the query heads count.
        if self.ca_kv_heads != self.ca_q_heads:
            ck, cv = self.match_headcount(ck, cv) # [batch_size, input_len, n_local_heads, sa_head_dim]

        # Transposes Q, K, and V tensors to align them for the batch matrix multiplication in attention calculation.
        xq = xq.transpose(1, 2) # [batch_size, n_local_heads, input_len, ca_head_dim]
        ck = ck.transpose(1, 2)
        cv = cv.transpose(1, 2)

        # Calculates attention scores by performing a batch matrix multiplication between queries and keys, followed by scaling.
        logits = self.attend(xq, ck) # [batch_size, n_local_heads, input_len, input_len]
        
        # Optionally applies the upper-triangular mask to the attention logits
        if self.predictive_mask:
            logits = self.apply_mask(logits, input_len_x, input_len_c)

        # applies values to get final output
        output = self.calc_output(logits, cv, batch_size, input_len_x, training)

        # Applies the final linear projection to the attention output, mapping it back to d_i. 
        return F.dropout(output @ self.Wo, p=self.dropout_rate, training=training) # and dropout if we're training

    @log_io
    def weight_splicing(self, Wqkv):
        Wq, Wk, Wv = Wqkv.split([self.ca_q_heads * self.ca_head_dim,
                                 self.ca_kv_heads * self.ca_head_dim,
                                 self.ca_kv_heads * self.ca_head_dim],dim = -1)
        return Wq, Wk, Wv
        
    @log_io
    def RoPE(self, xq, xk):
        xq = RoPE(xq, self.ca_head_dim, self.theta)
        xk = RoPE(xk, self.ca_head_dim, self.theta)
        return xq, xk

    @log_io
    def match_headcount(self, xk, xv):
        xk = torch.repeat_interleave(xk, self.num_queries_per_kv, dim=2)
        xv = torch.repeat_interleave(xv, self.num_queries_per_kv, dim=2)
        return xk, xv

    @log_io
    def attend(self, xq, ck):
        return torch.matmul(xq, ck.transpose(2, 3)) * (self.ca_head_dim ** -0.5)
        
    @log_io
    def apply_mask(self, logits, input_len_x, input_len_c):
        self.mask = torch.triu(torch.ones((config.max_seq_len, input_len_c), dtype=torch.uint8)
                                  ).view(1, 1, config.max_seq_len, input_len_c).to(dtype=torch.bool)
        
        return torch.where(self.mask[..., :input_len_x, :input_len_c].expand_as(logits),
                           logits,
                           torch.tensor(-1e30, device=logits.device, dtype=logits.dtype))
    
    @log_io
    def calc_output(self, logits, cv, batch_size, input_len_x, training):
        # Applies softmax to the logits to obtain attention probabilities
        scores = F.softmax(logits, dim=-1)

        # also applies dropout if we're training
        scores = F.dropout(scores, p=self.dropout_rate, training=training)
        
        # Computes the weighted sum of values based on the attention scores to obtain the output of the attention mechanism.
        output = scores @ cv # [batch_size, n_local_heads, input_len, sa_head_dim]

        # Reshapes the attention output to match the expected output dimensions, combining the heads back into the hidden dimension.
        return output.transpose(1, 2).contiguous().view(batch_size, input_len_x, -1)





hold1 = config.levels
config.levels = 2
hold2 = config.predictive_mask
config.predictive_mask = False
module = crossMQA(config)
module.enable_logging()

### Optionally disabling printing for sub-functions
#module.disable_function_logging('weight_splicing')
#module.disable_function_logging('RoPE')
#module.disable_function_logging('match_headcount')
#module.disable_function_logging('attend')
#module.disable_function_logging('apply_mask')
#module.disable_function_logging('calc_output')

x0 = torch.randn(32, config.max_seq_len, config.embed_dim)
c1 = torch.randn(32, config.max_seq_len // config.combo, config.embed_dim)
output = module(x0, c1)
config.levels = hold1
config.predictive_mask = hold2
del hold1, hold2, module, x0, c1, output


hold1 = config.levels
config.levels = 3
hold2 = config.predictive_mask
config.predictive_mask = False
module = crossMQA(config)
module.enable_logging()

### Optionally disabling printing for sub-functions
#module.disable_function_logging('weight_splicing')
#module.disable_function_logging('RoPE')
#module.disable_function_logging('match_headcount')
#module.disable_function_logging('attend')
#module.disable_function_logging('apply_mask')
#module.disable_function_logging('calc_output')

c1 = torch.randn(32, config.max_seq_len // (config.combo**1), config.embed_dim)
c2 = torch.randn(32, config.max_seq_len // (config.combo**2), config.embed_dim)
output = module(c1, c2)
config.levels = hold1
config.predictive_mask = hold2
del hold1, hold2, module, c1, c2, output





# Original tensor using torch.triu on a 3x3 tensor of ones
original_tensor = torch.triu(torch.ones(3, 3))

input_len = 8
# Expand the tensor by duplicating each row twice more
expanded_tensor = original_tensor.repeat_interleave(3, dim=0)

out = expanded_tensor[:input_len,:]
out


hold1 = config.levels
config.levels = 2
hold2 = config.predictive_mask
config.predictive_mask = True
module = crossMQA(config)
module.enable_logging()

### Optionally disabling printing for sub-functions
#module.disable_function_logging('weight_splicing')
#module.disable_function_logging('RoPE')
#module.disable_function_logging('match_headcount')
#module.disable_function_logging('attend')
#module.disable_function_logging('apply_mask')
#module.disable_function_logging('calc_output')

x0 = torch.randn(32, config.max_seq_len, config.embed_dim)
c1 = torch.randn(32, config.max_seq_len // config.combo, config.embed_dim)
output = module(x0, c1)
config.levels = hold1
config.predictive_mask = hold2
del hold1, hold2, module, x0, c1, output








class Layer(nn.Module):
    def __init__(self, config: Config):
        super().__init__()

        self.pre_self_mqa_norm = Norm(config)
        self.self_mqa = selfMQA(config)
        self.post_self_mqa_norm = Norm(config)
        
        self.pre_cross_mqa_x_norm = Norm(config)
        self.pre_cross_mqa_c_norm = Norm(config)
        self.cross_mqa = crossMQA(config)
        self.post_cross_mqa_norm = Norm(config)
        
        self.pre_mlp_norm = Norm(config)
        self.mlp = MLP(config.embed_dim, config.mlp_multiplier, config.dropout_rate)
        self.post_mlp_norm = Norm(config)
        
        self.logging_enabled = False
        self.disabled_logging_functions = set()
    def enable_logging(self):
        self.logging_enabled = True
    def disable_logging(self):
        self.logging_enabled = False
    def disable_function_logging(self, func_name):
        self.disabled_logging_functions.add(func_name)
    def enable_function_logging(self, func_name):
        self.disabled_logging_functions.discard(func_name)

    @log_io
    def forward(self, 
                x: torch.Tensor,
                c: torch.Tensor = None,
                training: bool = False,
               ) -> torch.Tensor:
        x = x + self.self_mqa_connection(x, training)
        if c is not None:
            x = x + self.cross_mqa_connection(x, c, training)
        x = x + self.mlp_connection(x, training)
        return x

    @log_io
    def self_mqa_connection(self, x, training):
        return self.post_self_mqa_norm(self.self_mqa(self.pre_self_mqa_norm(x, training), training), training)

    @log_io
    def cross_mqa_connection(self, x, c, training):
        return self.post_cross_mqa_norm(self.cross_mqa(self.pre_cross_mqa_x_norm(x, training), 
                                                       self.pre_cross_mqa_c_norm(c, training), training), training)

    @log_io
    def mlp_connection(self, x, training):
        return self.post_mlp_norm(self.mlp(self.pre_mlp_norm(x, training), training), training)





module = Layer(config)
module.enable_logging()

### enabling printing for sub-modules
#module.pre_self_mqa_norm.enable_logging()
#module.self_mqa.enable_logging()
#module.post_self_mqa_norm.enable_logging()
#module.pre_cross_mqa_norm.enable_logging()
#module.cross_mqa.enable_logging()
#module.post_cross_mqa_norm.enable_logging()
#module.pre_mlp_norm.enable_logging()
#module.mlp.enable_logging()
#module.post_mlp_norm.enable_logging()

### disabling printing for sub-functions
#module.disable_function_logging('self_mqa_connection')
#module.disable_function_logging('cross_mqa_connection')
#module.disable_function_logging('mlp_connection')

output = module(torch.randn(32,config.max_seq_len,config.embed_dim))
del module, output


hold = config.predictive_mask
config.predictive_mask = False
module = Layer(config)
module.enable_logging()

### enabling printing for sub-modules
#module.pre_self_mqa_norm.enable_logging()
#module.self_mqa.enable_logging()
#module.post_self_mqa_norm.enable_logging()
#module.pre_cross_mqa_norm.enable_logging()
#module.cross_mqa.enable_logging()
#module.post_cross_mqa_norm.enable_logging()
#module.pre_mlp_norm.enable_logging()
#module.mlp.enable_logging()
#module.post_mlp_norm.enable_logging()

### disabling printing for sub-functions
#module.disable_function_logging('self_mqa_connection')
#module.disable_function_logging('cross_mqa_connection')
#module.disable_function_logging('mlp_connection')

x = torch.randn(32, config.max_seq_len, config.embed_dim)
c = torch.randn(32, config.max_seq_len // config.combo, config.embed_dim)
output = module(x, c)
config.predictive_mask = hold
del hold, module, output














# to prevent the warning statement from printing hella times
cvec_warning = False

class Body(nn.Module):
    def __init__(self, config: Config, embedder: torch.Tensor):
        super().__init__()
        self.max_seq_len = config.max_seq_len
        self.combo = config.combo
        self.levels = config.levels
        self.embedding = embedder.weight
        
        # Initialize a sequence of Layer instances as specified by the number of hidden layers in the config
        self.layers = nn.ModuleList(Layer(config) for _ in range(config.num_layers))
        
        # Initialize normalization layer to be applied after the last decoder layers, stabilizing the output
        self.final_norms = nn.ModuleList(Norm(config) for _ in range(config.levels))
        
        self.logging_enabled = False
        self.disabled_logging_functions = set()
    def enable_logging(self):
        self.logging_enabled = True
    def disable_logging(self):
        self.logging_enabled = False
    def disable_function_logging(self, func_name):
        self.disabled_logging_functions.add(func_name)
    def enable_function_logging(self, func_name):
        self.disabled_logging_functions.discard(func_name)
        
    @log_io
    def forward(self,
                x0s: Tuple[torch.Tensor], # ordered from tokens -> highest concepts
                targets: Tuple[torch.Tensor] = None,
                cvec_samples: int = None,
                cvec_greedy: bool = False,
                cvec_temp: float = 1.0,
               ) -> Tuple[torch.Tensor]:
        return self.forward_training(x0s, targets) if targets is not None else self.forward_inference(x0s, cvec_samples, cvec_greedy, cvec_temp)
        
    @log_io
    def forward_training(self,
                         x0s: Tuple[torch.Tensor], # ordered from tokens -> highest concepts
                         targets: Tuple[torch.Tensor],
                        ) -> Tuple[torch.Tensor]:
        # initiate tuple to hold final residual states
        xfs = ()
        # iterate through model levels, starting from highest level concepts & ending at lowest level tokens
        for i, x in enumerate(reversed(x0s)): # reversed() makes us start at highest level

            if i == 0:
                # if we're dealing with the highest level concepts, there's nothing to cross-attend to
                x = self.layers_loop(x, i, c = None, training=True)
            else:
                # the current level x will cross-attend to the higher level c
                x = self.layers_loop(x, i, c=targets[-i], training=True)

            # final norm
            x = self.final_norms[i](x, training=True)
            
            # add the final residual state of the level to our tuple
            xfs += (x,)
                
        return xfs # return all final residual states
        
    @log_io
    def forward_inference(self,
                          x0s: Tuple[torch.Tensor], # ordered from tokens -> highest concepts
                          cvec_samples: int = None,
                          cvec_greedy: bool = False,
                          cvec_temp: float = 1.0,
                         ) -> Tuple[torch.Tensor]:
        # initiate tuple to hold final residual states
        xfs = ()
        # iterate through model levels, starting from highest level concepts & ending at lowest level tokens
        for i, x in enumerate(reversed(x0s)): # reversed() makes us start at highest level
            print(i)
            # if we're dealing with a concept level, then we need to create entire concept sequences. for token level we only want one pass
            if i != len(x0s)-1:
                effective_max_seq_len = self.max_seq_len // (self.combo ** (self.levels-1-i))
                assert x.shape[1] <= effective_max_seq_len, f'somehow at level {i} a too-long sequence ({x.shape[1]} vs {effective_max_seq_len}) made it all the way to Body'
                extra_runs = effective_max_seq_len - x.shape[1]
                for k in range(extra_runs): # if extra_runs == 0 then this just won't do anything
                    print(k)
                    # run
                    if i == 0:
                        x_ = self.layers_loop(x, i, training=False) # xfs[i-1] is the concept embedding to pay attention to
                    else:
                        x_ = self.layers_loop(x, i, xfs[i-1], training=False) # xfs[i-1] is the concept embedding to pay attention to
                    # splice out the final prediction
                    x_ = x_[:,-1,:]
                    # select most similar concept vectors to be appended to the sequence
                    c = self.concept_matchup(x_, cvec_samples, cvec_greedy, cvec_temp)
                    # append to x
                    x = torch.concat([x, c.unsqueeze(1)], dim=1)

                # the final run. this one will actually be logits to get used with crossMQA
                if i == 0:
                    x_ = self.layers_loop(x, i, training=False) # xfs[i-1] is the concept embedding to pay attention to
                else:
                    x_ = self.layers_loop(x, i, xfs[i-1], training=False) # xfs[i-1] is the concept embedding to pay attention to
            else:
                if i == 0:
                    x_ = self.layers_loop(x, i, training=False) # xfs[i-1] is the concept embedding to pay attention to
                else:
                    x_ = self.layers_loop(x, i, xfs[i-1], training=False) # xfs[i-1] is the concept embedding to pay attention to

            # final norm
            x = self.final_norms[i](x, training=False)
            
            # add the final residual state of the level to our tuple
            xfs += (x,)
            
        return xfs[-1] # for inference, we only need to return the resid state of the token level

    @log_io
    def layers_loop(self, x: torch.Tensor,
                    i: int,
                    c: torch.Tensor = None,
                    training : bool = False,
                   ) -> torch.Tensor:
        
        # Iteratively process the input through each Layer of the model
        for layer in self.layers:

            # run through layers. at i==0 there's no higher level to attend to
            x = layer(x, training=training) if i == 0 else layer(x, c, training)
                
        return x

    @log_io
    def concept_matchup(self,
                        c: torch.Tensor,
                        cvec_samples: int,
                        cvec_greedy: bool,
                        cvec_temp: float,
                        ) -> torch.Tensor:
        global cvec_warning
        batch_size, d = c.size()
        vocab_size = self.embedding.size(0)
        embedding = self.embedding[:,:d]
    
        # Batch cosine similarity
        # Reshape c: (batch_size x 1 x embedding_dim)
        # Reshape embedding: (1 x vocab_size x embedding_dim)
        # Resulting similarity: (batch_size x vocab_size)
        token_similarities = F.cosine_similarity(c.unsqueeze(1), embedding.unsqueeze(0), dim=-1)
        
        # how many tokens will we sample to build up our chosen concept vector?
        if cvec_samples is None:
            cvec_samples = self.combo ** (self.levels-1)
            if (cvec_warning == False) or (cvec_warning is None):
                print(f"cvec_samples not defined. defaulting to highest level's minimum size: combo**(levels-1) = {cvec_samples}")
                cvec_warning = True
        assert cvec_samples >= self.combo ** (self.levels-1), f'cvec_samples = {cvec_samples} needs to be >= self.combo ** (self.levels-1) = {self.combo ** (self.levels-1)}'
        
        # Select top-k token embeddings for each concept vector
        topk_token_indices = torch.topk(token_similarities, k=cvec_samples, dim=1).indices  # (batch_size x sample)
    
        # Generate concept embeddings for each set of top-k token embeddings
        concept_embeddings_batch = []
        X_sizes_batch = []
        for i in range(batch_size):
            # Pass the list of indices for each concept
            concept_embeddings, X_sizes = self.create_concept_embeddings(embedding, 
                                                                         [topk_token_indices[i].tolist()])
            concept_embeddings_batch.append(concept_embeddings.squeeze(0))  # Remove the extra batch dimension
            X_sizes_batch.append(X_sizes)
    
        # Convert list of tensors to a tensor
        concept_embeddings_batch = torch.stack(concept_embeddings_batch)  # (batch_size x max_X_size x d)
    
        # Calculate concept similarities for each concept in the batch
        concept_similarities_batch = F.cosine_similarity(c.unsqueeze(1), concept_embeddings_batch, dim=-1)
    
        # Select the best matching concept embedding for each concept vector in the batch
        if cvec_greedy:
            best_concept_indices = concept_similarities_batch.argmax(dim=1)
            matched_concepts = concept_embeddings_batch[torch.arange(batch_size), best_concept_indices]
        else:
            # Apply softmax with temperature and sample
            topk_concept_probs = F.softmax(concept_similarities_batch / cvec_temp, dim=1)
            concept_topk_idx = torch.multinomial(topk_concept_probs, num_samples=1).squeeze(1)
            matched_concepts = concept_embeddings_batch[torch.arange(batch_size), concept_topk_idx]
    
        return matched_concepts

    @log_io
    def create_concept_embeddings(self, E: torch.Tensor, indices: torch.Tensor):
        """
        Create concept embeddings for a batch of indices.
    
        E: Embedding matrix (vocab_size x embedding_dim)
        indices: A list of lists of indices (batch_size x num_indices)
        """
        batch_size = len(indices)
        d = E.size(1)
        X_sizes = [(len(ind) - 1) * len(ind) // 2 for ind in indices]
        max_X_size = max(X_sizes)
        X = torch.empty((batch_size, max_X_size, d), dtype=E.dtype)
        
        # this could prolly be done way more efficiently with tensor operations
        for b in range(batch_size):
            count = 0
            for i in range(len(indices[b])):
                for j in range(i + 1, len(indices[b])):
                    X[b, count] = E[indices[b][i]] + E[indices[b][j]]
                    count += 1
            # Padding the rest if necessary
            if count < max_X_size:
                X[b, count:] = torch.zeros((max_X_size - count, d))
        
        # X_sizes is not useful rn but i think it may be later when we switch away from TinyShakespeare
        # and over to data that actually has variable sequence lengths
        return X, X_sizes








# first let's do 2 levels training
embedding = nn.Embedding(config.vocab_size, config.embed_dim)
hold = config.levels
config.levels = 2
module = Body(config, embedding)
module.enable_logging()

### enabling logging for sub-modules
#module.layers[0].enable_logging()
#module.final_norms[0].enable_logging()

### disabling logging for sub-functions
module.disable_function_logging('forward')
#module.disable_function_logging('forward_training')
#module.disable_function_logging('forward_inference')
#module.disable_function_logging('layers_loop')
#module.disable_function_logging('concept_matchup')
#module.disable_function_logging('create_concept_embeddings')

x = torch.randn(32, config.max_seq_len, config.embed_dim)
c = torch.randn(32, config.max_seq_len // config.combo, config.embed_dim)
x0s = (x,c)
targets = (x + torch.randn_like(x), c + torch.randn_like(c))
output = module(x0s, targets)
config.levels = hold
del embedding, hold, module, x, c, x0s, targets, output


# 3 levels training
embedding = nn.Embedding(config.vocab_size, config.embed_dim)
hold = config.levels
config.levels = 3
module = Body(config, embedding)
module.enable_logging()

### enabling logging for sub-modules
#module.layers[0].enable_logging()
#module.final_norms[0].enable_logging()

### disabling logging for sub-functions
module.disable_function_logging('forward')
#module.disable_function_logging('forward_training')
#module.disable_function_logging('forward_inference')
#module.disable_function_logging('layers_loop')
#module.disable_function_logging('concept_matchup')
#module.disable_function_logging('create_concept_embeddings')

x0 = torch.randn(32, config.max_seq_len // (config.combo**0), config.embed_dim)
c1 = torch.randn(32, config.max_seq_len // (config.combo**1), config.embed_dim)
c2 = torch.randn(32, config.max_seq_len // (config.combo**2), config.embed_dim)
x0s = (x0, c1, c2)
targets = (x0 + torch.randn_like(x0), c1 + torch.randn_like(c1), c2 + torch.randn_like(c2))
output = module(x0s, targets)
config.levels = hold
del embedding, hold, module, x0, c1, c2, x0s, targets, output


# 4 levels training
embedding = nn.Embedding(config.vocab_size, config.embed_dim)
hold = config.levels
config.levels = 4
module = Body(config, embedding)
module.enable_logging()

### enabling logging for sub-modules
#module.layers[0].enable_logging()
#module.final_norms[0].enable_logging()

### disabling logging for sub-functions
module.disable_function_logging('forward')
#module.disable_function_logging('forward_training')
#module.disable_function_logging('forward_inference')
#module.disable_function_logging('layers_loop')
#module.disable_function_logging('concept_matchup')
#module.disable_function_logging('create_concept_embeddings')

x0 = torch.randn(32, config.max_seq_len // (config.combo**0), config.embed_dim)
c1 = torch.randn(32, config.max_seq_len // (config.combo**1), config.embed_dim)
c2 = torch.randn(32, config.max_seq_len // (config.combo**2), config.embed_dim)
c3 = torch.randn(32, config.max_seq_len // (config.combo**3), config.embed_dim)
x0s = (x0, c1, c2, c3)
targets = (x0 + torch.randn_like(x0), c1 + torch.randn_like(c1), c2 + torch.randn_like(c2), c3 + torch.randn_like(c3))
output = module(x0s, targets)
config.levels = hold
del embedding, hold, module, x0, c1, c2, c3, x0s, targets, output





# now 2 levels partial sequence length, so like we're doing inference
# our partial sequence will always be a clean interval of config.combo because NCP_Matformer.create_x0s() uses CombineEmbeddings() to make the intervals clean
embedding = nn.Embedding(config.vocab_size, config.embed_dim)
hold = config.levels
config.levels = 2
module = Body(config, embedding)
module.enable_logging()

### enabling logging for sub-modules
#module.layers[0].enable_logging()
#module.final_norms[0].enable_logging()

### disabling logging for sub-functions
#module.disable_function_logging('forward')
#module.disable_function_logging('forward_training')
#module.disable_function_logging('forward_inference')
#module.disable_function_logging('layers_loop')
#module.disable_function_logging('concept_matchup')
module.disable_function_logging('create_concept_embeddings') # rn this one is hella inefficient using a for loop over batch so not fun to print

x = torch.randn(32, config.max_seq_len - (config.combo**2), config.embed_dim)
c = torch.randn(32, (config.max_seq_len // config.combo) - config.combo, config.embed_dim)
x0s = (x,c)
output = module(x0s, cvec_samples = config.combo ** config.levels)
config.levels = hold
del embedding, hold, module, x, c, x0s, output


# now 3 levels partial sequence length, so like we're doing inference
# our partial sequence will always be a clean interval of config.combo because NCP_Matformer.create_x0s() uses CombineEmbeddings() to make the intervals clean
embedding = nn.Embedding(config.vocab_size, config.embed_dim)
hold = config.levels
config.levels = 3
module = Body(config, embedding)
module.enable_logging()

### enabling logging for sub-modules
#module.layers[0].enable_logging()
#module.final_norms[0].enable_logging()

### disabling logging for sub-functions
#module.disable_function_logging('forward')
#module.disable_function_logging('forward_training')
#module.disable_function_logging('forward_inference')
#module.disable_function_logging('layers_loop')
module.disable_function_logging('concept_matchup') # this one is pretty boring and our prints are already long enough
module.disable_function_logging('create_concept_embeddings') # rn this one is hella inefficient using a for loop over batch so not fun to print

x0 = torch.randn(32, config.max_seq_len // (config.combo ** 0) - (config.combo**3), config.embed_dim)
c1 = torch.randn(32, config.max_seq_len // (config.combo ** 1) - (config.combo**2), config.embed_dim)
c2 = torch.randn(32, config.max_seq_len // (config.combo ** 2) - (config.combo**1), config.embed_dim)
x0s = (x0, c1, c2)
output = module(x0s, cvec_samples = config.combo ** config.levels)
config.levels = hold
del embedding, hold, module, x0, c1, c2, x0s, output





class CombineEmbeddings(nn.Module):
    def __init__(self, embed_dim):
        super().__init__()
        self.padding_vector = nn.Parameter(torch.zeros(embed_dim), requires_grad=True)
        
        self.logging_enabled = False
        self.disabled_logging_functions = set()
    def enable_logging(self):
        self.logging_enabled = True
    def disable_logging(self):
        self.logging_enabled = False
    def disable_function_logging(self, func_name):
        self.disabled_logging_functions.add(func_name)
    def enable_function_logging(self, func_name):
        self.disabled_logging_functions.discard(func_name)
    
    @log_io
    def forward(self, tensor, combine_factor):
        b, t, d = tensor.shape

        # Calculate the necessary amount of padding
        remainder = t % combine_factor
        padding_needed = 0 if remainder == 0 else combine_factor - remainder
        
        if padding_needed > 0:
            # Replicate the padding vector the necessary number of times
            padding = self.padding_vector.repeat(padding_needed, 1).unsqueeze(0).expand(b, -1, -1)
            tensor = torch.cat([padding[...,:d], tensor], dim=1) # subset padding to fit with matryoshka size
        
        # Update t after padding
        t_padded = t + padding_needed
        
        # Reshape the tensor to group 'combine_factor' entries along the t dimension
        reshaped_tensor = tensor.view(b, t_padded // combine_factor, combine_factor, d)
        
        # Sum over the groups
        combined_tensor = reshaped_tensor.sum(dim=2)

        return combined_tensor





module = CombineEmbeddings(config.embed_dim)
module.enable_logging()
x = torch.randn(32, config.max_seq_len, config.embed_dim )
output = module(x, config.combo)
del module, x, output





class ConceptLoss(nn.Module):
    def __init__(self, config: Config):
        super().__init__()
        self.combo = config.combo
        self.levels = config.levels
        self.level_loss_weight = config.level_loss_weight

        self.MAE_loss = nn.L1Loss() if config.MAE_loss else None
        self.MSE_loss = nn.MSELoss() if config.MSE_loss else None
        self.COS_loss = nn.CosineSimilarity(dim=-1, eps=1e-6) if config.COS_loss else None
        
        self.logging_enabled = False
        self.disabled_logging_functions = set()
    def enable_logging(self):
        self.logging_enabled = True
    def disable_logging(self):
        self.logging_enabled = False
    def disable_function_logging(self, func_name):
        self.disabled_logging_functions.add(func_name)
    def enable_function_logging(self, func_name):
        self.disabled_logging_functions.discard(func_name)
            
    @log_io
    def forward(self, 
                xfs: Tuple[torch.Tensor], # xfs are ordered highest concept level -> token level
                targets: Tuple[torch.Tensor], # targets are ordered highest concept level -> token level
               ) -> torch.Tensor:
        # initialize loss value
        concept_loss = torch.tensor(0.0)
        
        # iterate through all concept-embedding layers and calculate loss
        for i in range(self.levels - 1):
            # select our relevant final residual state and target vectors
            lvl_output = xfs[i]
            lvl_targets = targets[i]
            
            # calculate the decay value placed on this level's total amount of loss
            lambadada = (self.level_loss_weight ** (self.levels -1 -i))
            
            # setup flattening for if we're doing MAE or MSE
            if (self.MAE_loss is not None) or (self.MSE_loss is not None):
                # Reshape output and target_vectors to combine batch and seq_len dimensions
                lvl_output_flat = lvl_output.view(-1, lvl_output.size(-1))
                lvl_targets_flat = lvl_targets.view(-1, lvl_targets.size(-1))

            # calculate loss values. notice multiple might occur or even none at all
            if self.MAE_loss is not None:
                concept_loss = concept_loss + self.MAE_loss(lvl_output_flat, target_vectors_flat) * lambadada
            if self.MSE_loss is not None:
                concept_loss = concept_loss + self.MSE_loss(lvl_output_flat, target_vectors_flat) * lambadada
            if self.COS_loss is not None:
                cosine_loss = (1 - self.concept_loss_fn(lvl_output, target_vectors)).mean()
                concept_loss = concept_loss + cosine_loss * lambadada

        return concept_loss








class NCPFormer(nn.Module):
    def __init__(self, config: Config, tokenizer: tokenizer):
        super().__init__()
        self.config = config
        self.tokenizer = tokenizer
        
        ### hyperparameters
        self.max_seq_len = config.max_seq_len
        self.sa_head_dim = config.sa_head_dim
        self.vocab_size = config.vocab_size
        self.embed_dim = config.embed_dim
        self.combo = config.combo
        self.levels = config.levels
        
        ### embedding
        # the embedding matrix. for converting tokens to the first residual state, and the last residual state to logits
        self.embedder = nn.Embedding(config.vocab_size, config.embed_dim)
        # the function that combines embeddings into higher level concept residual states
        self.embedding_combiner = CombineEmbeddings(config.embed_dim)

        ### the actual bulk of the model
        self.body = Body(config, self.embedder)
                
        ### the loss functions
        # lowest-level token model
        self.ce_loss_fn = nn.CrossEntropyLoss()
        # concept models
        self.concept_loss_fn = ConceptLoss(config)
        
        self.logging_enabled = False
        self.disabled_logging_functions = set()
    def enable_logging(self):
        self.logging_enabled = True
    def disable_logging(self):
        self.logging_enabled = False
    def disable_function_logging(self, func_name):
        self.disabled_logging_functions.add(func_name)
    def enable_function_logging(self, func_name):
        self.disabled_logging_functions.discard(func_name)
        
    @log_io
    def forward(self,
                input_token_ids: torch.Tensor, # a shape (batch_size, input_seq_len) list of integer token ids to run forward pass on
                target_token_ids: torch.Tensor = None, # a shape (batch_size, input_seq_len + combo ** (levels-1)) list of token ids to train on
                cvec_samples: int = None,
                cvec_greedy: bool = False,
                cvec_temp: float = 1.0,
               ) -> torch.Tensor:
        training = True if target_token_ids is not None else False
        if training: assert input_token_ids.shape[1] == target_token_ids.shape[1], 'inputs and targets have different length'

        # create the tuple of initial residual states to calculate on
        x0s = self.create_x0s(input_token_ids) # x0s are ordered token level -> highest concept level

        # create the tuple of target embedding vectors
        if training: targets = self.create_targets(target_token_ids, input_token_ids.shape[1]) # targets are ordered token level -> highest concept level

        # the body of the model that iterates through the decoder & cross-attention layers
        if training:
            xfs = self.body(x0s, targets) # xfs are ordered highest concept level -> token level
        else:
            xfs = self.body(x0s, cvec_samples=cvec_samples, cvec_greedy=cvec_greedy, cvec_temp=cvec_temp) 
        
        # calculating output logits
        logits = xfs[-1] @ self.embedder.weight.t()

        # loss calculations
        if training:
            ### first up is regular CE token loss
            batch_size, input_len, vocab_size = logits.shape
            # splice target tokens to exclude the ones that were only to be used by concept levels
            target_token_ids_spliced = target_token_ids[:,:input_len]
            # we reshape our logits & targets before calculating cross-entropy loss
            ce_loss = self.ce_loss_fn(logits.view(batch_size*input_len, vocab_size),
                                      target_token_ids_spliced.reshape(batch_size*input_len))

            ### the new thing, a regression loss for all our concept-embedding layers
            concept_loss = self.concept_loss_fn(xfs, reversed(targets))
            # adding it all together
            loss = ce_loss + concept_loss
        else: 
            # if we're not training, then we don't need to calculate loss
            loss = None
        
        return logits, loss
        
    @log_io
    def create_x0s(self, input_token_ids: torch.Tensor) -> Tuple[torch.Tensor]:
        # turn the input tokens into the first residual state using the embedding matrix
        x0 = self.embedder(input_token_ids) # (batch_size, input_len, embed_dim)
        
        # instantiate the tuple that'll hold all the residual states
        x0s = (x0 * (self.embed_dim ** 0.5),) 
        
        ### iterating through levels to create each higher-level concept residual state
        for i in range(self.levels-1):
            # combine into smaller tensor by adding token (or lower level concept) embeddings together
            lvl_combo = self.combo ** (i+1)
            x0c = self.embedding_combiner(x0, lvl_combo) # c stands for concept
            
            # finally add it to the tuple of residual states
            x0s += (x0c * (self.embed_dim ** 0.5),)
        
        return x0s

    @log_io
    def create_targets(self, target_token_ids: torch.Tensor, input_len: int) -> Tuple[torch.Tensor]:
        # turn the target tokens into the first residual state using the embedding matrix
        t0 = self.embedder(target_token_ids[:,input_len]) # (batch_size, input_len, embed_dim)
        
        # instantiate the tuple that'll hold all the residual states
        targets = (t0,) 
        
        ### iterating through levels to create each higher-level concepts
        for i in range(self.levels-1):
            # calculate the correct combo factor for this level
            lvl_combo = self.combo ** (i+1)

            # adjust input_len to ceiling of multiple of combo. this ensures we're predicting full concept vecs w/out any padding
            input_len_adj = input_len + (input_len % lvl_combo)

            # subset the currect targets to be predicted at this level
            target_token_ids_adj = target_token_ids[:, lvl_combo:lvl_combo + input_len_adj]

            # turn them into embeddings
            raw_target_vectors = embedder(target_token_ids_adj)

            # combine the token embeddings into concepts
            c_vecs = self.embedding_combiner(raw_target_vectors, lvl_combo)

            # append to tuple
            targets += (c_vecs,)
        
        return targets # later imma need to use .detach().clone() after cross-attention & before loss function
        
    @log_io
    def generate(self,
                 prompt: str,
                 output_len: int = 1, # the model will output 1 token by default
                 temperature: float = 0.7, # 1.0 would be no effect
                 top_p: float = 0.8,
                 top_k: int = 4,
                ) -> str: 
        """ Wrapper around sampler() that deals with manipulation of the sequence """
        # encoding the prompt into token indices
        tokens = self.tokenizer.encode(prompt)

        # turning it into the right tensor shape
        tokens = torch.tensor(tokens, device=config.device).unsqueeze(0)
        
        # we wouldn't want to go past the maximum context length we trained on
        if len(tokens) + output_len > self.config.max_seq_len:
            output_len = self.max_seq_len - len(tokens)
            print("capping output at maximum sequence length")

        for i in range(output_len):
            # get the model's output logits and ignore the loss, which would be a NoneType object
            logits, _ = self(tokens[:,:self.max_seq_len])
            
            next_token = self.Sampler(logits, temperature, top_p, top_k)

            # add our new token to the sequence
            tokens = torch.cat((tokens, next_token), dim=1)

        # resets this variable so that the corresponding warning in Body.concept_matchup can come up next time we perform inference
        global cvec_warning 
        cvec_warning = False

        # decode our list of tokens to an actual string
        return self.tokenizer.decode(tokens.squeeze(0).tolist())

    @torch.no_grad() # no need to keep track of gradients during inference
    @log_io
    def Sampler(
        self,
        logits: torch.Tensor, # shape (batch_size, input_len, vocab_size)
        temperature: float, # controls how boring vs random the outputs should be
        top_p: float, # the maximum cumulative probability of output options we're willing to consider
        top_k: int, # the maximum number of output options we're willing to consider
    ) -> torch.Tensor:
        # Select the last element for each sequence & apply temperature scaling
        logits = logits[:,-1,:].div_(temperature) # -> (batch_size, vocab_size)

        # Calculate probabilities with softmax.
        probs = torch.softmax(logits, dim=-1, dtype=torch.float) # dim=-1 is the vocab_size dimension that we calculate along

        # sort the probabilities to for use in top-p & top-k. both are (batch_size, vocab_size)
        probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)

        ### calculating top-p
        # creates same-size tensor of cumulatve probabilities instead of indivdiual probs
        probs_sum = torch.cumsum(probs_sort, dim=-1) 
        # mask where 0's are top-p selections & 1's are to be excluded
        top_ps_mask = (probs_sum - probs_sort) > top_p
        # the original probabilities with excluded tokens changed to 0.0
        probs_sort = torch.where(top_ps_mask, 0, probs_sort) 

        ### calculating top_k
        # create a shape (vocab_size) tensor that just iterates up by 1's
        top_ks_mask = torch.arange(probs_idx.shape[-1], device=probs_idx.device) 
        # expand our mask along the batch_size dimension to become size (batch_size, vocab_size)
        top_ks_mask = top_ks_mask.expand(probs_idx.shape[0], -1)
        # top_ks is a list of integers. we keep whichever entries in top_ks_mask are greater than their corresponding entries in top_ks
        top_ks_mask = top_ks_mask >= top_k

        # we'll be combining top-p with top-k and using whichever gives us fewer tokens. a very conservative approach
        # this trims probs_sort to also fit within our top_k requirement
        probs_sort = torch.where(top_ks_mask, 0, probs_sort)

        # Re-normalization so that total probabilities add up to 1
        probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))
        
        # now we rearrange the modified probabilities in probs_sort back to their original order according to probs_idx
        probs = torch.gather(probs_sort, dim=-1, index=torch.argsort(probs_idx, dim=-1))
        
        # samples from the distribution
        next_token_id = torch.multinomial(probs, num_samples=1)
        
        return next_token_id # returns the predicted token





module = NCP_MatFormer(config, tokenizer)
module.enable_logging()

### enabling logging for sub-modules
#module.embedding_combiner.enable_logging()
#module.body.enable_logging()
#module.concept_loss_fn.enable_logging()

### disabling logging for sub-functions
#module.disable_function_logging('create_x0s')
#module.disable_function_logging('create_targets')
#module.disable_function_logging('generate')
#module.disable_function_logging('sampler')

input_token_ids = torch.randint(config.vocab_size, 
                                (32, config.max_seq_len))
target_token_ids = torch.randint(config.vocab_size, 
                                 (32, config.max_seq_len + (config.combo ** (config.levels-1))))
output, loss = module(input_token_ids, target_token_ids)
del module, input_token_ids, target_token_ids, output, loss





model = NCPFormer(config, tokenizer).to(config.device)

# print the number of parameters in the model
print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')

print(model)





# Train and test splits
data = torch.tensor(tokenizer.encode(text), dtype=torch.long)
n = int(0.9*len(data)) # first 90% will be our training dataset, the rest for validation
train_data = data[:n]
val_data = data[n:]


# data loading for training which generates a small batch of data of inputs x and targets y
def get_batch(split, batch_size):
    # whether we grab from our training or validation dataset
    data = train_data if split == 'train' else val_data
    ix = torch.randint(len(data) - (config.max_seq_len + (config.combo ** (config.levels-1))), (batch_size,))
    x = torch.stack([data[i:i+config.max_seq_len] for i in ix])
    ### i actually need the y tensor to be + (config.combo ** (config.levels-1)) to fit the future concepts
    y = torch.stack([data[i+1:i+1+(config.max_seq_len + (config.combo ** (config.levels-1)))] for i in ix])
    x, y = x.to(config.device), y.to(config.device)
    return x, y


@torch.no_grad()
def estimate_loss(model, batch_size, eval_iters = 5): # to estimate loss during the training loop
    out = {}
    model.eval() # sets model to eval mode
    for split in ['train', 'val']:
        losses = torch.zeros(eval_iters)
        for k in range(eval_iters):
            X, Y = get_batch(split, batch_size)
            logits, loss = model(X, Y)
            losses[k] = loss.item()
        out[split] = losses.mean()
    model.train() # just resets to training mode
    return out


# create a PyTorch optimizer
# this is not what they used, but this learning rate & weight decay work for our tiny minGemma
config.learning_rate = 1e-5
config.weight_decay = 0.02
optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)

# how long we want to train for
config.max_iters = 2

# how often we want to check & see how our loss is doing
eval_interval = 1

# batch size to use
config.batch_size = 32


start_time = time.time()

# Enable anomaly detection. uncomment these lines if you need to do extensive debugging
#torch.autograd.set_detect_anomaly(True)

for iter in range(config.max_iters):

    # sample a batch of data
    xb, yb = get_batch('train', config.batch_size)
    
    # train
    logits, loss = model(xb, yb)
    optimizer.zero_grad(set_to_none=True)
    loss.backward()
    optimizer.step()
    
    # every once in a while evaluate the loss on train and val sets
    if iter % eval_interval == 0 or iter == config.max_iters - 1:
        current_time = time.time()
        elapsed_time = current_time - start_time
        losses = estimate_loss(model, config.batch_size)
        print(f"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, time elapsed: {elapsed_time:.2f} seconds")

# Disable anomaly detection after the training loop
#torch.autograd.set_detect_anomaly(False)





name = f'models/{model.__class__.__name__}_{time.strftime("%Y-%m-%d|%H-%M-%S")}'
torch.save(model.state_dict(), f'{name}.pth')

# Convert the dataclass object to a dictionary
config_dict = asdict(config)

# Serialize the dictionary to a JSON file
with open(f'{name}.json', 'w') as f:
    json.dump(config_dict, f)





name = 'NCP_MatFormer_2024-03-27|01-39-19'

# Deserialize the JSON file back to a dictionary
with open(f'models/{name}.json', 'r') as f:
    config_dict = json.load(f)

# Convert the dictionary back to a dataclass object
config = Config(**config_dict)

# Initialize a blank model
model = NCP_MatFormer(config, tokenizer).to(config.device)  

# here's the path to a minGemma model that i've trained with roughly 1m parameters
path = f'models/{name}.pth'

# Load the saved state dictionary
model.load_state_dict(torch.load(path)) 
# REMEMBER TO CHANGE VALUES IN CONFIG TO MATCH THE MODEL YOU'VE LOADED

# print the number of parameters in the model
print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')

# If you only plan to do inference, switch to evaluation mode
model.eval()

# If you plan to continue training the model, switch to training mode
#model.train()





input_str = "JULIET:\nO Romeo, Romeo! wherefore art thou " # the classic line
max_useable_output_len = config.max_seq_len - len(input_str)
output = model.generate(input_str, output_len = max_useable_output_len)
print(output)


len(output)
