# my virtual environments are rarely properly connected to jupyter so this fixes that
import sys
import os
current_dir = os.getcwd()  # Get the current working directory
venv_dir = os.path.join(current_dir, 'venv') 
python_version = str(sys.version_info.major) + '.' + str(sys.version_info.minor)
site_packages_path = os.path.join(venv_dir, 'lib', 'python' + python_version, 'site-packages')
sys.path.append(site_packages_path) 

# Importing pytorch
import torch
import torch.nn as nn
from torch.nn import functional as F

import random as r


v, d = 128, 256
embedder = nn.Embedding(v, d)
embedder.weight


class Norm(torch.nn.Module):
    def __init__(self, embed_dim: int, norm_type: str = "cos", eps: float = 1e-6, norm_affine: bool = False, dropout_rate: float = 0.1):
        super().__init__()
        self.eps = eps
        self.affine = norm_affine
        self.dropout_rate = dropout_rate
        self.norm_type = norm_type

        # Initialize weight and bias parameters for affine transformation
        # We start with ones for weight to keep the original scale initially, and zeros for bias.
        self.w = nn.Parameter(torch.ones(embed_dim))
        self.b = nn.Parameter(torch.zeros(embed_dim))

        self.logging_enabled = False
    def enable_logging(self):
        self.logging_enabled = True
    def disable_logging(self):
        self.logging_enabled = False

    def CosineNorm(self, x):
        # normalize x by dividing by its L2 norm along the last dimension.
        # this places x on the unit hypersphere centered at the origin
        # Add a small constant to the denominator to avoid division by zero.
        return x / torch.norm(x, p=2, dim=-1, keepdim=True).clamp(min=self.eps)

    def RMSNorm(self, x):
        # normalize x by dividing by its root-mean-square along the last dimension
        # this places x on a hypersphere of radius sqrt(dimension) with no certain center
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)

    def LayerNorm(self, x):
        # normalize x by subtracting by its mean then dividing by its variance
        # this places x on a hypersphere of radius sqrt(dimension) centered at the origin
        mean = x.mean(dim=-1, keepdim=True)
        var = x.var(dim=-1, keepdim=True, unbiased=False)
        return (x - mean) / torch.sqrt(var + self.eps)

    def splice_affine(self, weight, bias, d_i):
        return weight[:d_i], bias[:d_i]

    def forward(self, x: torch.Tensor, training: bool = False) -> torch.Tensor:
        # Normalize the input tensor
        if self.norm_type == "CosineNorm":
            x = self.CosineNorm(x)
        elif self.norm_type == "LayerNorm":
            x = self.LayerNorm(x)
        else: # defaults to RMSNorm bc that's the most commonly used nowadays
            x = self.RMSNorm(x)

        # Optionally apply the affine transformation with splicing
        if self.affine:
            w, b = self.splice_affine(self.w, self.b, x.shape[-1])
            x = x * w + b
            x = F.dropout(x, p=self.dropout_rate, training=training) # and dropout if we're training

        return x


# prompt: grab 2 random vectors from the embed and sum them

# Get two random indices within the vocabulary size
index1 = r.randint(0, v - 1)
index2 = r.randint(0, v - 1)

# Extract the corresponding vectors from the embedding layer
vector1 = embedder.weight[index1]
vector2 = embedder.weight[index2]

# create some noise
noise_sd = 0.3
noise = torch.randn(d) * noise_sd

# Sum the two vectors & add some noise
concept = vector1 + vector2 + noise



# prompt: Now initialize a `Norm` and use it on our summed vector

# Initialize a Norm layer with the same embedding dimension as the vectors
norm_layer = Norm(d)

# Apply the normalization to the sum vector
normalized_concept = norm_layer(concept)



# prompt: Initialize an nn.CosineSimilarity(dim=-1, eps=1e-6) and use it to compare normalized_concept to all the vectors in embedder

cosine_similarity = nn.CosineSimilarity(dim=-1, eps=1e-6)
similarity_scores = cosine_similarity(normalized_concept, embedder.weight)
similarity_scores


# prompt: Grab the topk of those similarity scores with k=8 and print them for me to see

topk_values, topk_indices = torch.topk(similarity_scores, k=8)
print(topk_values)
print(topk_indices)



def create_concept_embeddings(E, indices):
    k = len(indices)
    d = E.size(1)
    X_size = (k - 1) * k // 2
    X = torch.empty((X_size, d), dtype=E.dtype)

    count = 0
    for i in range(k):
        for j in range(i + 1, k):
            X[count] = E[indices[i]] + E[indices[j]]
            count += 1

    return X


E_comb = create_concept_embeddings(embedder.weight, topk_indices)
E_comb.shape, E_comb


degree2_similarity_scores = cosine_similarity(normalized_concept, E_comb)
degree2_similarity_scores[0]


def concept_matchup(c: torch.Tensor, embedding: torch.Tensor, combo: int, sample: int, greedy: bool = False, temp: float = 1.0):
    assert sample >= combo, 'you need to sample at least as many token embedding vectors as what you plan to combine'
    token_similarities = F.cosine_similarity(c, embedding, dim=-1)
    print(token_similarities.shape, token_similarities)
    topk_token_indices = torch.topk(token_similarities, k=sample).indices
    print(topk_token_indices.shape, topk_token_indices)
    concept_embeddings = create_concept_embeddings(embedding, topk_token_indices)
    print(concept_embeddings.shape, concept_embeddings)
    concept_similarities = F.cosine_similarity(c, concept_embeddings, dim=-1)
    print(concept_similarities.shape, concept_similarities)
    topk_concept_similarities, topk_concept_indices = torch.topk(concept_similarities, k=sample)
    print(topk_concept_similarities.shape, topk_concept_similarities)
    print(topk_concept_indices.shape, topk_concept_indices)
    if greedy:
        concept_idx = int(topk_concept_indices[0])
        print(concept_idx)
        return concept_embeddings[:,concept_idx]
    else:
        topk_concept_probs = F.softmax(topk_concept_similarities / temp, dim=-1)
        print(topk_concept_probs.shape, topk_concept_probs)
        concept_topk_idx = torch.multinomial(topk_concept_probs, num_samples = 1)
        print(concept_topk_idx)
        concept_idx = topk_concept_indices[concept_topk_idx]
        print(concept_idx)
        return concept_embeddings[concept_idx]



matched_concept = concept_matchup(concept, embedder.weight, 4, 8, greedy=False, temp=0.2)
matched_concept


import torch
import random as r

# Assuming embedder is defined and has an attribute 'weight'
# v is the size of the vocabulary, d is the dimension of the embedding, and b is the batch size

v, d = 128, 256
embedder = nn.Embedding(v, d)
embedder.weight

b = 32  # Batch size

# Get two batches of random indices within the vocabulary size
indices1 = torch.randint(0, v, (b,))
indices2 = torch.randint(0, v, (b,))

# Extract the corresponding vectors from the embedding layer for each index in the batch
vectors1 = embedder.weight[indices1]
vectors2 = embedder.weight[indices2]

# Create some noise for each vector in the batch
noise_sd = 0.3
noise = torch.randn(b, d) * noise_sd

# Sum the two batches of vectors & add some noise to each
concept_batch = vectors1 + vectors2 + noise
concept_batch.shape, concept_batch


def concept_matchup(c: torch.Tensor, embedding: torch.Tensor, combo: int, sample: int, greedy: bool = False, temp: float = 1.0):
    assert sample >= combo, 'you need to sample at least as many token embedding vectors as what you plan to combine'
    if len(c.shape) == 2:
        embedding = embedding.unsqueeze(0)
        print(embedding.shape)
        embedding = embedding.expand(c.shape[0], -1, -1)
        print(embedding.shape)
    
    token_similarities = F.cosine_similarity(c, embedding, dim=-1)
    print(token_similarities.shape, token_similarities)
    topk_token_indices = torch.topk(token_similarities, k=sample).indices
    print(topk_token_indices.shape, topk_token_indices)
    concept_embeddings = create_concept_embeddings(embedding, topk_token_indices)
    print(concept_embeddings.shape, concept_embeddings)
    concept_similarities = F.cosine_similarity(c, concept_embeddings, dim=-1)
    print(concept_similarities.shape, concept_similarities)
    topk_concept_similarities, topk_concept_indices = torch.topk(concept_similarities, k=sample)
    print(topk_concept_similarities.shape, topk_concept_similarities)
    print(topk_concept_indices.shape, topk_concept_indices)
    if greedy:
        concept_idx = int(topk_concept_indices[0])
        print(concept_idx)
        return concept_embeddings[:,concept_idx]
    else:
        topk_concept_probs = F.softmax(topk_concept_similarities / temp, dim=-1)
        print(topk_concept_probs.shape, topk_concept_probs)
        concept_topk_idx = torch.multinomial(topk_concept_probs, num_samples = 1)
        print(concept_topk_idx)
        concept_idx = topk_concept_indices[concept_topk_idx]
        print(concept_idx)
        return concept_embeddings[concept_idx]


matched_concepts = concept_matchup(concept_batch, embedder.weight, 4, 8, greedy=False, temp=0.2)
matched_concepts.shape, matched_concepts





import torch
import torch.nn as nn
from torch.nn import functional as F
import random as r

# Sample code modification for batch processing
def create_concept_embeddings(E, indices):
    """
    Create concept embeddings for a batch of indices.

    E: Embedding matrix (vocab_size x embedding_dim)
    indices: A list of lists of indices (batch_size x num_indices)
    """
    batch_size = len(indices)
    d = E.size(1)
    X_sizes = [(len(ind) - 1) * len(ind) // 2 for ind in indices]
    max_X_size = max(X_sizes)
    X = torch.empty((batch_size, max_X_size, d), dtype=E.dtype)

    for b in range(batch_size):
        count = 0
        for i in range(len(indices[b])):
            for j in range(i + 1, len(indices[b])):
                X[b, count] = E[indices[b][i]] + E[indices[b][j]]
                count += 1
        # Padding the rest if necessary
        if count < max_X_size:
            X[b, count:] = torch.zeros((max_X_size - count, d))

    return X, X_sizes

# Mock variables to illustrate the next steps
v, d = 128, 256
batch_size = 32
sample = 8
embedder = nn.Embedding(v, d)
indices = [[r.randint(0, v - 1) for _ in range(sample)] for _ in range(batch_size)]  # example batch of indices

# Example usage
concept_embeddings, _ = create_concept_embeddings(embedder.weight, indices) 
concept_embeddings.shape
# the _ may be useful if we ever do variable concept combo sizes


def concept_matchup(c: torch.Tensor, embedding: torch.Tensor, combo: int, sample: int, greedy: bool = False, temp: float = 1.0):
    """
    Adjust concept_matchup to handle a batch of concept vectors.

    c: Batch of concept vectors (batch_size x embedding_dim)
    embedding: Embedding matrix (vocab_size x embedding_dim)
    combo: Number of tokens to combine (ignored for this simplified example)
    sample: Number of top tokens to consider
    greedy: If True, select the top concept embedding, else sample based on similarity
    temp: Temperature for softmax
    """
    batch_size, d = c.size()
    vocab_size = embedding.size(0)

    # Batch cosine similarity
    # Reshape c for broadcasting: (batch_size x 1 x embedding_dim)
    # Reshape embedding for broadcasting: (1 x vocab_size x embedding_dim)
    # Resulting similarity: (batch_size x vocab_size)
    token_similarities = F.cosine_similarity(c.unsqueeze(1), embedding.unsqueeze(0), dim=-1)

    # Select top-k token embeddings for each concept vector
    topk_token_indices = torch.topk(token_similarities, k=sample, dim=1).indices  # (batch_size x sample)

    # Generate concept embeddings for each set of top-k token embeddings
    concept_embeddings_batch = []
    X_sizes_batch = []
    for i in range(batch_size):
        # Pass the list of indices for each concept
        concept_embeddings, X_sizes = create_concept_embeddings(embedding, [topk_token_indices[i].tolist()])
        concept_embeddings_batch.append(concept_embeddings.squeeze(0))  # Remove the extra batch dimension
        X_sizes_batch.append(X_sizes)

    # Convert list of tensors to a tensor
    concept_embeddings_batch = torch.stack(concept_embeddings_batch)  # (batch_size x max_X_size x d)

    # Calculate concept similarities for each concept in the batch
    concept_similarities_batch = F.cosine_similarity(c.unsqueeze(1), concept_embeddings_batch, dim=-1)

    # Select the best matching concept embedding for each concept vector in the batch
    if greedy:
        best_concept_indices = concept_similarities_batch.argmax(dim=1)
        matched_concepts = concept_embeddings_batch[torch.arange(batch_size), best_concept_indices]
    else:
        # Apply softmax with temperature and sample
        topk_concept_probs = F.softmax(concept_similarities_batch / temp, dim=1)
        concept_topk_idx = torch.multinomial(topk_concept_probs, num_samples=1).squeeze(1)
        matched_concepts = concept_embeddings_batch[torch.arange(batch_size), concept_topk_idx]

    return matched_concepts

# Example usage with dummy data
batch_size = 32
c = torch.randn(batch_size, d)  # Batch of concept vectors
output_concepts = concept_matchup(c, embedder.weight, 4, 8, greedy=False, temp=0.2)
output_concepts.unsqueeze(1).shape



