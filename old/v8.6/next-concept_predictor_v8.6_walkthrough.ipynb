{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bf96018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMMY DO NOT RUN, this is just for my venv\n",
    "import sys\n",
    "sys.path.append('/Users/tunadorable/local-repos/next-concept-predictor/venv/lib/python3.11/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d00c4831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85a5b920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting regular hyperparameters\n",
    "d=4\n",
    "v=12\n",
    "l = 2\n",
    "h=4\n",
    "\n",
    "activation = nn.ReLU()\n",
    "norm = nn.LayerNorm(d)\n",
    "\n",
    "# our tokens being mapped to indices. obvi a stupidly minimally small vocab\n",
    "vocabulary_dict = {\" I\":0,\n",
    "          \" think\":1,\n",
    "         \" there\":2,\n",
    "         \"fore\":3,\n",
    "         \" am\":4,\n",
    "         \"Every\":5,\n",
    "         \" cloud\":6,\n",
    "         \" has\":7,\n",
    "         \" a\":8,\n",
    "         \" silver\":9,\n",
    "         \" lining\":10,\n",
    "         \"<endoftext>\":11}\n",
    "\n",
    "single_seq_list = [[' I', ' think', ' there', 'fore', ' I', ' am', '<endoftext>']]\n",
    "double_seq_list = [[' I', ' think', ' there', 'fore', ' I', ' am', '<endoftext>'],\n",
    "               ['Every', ' cloud', ' has', ' a', ' silver', ' lining', '<endoftext>']]\n",
    "\n",
    "# ofc\n",
    "seed = 42069"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285be5a0",
   "metadata": {},
   "source": [
    "# table of contents\n",
    "\n",
    "1. regular next-token predictor functions\n",
    "2. regular NTP 2-step inference walkthrough\n",
    "3. regular NTP training loop\n",
    "4. my next-concept predictor functions\n",
    "5. NCP 3-step inference walkthrough\n",
    "6. NCP training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6480ed",
   "metadata": {},
   "source": [
    "### 1. regular next-token predictor functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3768a143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(seq_list, vocabulary_dict, verbose=False):\n",
    "    global seed, i, n, d, v, l, h, activation, norm\n",
    "    \n",
    "    # embedding matrix\n",
    "    #torch.manual_seed(seed)\n",
    "    E = norm(torch.randn(v,d))\n",
    "    if verbose:\n",
    "        print(\"E: \", E.shape, E)\n",
    "        \n",
    "    # turn list of tokens into indices\n",
    "    S_n = torch.tensor([[vocabulary_dict[word] for word in sentence] for sentence in seq_list])\n",
    "    if verbose:\n",
    "        print(\"S_n: \", S_n.dtype, S_n.shape, S_n)\n",
    "\n",
    "    # starting off with the first token for each sequence\n",
    "    if i==0: \n",
    "        S_i = S_n[:,i].unsqueeze(dim=1) \n",
    "    else: \n",
    "        S_i = S_n[:,0:i]\n",
    "    if verbose:\n",
    "        print(f\"i={i}\")\n",
    "        print(\"S_i: \", S_i.dtype, S_i.shape, S_i)\n",
    "        \n",
    "    return E, S_n, S_i\n",
    "\n",
    "def ffn(X_0, verbose=False):\n",
    "    \"\"\"\n",
    "    simple feedforward layer. \n",
    "    only included this so that there'd actually be some variation in inference output\n",
    "    \"\"\"\n",
    "    global seed, i, n, d, v, l, h, activation, norm\n",
    "    \n",
    "    # first linear layer\n",
    "    #torch.manual_seed(seed)\n",
    "    W_1 = torch.randn(d,d**2)\n",
    "    if verbose:\n",
    "        print(\"W_1: \", W_1.shape, W_1)\n",
    "    X_1 = torch.matmul(X_0,W_1)\n",
    "    if verbose:\n",
    "        print(\"X_1: \", X_1.shape, X_1)\n",
    "\n",
    "    # activation function\n",
    "    X_2 = activation(X_1)\n",
    "    if verbose:\n",
    "        print(\"X_2: \", X_2.shape, X_2)\n",
    "\n",
    "    # second linear layer\n",
    "    #torch.manual_seed(seed)\n",
    "    W_2 = torch.randn(d**2,d)\n",
    "    if verbose:\n",
    "        print(\"W_2: \", W_2.shape, W_2)\n",
    "    X_3 = torch.matmul(X_2,W_2)\n",
    "    if verbose:\n",
    "        print(\"X_3: \", X_3.shape, X_3)\n",
    "    \n",
    "    return X_3\n",
    "\n",
    "def layers(X_0):\n",
    "    global seed, i, n, d, v, l, h, activation, norm\n",
    "    \n",
    "    for j in range(l):\n",
    "        \n",
    "        # imma just ignore MHA cuz i'm not messing with it\n",
    "        # keeping FFN tho cuz i think i need nolinearity to ensure outputs differ from input\n",
    "        \n",
    "        if j == 0: \n",
    "            # first layer\n",
    "            X_i_ = ffn(X_0)\n",
    "            X_i = norm(X_0+X_i_)\n",
    "        elif j == l-1: \n",
    "            # last layer\n",
    "            X_i_ = ffn(X_i)\n",
    "            X_f = norm(X_i+X_i_)\n",
    "        else: \n",
    "            # all layers in the middle\n",
    "            X_i_ = ffn(X_i)\n",
    "            X_i = norm(X_i+X_i_)\n",
    "        \n",
    "    return X_f\n",
    "\n",
    "# a function to give us a pretty read off of the sequences\n",
    "def tensor_to_string(tensor, token_to_index):\n",
    "    \"\"\"\n",
    "    Convert a PyTorch tensor of indices into readable strings using a mapping dictionary.\n",
    "\n",
    "    :param tensor: PyTorch tensor of indices. Can be 1D or 2D.\n",
    "    :param token_to_index: Dictionary mapping tokens to indices.\n",
    "    :return: A single string if the tensor is 1D, or a list of strings if the tensor is 2D.\n",
    "    \"\"\"\n",
    "    \n",
    "    index_to_token = {v: k for k, v in token_to_index.items()}\n",
    "    \n",
    "    # Check if the tensor is 1D or 2D\n",
    "    if len(tensor.shape) == 1:\n",
    "        # Convert the 1D tensor to a single string\n",
    "        return ' '.join([index_to_token[int(idx)] for idx in tensor])\n",
    "    elif len(tensor.shape) == 2:\n",
    "        # Convert the tensor to a list of lists and then to strings\n",
    "        sequences = tensor.tolist()\n",
    "        return [' '.join([index_to_token[int(idx)] for idx in seq]) for seq in sequences]\n",
    "    else:\n",
    "        raise ValueError(\"Tensor must be either 1D or 2D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66089613",
   "metadata": {},
   "source": [
    "### 2. regular NTP 2-step inference walkthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "412ac581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIRST PASS\n",
      "i=1\n",
      "S_i:  torch.Size([1, 1])\n",
      "E:  torch.Size([12, 4])\n",
      "X_0:  torch.Size([1, 1, 4])\n",
      "X_f:  torch.Size([1, 1, 4])\n",
      "Y:  torch.Size([1, 1, 12])\n",
      "Z:  torch.Size([1, 12])\n",
      "P:  torch.Size([1, 12]) tensor([1.0000], grad_fn=<SumBackward1>)\n",
      "s_iplus1:  torch.Size([1])\n",
      "S_iplus1:  torch.Size([1, 2]) [' I  there']\n",
      "SECOND PASS\n",
      "i=2\n",
      "X_0:  torch.Size([1, 2, 4])\n",
      "X_f:  torch.Size([1, 2, 4])\n",
      "Y:  torch.Size([1, 2, 12])\n",
      "Z:  torch.Size([1, 12])\n",
      "P:  torch.Size([1, 12]) tensor([1.0000], grad_fn=<SumBackward1>)\n",
      "s_iplus1:  torch.Size([1])\n",
      "S_iplus1:  torch.Size([1, 3]) [' I  there  think']\n"
     ]
    }
   ],
   "source": [
    "### FIRST PASS\n",
    "print(\"FIRST PASS\")\n",
    "verbose=False\n",
    "single_seq = [[' I']]\n",
    "\n",
    "i=1\n",
    "print(f\"i={i}\")\n",
    "\n",
    "E, _, S_i = initialize(single_seq, vocabulary_dict)\n",
    "print(\"S_i: \", S_i.shape)\n",
    "print(\"E: \", E.shape)\n",
    "if verbose:\n",
    "    print(S_i)\n",
    "    print(E)\n",
    "\n",
    "X_0 = F.embedding(S_i, E)\n",
    "print(\"X_0: \", X_0.shape)\n",
    "if verbose: print(X_0)\n",
    "\n",
    "X_f = layers(X_0)\n",
    "print(\"X_f: \", X_f.shape)\n",
    "if verbose: print(X_f)\n",
    "\n",
    "Y = torch.matmul(X_f,E.T)\n",
    "print(\"Y: \", Y.shape)\n",
    "if verbose: print(Y)\n",
    "\n",
    "Z = Y[:,-1,:]\n",
    "print(\"Z: \", Z.shape)\n",
    "if verbose: print(Z)\n",
    "\n",
    "P = F.softmax(Z, dim=-1)\n",
    "print(\"P: \", P.shape, torch.sum(P,dim=1))\n",
    "if verbose: print(P)\n",
    "\n",
    "s_iplus1 = torch.max(P, dim=1).indices\n",
    "print(\"s_iplus1: \", s_iplus1.shape)\n",
    "if verbose: print(s_iplus1)\n",
    "\n",
    "S_iplus1 = torch.cat((S_i, s_iplus1.unsqueeze(dim=1)), dim=1)\n",
    "print(\"S_iplus1: \", S_iplus1.shape, tensor_to_string(S_iplus1, vocabulary_dict))\n",
    "if verbose: print(S_iplus1)\n",
    "    \n",
    "### second pass\n",
    "print(\"SECOND PASS\")\n",
    "i=2\n",
    "print(f\"i={i}\")\n",
    "\n",
    "S_i = S_iplus1\n",
    "\n",
    "X_0 = F.embedding(S_i, E)\n",
    "print(\"X_0: \", X_0.shape)\n",
    "if verbose: print(X_0)\n",
    "\n",
    "X_f = layers(X_0)\n",
    "print(\"X_f: \", X_f.shape)\n",
    "if verbose: print(X_f)\n",
    "\n",
    "Y = torch.matmul(X_f,E.T)\n",
    "print(\"Y: \", Y.shape)\n",
    "if verbose: print(Y)\n",
    "\n",
    "Z = Y[:,-1,:]\n",
    "print(\"Z: \", Z.shape)\n",
    "if verbose: print(Z)\n",
    "\n",
    "P = F.softmax(Z, dim=-1)\n",
    "print(\"P: \", P.shape, torch.sum(P,dim=1))\n",
    "if verbose: print(P)\n",
    "\n",
    "s_iplus1 = torch.max(P, dim=1).indices\n",
    "print(\"s_iplus1: \", s_iplus1.shape)\n",
    "if verbose: print(s_iplus1)\n",
    "\n",
    "S_iplus1 = torch.cat((S_i, s_iplus1.unsqueeze(dim=1)), dim=1)\n",
    "print(\"S_iplus1: \", S_iplus1.shape, tensor_to_string(S_iplus1, vocabulary_dict))\n",
    "if verbose: print(S_iplus1)\n",
    "    \n",
    "# really i should loop it & add logic to make the whole thing stop when <endoftext> is predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0317da",
   "metadata": {},
   "source": [
    "### 3. regular NTP training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9bb7d876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=1\n",
      "S_n:  torch.Size([2, 7]) tensor([[ 0,  1,  2,  3,  0,  4, 11],\n",
      "        [ 5,  6,  7,  8,  9, 10, 11]])\n",
      "E:  torch.Size([12, 4])\n",
      "S_i:  torch.Size([2, 1])\n",
      "X_0:  torch.Size([2, 1, 4])\n",
      "X_f:  torch.Size([2, 1, 4])\n",
      "Y:  torch.Size([2, 1, 12])\n",
      "Z:  torch.Size([2, 12])\n",
      "s_iplus1:  torch.Size([2]) tensor([1, 6])\n",
      "loss:  torch.Size([]) tensor(7.6477, grad_fn=<NllLossBackward0>)\n",
      "i=2\n",
      "S_i:  torch.Size([2, 2])\n",
      "X_0:  torch.Size([2, 2, 4])\n",
      "X_f:  torch.Size([2, 2, 4])\n",
      "Y:  torch.Size([2, 2, 12])\n",
      "Z:  torch.Size([2, 12])\n",
      "s_iplus1:  torch.Size([2]) tensor([2, 7])\n",
      "loss:  torch.Size([]) tensor(6.5355, grad_fn=<NllLossBackward0>)\n",
      "i=3\n",
      "S_i:  torch.Size([2, 3])\n",
      "X_0:  torch.Size([2, 3, 4])\n",
      "X_f:  torch.Size([2, 3, 4])\n",
      "Y:  torch.Size([2, 3, 12])\n",
      "Z:  torch.Size([2, 12])\n",
      "s_iplus1:  torch.Size([2]) tensor([3, 8])\n",
      "loss:  torch.Size([]) tensor(2.6618, grad_fn=<NllLossBackward0>)\n",
      "i=4\n",
      "S_i:  torch.Size([2, 4])\n",
      "X_0:  torch.Size([2, 4, 4])\n",
      "X_f:  torch.Size([2, 4, 4])\n",
      "Y:  torch.Size([2, 4, 12])\n",
      "Z:  torch.Size([2, 12])\n",
      "s_iplus1:  torch.Size([2]) tensor([0, 9])\n",
      "loss:  torch.Size([]) tensor(7.1514, grad_fn=<NllLossBackward0>)\n",
      "i=5\n",
      "S_i:  torch.Size([2, 5])\n",
      "X_0:  torch.Size([2, 5, 4])\n",
      "X_f:  torch.Size([2, 5, 4])\n",
      "Y:  torch.Size([2, 5, 12])\n",
      "Z:  torch.Size([2, 12])\n",
      "s_iplus1:  torch.Size([2]) tensor([ 4, 10])\n",
      "loss:  torch.Size([]) tensor(3.5018, grad_fn=<NllLossBackward0>)\n",
      "i=6\n",
      "S_i:  torch.Size([2, 6])\n",
      "X_0:  torch.Size([2, 6, 4])\n",
      "X_f:  torch.Size([2, 6, 4])\n",
      "Y:  torch.Size([2, 6, 12])\n",
      "Z:  torch.Size([2, 12])\n",
      "s_iplus1:  torch.Size([2]) tensor([11, 11])\n",
      "loss:  torch.Size([]) tensor(5.1636, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "verbose = False\n",
    "\n",
    "double_seq_list = [[' I', ' think', ' there', 'fore', ' I', ' am', '<endoftext>'],\n",
    "               ['Every', ' cloud', ' has', ' a', ' silver', ' lining', '<endoftext>']]\n",
    "\n",
    "for i in range(1,n):\n",
    "    print(f\"i={i}\")\n",
    "    \n",
    "    # initialize\n",
    "    if i == 1:\n",
    "        E, S_n, _ = initialize(double_seq_list, vocabulary_dict)\n",
    "        print(\"S_n: \", S_n.shape, S_n)\n",
    "        print(\"E: \", E.shape)\n",
    "        if verbose:\n",
    "            print(S_n)\n",
    "            print(E)\n",
    "    \n",
    "    # iterate S_i\n",
    "    S_i = S_n[:,0:i]\n",
    "    print(\"S_i: \", S_i.shape)\n",
    "    if verbose:\n",
    "        print(S_i)\n",
    "        \n",
    "    X_0 = F.embedding(S_i, E)\n",
    "    print(\"X_0: \", X_0.shape)\n",
    "    if verbose: print(X_0)\n",
    "\n",
    "    X_f = layers(X_0)\n",
    "    print(\"X_f: \", X_f.shape)\n",
    "    if verbose: print(X_f)\n",
    "\n",
    "    Y = torch.matmul(X_f,E.T)\n",
    "    print(\"Y: \", Y.shape)\n",
    "    if verbose: print(Y)\n",
    "\n",
    "    Z = Y[:,-1,:]\n",
    "    print(\"Z: \", Z.shape)\n",
    "    if verbose: print(Z)\n",
    "        \n",
    "    # correct indices to be trained on\n",
    "    s_iplus1 = S_n[:,i]\n",
    "    print(\"s_iplus1: \", s_iplus1.shape, s_iplus1)\n",
    "    \n",
    "    # cross-entropy loss\n",
    "    loss = F.cross_entropy(Z, s_iplus1)\n",
    "    print(\"loss: \", loss.shape, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7570901",
   "metadata": {},
   "source": [
    "### 4. my next-concept predictor functions\n",
    "\n",
    "we'll be re-using initialize(), layers(), and tensor_to_string() from earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9cb03d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_conditionals(P_max, verbose = False):\n",
    "    global seed, i, n, d, v, l, h, activation, norm, gamma_0\n",
    "    \n",
    "    Gamma_i = torch.ones(P_max.shape[0])*gamma_0\n",
    "    \n",
    "    A_i_bar = torch.zeros(P_max.shape[0])\n",
    "    A_i_bar[P_max <= gamma_0] = 1\n",
    "    if verbose: print(\"A_i_bar: \", A_i_bar.dtype, A_i_bar.shape, A_i_bar)\n",
    "    \n",
    "    not_A_i_bar = torch.zeros(P_max.shape[0])\n",
    "    not_A_i_bar[P_max > gamma_0] = 1\n",
    "    if verbose: print(\"not_A_i_bar: \", not_A_i_bar.dtype, not_A_i_bar.shape, not_A_i_bar)\n",
    "    \n",
    "    return Gamma_i, A_i_bar, not_A_i_bar\n",
    "\n",
    "def iter_conditionals(P_max, A_iminus1_bar, Gamma_iminus1, t_max, gamma_0, verbose = False):\n",
    "    \n",
    "    Delta_gamma = (1-gamma_0)/t_max\n",
    "    \n",
    "    D_i = A_iminus1_bar*Delta_gamma\n",
    "    if verbose: print(\"D_i: \", D_i.shape, D_i)\n",
    "    \n",
    "    Gamma_i = Gamma_iminus1 - D_i\n",
    "    if verbose: print(\"Gamma_i: \", Gamma_i.shape, Gamma_i)\n",
    "    \n",
    "    A_i_bar = torch.zeros(P_max.shape[0])\n",
    "    A_i_bar[P_max <= Gamma_i] = 1\n",
    "    if verbose: print(\"A_i_bar: \", A_i_bar.dtype, A_i_bar.shape, A_i_bar)\n",
    "    \n",
    "    not_A_i_bar = torch.zeros(P_max.shape[0])\n",
    "    not_A_i_bar[P_max > Gamma_i] = 1\n",
    "    if verbose: print(\"not_A_i_bar: \", not_A_i_bar.dtype, not_A_i_bar.shape, not_A_i_bar)\n",
    "    \n",
    "    return Gamma_i, A_i_bar, not_A_i_bar\n",
    "\n",
    "def build_concepts_tensor(Y, A_i_bar, C_i = None, verbose=False):\n",
    "    global seed, i, n, d, v, l, h, activation, norm, gamma_0\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Y: \", Y.shape)\n",
    "        print(\"A_i_bar: \", A_i_bar.shape, A_i_bar)\n",
    "    C_i_bar = Y*A_i_bar.unsqueeze(dim=1)\n",
    "    \n",
    "    if torch.is_tensor(C_i):\n",
    "        prepend = C_i\n",
    "    else:\n",
    "        prepend = torch.zeros((Y.shape[0],i,d))\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"prepend: \", prepend.shape)\n",
    "        print(\"C_i_bar: \", C_i_bar.shape, C_i_bar)\n",
    "    C_iplus1 = torch.cat((prepend, C_i_bar.unsqueeze(dim=1)), dim=1)\n",
    "    if verbose: print(\"C_iplus1: \", C_iplus1.shape, C_iplus1)\n",
    "    \n",
    "    return C_iplus1\n",
    "\n",
    "def inference(A_i_bar, not_A_i_bar, s_iplus1, S_i, verbose=False):\n",
    "    global seed, i, n, d, v, l, h, activation, norm, gamma_0\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"A_i_bar: \", A_i_bar.shape, A_i_bar)\n",
    "        print(\"not_A_i_bar: \", not_A_i_bar.shape, not_A_i_bar, not_A_i_bar.type(torch.bool))\n",
    "        print(\"s_iplus1: \", s_iplus1.shape, s_iplus1)\n",
    "        \n",
    "    A_i_bar_v = A_i_bar*v\n",
    "    if verbose: print(\"A_i_bar_v: \", A_i_bar_v.shape, A_i_bar_v)\n",
    "    \n",
    "    s_iplus1_bar = s_iplus1*not_A_i_bar\n",
    "    if verbose: print(\"s_iplus1_bar: \", s_iplus1_bar.shape, s_iplus1_bar)\n",
    "    \n",
    "    s_iplus1_bar_v = s_iplus1_bar + A_i_bar_v\n",
    "    if verbose: print(\"s_iplus1_bar_v: \", s_iplus1_bar_v.shape, s_iplus1_bar_v)\n",
    "    \n",
    "    S_iplus1 = torch.cat((S_i, s_iplus1_bar_v.unsqueeze(dim=1)), dim=1)\n",
    "    \n",
    "    return S_iplus1\n",
    "\n",
    "def custom_loss(logits, target, mask, verbose=False):\n",
    "    \n",
    "    # Apply LogSoftmax\n",
    "    log_probs = F.log_softmax(logits, dim=1)\n",
    "    if verbose: print(\"log_probs: \", log_probs.shape, log_probs)\n",
    "    \n",
    "    # Gather the log probabilities corresponding to the true classes\n",
    "    gathered_probs = log_probs.gather(1, target.unsqueeze(1)).squeeze(1)\n",
    "    if verbose: print(\"gathered_probs: \", gathered_probs.shape, gathered_probs)\n",
    "    \n",
    "    # Apply the mask\n",
    "    masked_probs = gathered_probs * mask\n",
    "    if verbose: print(\"masked_probs: \", masked_probs.shape, masked_probs)\n",
    "\n",
    "    # Compute the average loss over unmasked entries\n",
    "    # Use A.sum() to count the number of unmasked entries\n",
    "    loss = -torch.sum(masked_probs) / torch.clamp(mask.sum(), min=1)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15199f6b",
   "metadata": {},
   "source": [
    "### 5. NCP 3-step inference walkthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8411cf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# but first, all the new hyperparameters & whatnot\n",
    "gamma_0 = 0.5\n",
    "t_max=2\n",
    "\n",
    "vocabulary_dict_ext = vocabulary_dict\n",
    "vocabulary_dict_ext[''] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "003f7a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIRST PASS: i=1\n",
      "S_i:  torch.Size([2, 1])\n",
      "E:  torch.Size([12, 4])\n",
      "X_0:  torch.Size([2, 1, 4])\n",
      "X_f:  torch.Size([2, 1, 4])\n",
      "Y:  torch.Size([2, 4])\n",
      "Z:  torch.Size([2, 12])\n",
      "P:  torch.Size([2, 12]) tensor([1., 1.], grad_fn=<SumBackward1>)\n",
      "P_max:  torch.Size([2]) tensor([0.2365, 0.6543], grad_fn=<MaxBackward0>)\n",
      "s_iplus1:  torch.Size([2]) tensor([3, 7]) fore  has\n",
      "Gamma_i:  torch.Size([2]) tensor([0.5000, 0.5000])\n",
      "A_i_bar:  torch.Size([2]) tensor([1., 0.])\n",
      "not_A_i_bar:  torch.Size([2]) tensor([0., 1.])\n",
      "C_iplus1:  torch.Size([2, 2, 4]) tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 1.1422, -1.5985,  0.0936,  0.3627]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.0000,  0.0000, -0.0000,  0.0000]]], grad_fn=<CatBackward0>)\n",
      "S_iplus1:  torch.Size([2, 2]) tensor([[ 0., 12.],\n",
      "        [ 5.,  7.]])\n",
      "[' I ', 'Every  has']\n",
      "SECOND PASS: i=2\n",
      "E_prime:  torch.Size([13, 4])\n",
      "T:  torch.Size([2, 2, 4]) tensor([[[-1.3884, -0.5056,  0.8399,  1.0541],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.5299, -1.7035,  0.8403,  0.3334],\n",
      "         [-1.5883,  0.5076, -0.0234,  1.1040]]], grad_fn=<EmbeddingBackward0>)\n",
      "X_0:  torch.Size([2, 2, 4])\n",
      "X_f:  torch.Size([2, 2, 4])\n",
      "Y:  torch.Size([2, 4])\n",
      "Z:  torch.Size([2, 12])\n",
      "P:  torch.Size([2, 12]) tensor([1.0000, 1.0000], grad_fn=<SumBackward1>)\n",
      "P_max:  torch.Size([2]) tensor([0.3055, 0.2385], grad_fn=<MaxBackward0>)\n",
      "Gamma_i:  torch.Size([2]) tensor([0.2500, 0.5000])\n",
      "A_i_bar:  torch.Size([2]) tensor([0., 1.])\n",
      "not_A_i_bar:  torch.Size([2]) tensor([1., 0.])\n",
      "C_iplus1:  torch.Size([2, 3, 4]) tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 1.1422, -1.5985,  0.0936,  0.3627],\n",
      "         [ 0.0000, -0.0000,  0.0000, -0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.0000,  0.0000, -0.0000,  0.0000],\n",
      "         [ 0.0356, -1.6445,  0.8007,  0.8082]]], grad_fn=<CatBackward0>)\n",
      "S_iplus1:  torch.Size([2, 3]) tensor([[ 0., 12.,  3.],\n",
      "        [ 5.,  7., 12.]])\n",
      "[' I  fore', 'Every  has ']\n",
      "SECOND PASS: i=3\n",
      "E_prime:  torch.Size([13, 4])\n",
      "T:  torch.Size([2, 3, 4]) tensor([[[-1.3884, -0.5056,  0.8399,  1.0541],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 1.0190, -1.5035,  0.7825, -0.2980]],\n",
      "\n",
      "        [[ 0.5299, -1.7035,  0.8403,  0.3334],\n",
      "         [-1.5883,  0.5076, -0.0234,  1.1040],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000]]], grad_fn=<EmbeddingBackward0>)\n",
      "X_0:  torch.Size([2, 3, 4])\n",
      "X_f:  torch.Size([2, 3, 4])\n",
      "Y:  torch.Size([2, 4])\n",
      "Z:  torch.Size([2, 12])\n",
      "P:  torch.Size([2, 12]) tensor([1., 1.], grad_fn=<SumBackward1>)\n",
      "P_max:  torch.Size([2]) tensor([0.4459, 0.3020], grad_fn=<MaxBackward0>)\n",
      "Gamma_i:  torch.Size([2]) tensor([0.2500, 0.2500])\n",
      "A_i_bar:  torch.Size([2]) tensor([0., 0.])\n",
      "not_A_i_bar:  torch.Size([2]) tensor([1., 1.])\n",
      "C_iplus1:  torch.Size([2, 4, 4]) tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 1.1422, -1.5985,  0.0936,  0.3627],\n",
      "         [ 0.0000, -0.0000,  0.0000, -0.0000],\n",
      "         [-0.0000,  0.0000,  0.0000, -0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.0000,  0.0000, -0.0000,  0.0000],\n",
      "         [ 0.0356, -1.6445,  0.8007,  0.8082],\n",
      "         [-0.0000,  0.0000,  0.0000,  0.0000]]], grad_fn=<CatBackward0>)\n",
      "S_iplus1:  torch.Size([2, 4]) tensor([[ 0., 12.,  3.,  3.],\n",
      "        [ 5.,  7., 12.,  7.]])\n",
      "[' I  fore fore', 'Every  has   has']\n"
     ]
    }
   ],
   "source": [
    "### FIRST PASS\n",
    "verbose=False\n",
    "batch_seq = [[' I'], ['Every']]\n",
    "\n",
    "i=1\n",
    "print(f\"FIRST PASS: i={i}\")\n",
    "\n",
    "E, _, S_i = initialize(batch_seq, vocabulary_dict)\n",
    "print(\"S_i: \", S_i.shape)\n",
    "print(\"E: \", E.shape)\n",
    "if verbose:\n",
    "    print(S_i)\n",
    "    print(E)\n",
    "\n",
    "X_0 = F.embedding(S_i, E)\n",
    "print(\"X_0: \", X_0.shape)\n",
    "if verbose: print(X_0)\n",
    "\n",
    "X_f = layers(X_0)\n",
    "print(\"X_f: \", X_f.shape)\n",
    "if verbose: print(X_f)\n",
    "\n",
    "#Y = torch.matmul(X_f,E.T)\n",
    "Y = X_f[:,-1,:]\n",
    "print(\"Y: \", Y.shape)\n",
    "if verbose: print(Y)\n",
    "\n",
    "#Z = Y[:,-1,:]\n",
    "Z = torch.matmul(Y,E.T)\n",
    "print(\"Z: \", Z.shape)\n",
    "if verbose: print(Z)\n",
    "\n",
    "P = F.softmax(Z, dim=-1)\n",
    "print(\"P: \", P.shape, torch.sum(P,dim=1))\n",
    "if verbose: print(P)\n",
    "    \n",
    "P_max = torch.max(P, dim=1).values\n",
    "print(\"P_max: \", P_max.shape, P_max)\n",
    "\n",
    "s_iplus1 = torch.max(P, dim=1).indices\n",
    "print(\"s_iplus1: \", s_iplus1.shape, s_iplus1, tensor_to_string(s_iplus1, vocabulary_dict))\n",
    "if verbose: print(s_iplus1)\n",
    "    \n",
    "Gamma_i, A_i_bar, not_A_i_bar = initialize_conditionals(P_max)\n",
    "print(\"Gamma_i: \", Gamma_i.shape, Gamma_i)\n",
    "print(\"A_i_bar: \", A_i_bar.shape, A_i_bar)\n",
    "print(\"not_A_i_bar: \", not_A_i_bar.shape, not_A_i_bar)\n",
    "\n",
    "C_iplus1 = build_concepts_tensor(Y, A_i_bar)\n",
    "print(\"C_iplus1: \", C_iplus1.shape, C_iplus1)\n",
    "\n",
    "S_iplus1 = inference(A_i_bar, not_A_i_bar, s_iplus1, S_i)\n",
    "print(\"S_iplus1: \", S_iplus1.shape, S_iplus1)\n",
    "print(tensor_to_string(S_iplus1, vocabulary_dict_ext))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### SECOND PASS\n",
    "i=2\n",
    "print(f\"SECOND PASS: i={i}\")\n",
    "\n",
    "# using outputs from previous iter\n",
    "S_i, C_i = S_iplus1, C_iplus1\n",
    "A_iminus1_bar, Gamma_iminus1 = A_i_bar, Gamma_i\n",
    "\n",
    "# setting up our augmented E with an empty (zeros) vector for concept vectors\n",
    "E_prime = torch.cat((E, torch.zeros((1,d))), dim=0)\n",
    "print(\"E_prime: \", E_prime.shape)\n",
    "\n",
    "# all of our token embedding vectors\n",
    "T = F.embedding(S_i.type(torch.int32), E_prime)\n",
    "print(\"T: \", T.shape, T)\n",
    "\n",
    "# add correctly placed concept vectors and token embedding vectors to get our first resid\n",
    "X_0 = C_i + T\n",
    "print(\"X_0: \", X_0.shape)\n",
    "if verbose: print(X_0)\n",
    "\n",
    "X_f = layers(X_0)\n",
    "print(\"X_f: \", X_f.shape)\n",
    "if verbose: print(X_f)\n",
    "\n",
    "#Y = torch.matmul(X_f,E.T)\n",
    "Y = X_f[:,-1,:]\n",
    "print(\"Y: \", Y.shape)\n",
    "if verbose: print(Y)\n",
    "\n",
    "#Z = Y[:,-1,:]\n",
    "Z = torch.matmul(Y,E.T)\n",
    "print(\"Z: \", Z.shape)\n",
    "if verbose: print(Z)\n",
    "\n",
    "P = F.softmax(Z, dim=-1)\n",
    "print(\"P: \", P.shape, torch.sum(P,dim=1))\n",
    "if verbose: print(P)\n",
    "    \n",
    "P_max = torch.max(P, dim=1).values\n",
    "print(\"P_max: \", P_max.shape, P_max)\n",
    "\n",
    "Gamma_i, A_i_bar, not_A_i_bar = iter_conditionals(P_max, A_iminus1_bar, Gamma_iminus1, t_max, gamma_0)\n",
    "print(\"Gamma_i: \", Gamma_i.shape, Gamma_i)\n",
    "print(\"A_i_bar: \", A_i_bar.shape, A_i_bar)\n",
    "print(\"not_A_i_bar: \", not_A_i_bar.shape, not_A_i_bar)\n",
    "\n",
    "C_iplus1 = build_concepts_tensor(Y, A_i_bar, C_i)\n",
    "print(\"C_iplus1: \", C_iplus1.shape, C_iplus1)\n",
    "\n",
    "S_iplus1 = inference(A_i_bar, not_A_i_bar, s_iplus1, S_i)\n",
    "print(\"S_iplus1: \", S_iplus1.shape, S_iplus1)\n",
    "print(tensor_to_string(S_iplus1, vocabulary_dict_ext))\n",
    "\n",
    "\n",
    "\n",
    "### THIRD PASS\n",
    "i=3\n",
    "print(f\"SECOND PASS: i={i}\")\n",
    "\n",
    "# using outputs from previous iter\n",
    "S_i, C_i = S_iplus1, C_iplus1\n",
    "A_iminus1_bar, Gamma_iminus1 = A_i_bar, Gamma_i\n",
    "\n",
    "# setting up our augmented E with an empty (zeros) vector for concept vectors\n",
    "E_prime = torch.cat((E, torch.zeros((1,d))), dim=0)\n",
    "print(\"E_prime: \", E_prime.shape)\n",
    "\n",
    "# all of our token embedding vectors\n",
    "T = F.embedding(S_i.type(torch.int32), E_prime)\n",
    "print(\"T: \", T.shape, T)\n",
    "\n",
    "# add correctly placed concept vectors and token embedding vectors to get our first resid\n",
    "X_0 = C_i + T\n",
    "print(\"X_0: \", X_0.shape)\n",
    "if verbose: print(X_0)\n",
    "\n",
    "X_f = layers(X_0)\n",
    "print(\"X_f: \", X_f.shape)\n",
    "if verbose: print(X_f)\n",
    "\n",
    "#Y = torch.matmul(X_f,E.T)\n",
    "Y = X_f[:,-1,:]\n",
    "print(\"Y: \", Y.shape)\n",
    "if verbose: print(Y)\n",
    "\n",
    "#Z = Y[:,-1,:]\n",
    "Z = torch.matmul(Y,E.T)\n",
    "print(\"Z: \", Z.shape)\n",
    "if verbose: print(Z)\n",
    "\n",
    "P = F.softmax(Z, dim=-1)\n",
    "print(\"P: \", P.shape, torch.sum(P,dim=1))\n",
    "if verbose: print(P)\n",
    "    \n",
    "P_max = torch.max(P, dim=1).values\n",
    "print(\"P_max: \", P_max.shape, P_max)\n",
    "\n",
    "Gamma_i, A_i_bar, not_A_i_bar = iter_conditionals(P_max, A_iminus1_bar, Gamma_iminus1, t_max, gamma_0)\n",
    "print(\"Gamma_i: \", Gamma_i.shape, Gamma_i)\n",
    "print(\"A_i_bar: \", A_i_bar.shape, A_i_bar)\n",
    "print(\"not_A_i_bar: \", not_A_i_bar.shape, not_A_i_bar)\n",
    "\n",
    "C_iplus1 = build_concepts_tensor(Y, A_i_bar, C_i)\n",
    "print(\"C_iplus1: \", C_iplus1.shape, C_iplus1)\n",
    "\n",
    "S_iplus1 = inference(A_i_bar, not_A_i_bar, s_iplus1, S_i)\n",
    "print(\"S_iplus1: \", S_iplus1.shape, S_iplus1)\n",
    "print(tensor_to_string(S_iplus1, vocabulary_dict_ext))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f833c2",
   "metadata": {},
   "source": [
    "### 6. NCP training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab85b458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_max:  2.0\n"
     ]
    }
   ],
   "source": [
    "# since we no longer have a pre-set sequence length n, we now need\n",
    "# to determine a maximum number of tokens t_max to add to prevent memory from overflowing\n",
    "import numpy as np\n",
    "\n",
    "gamma_0 = 0.5\n",
    "C_max = 2\n",
    "double_seq_list = [[' I', ' think', ' there', 'fore', ' I', ' am', '<endoftext>'],\n",
    "               ['Every', ' cloud', ' has', ' a', ' silver', ' lining', '<endoftext>']]\n",
    "n = len(double_seq_list[0])\n",
    "b = len(double_seq_list)\n",
    "\n",
    "t_max = np.floor(n*(np.sqrt(C_max)-1))\n",
    "print(\"t_max: \", t_max)\n",
    "\n",
    "vocabulary_dict_ext = vocabulary_dict\n",
    "vocabulary_dict_ext[''] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "02481c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** ITERATION i=1 *************\n",
      "S_n:  torch.Size([2, 7]) tensor([[ 0,  1,  2,  3,  0,  4, 11],\n",
      "        [ 5,  6,  7,  8,  9, 10, 11]])\n",
      "E:  torch.Size([12, 4])\n",
      "X_0:  torch.Size([2, 1, 4])\n",
      "X_f:  torch.Size([2, 1, 4])\n",
      "Y:  torch.Size([2, 4])\n",
      "Z:  torch.Size([2, 12])\n",
      "P:  torch.Size([2, 12]) tensor([1.0000, 1.0000], grad_fn=<SumBackward1>)\n",
      "P_max:  torch.Size([2]) tensor([0.4952, 0.6295], grad_fn=<MaxBackward0>)\n",
      "Gamma_i:  torch.Size([2]) tensor([0.5000, 0.5000])\n",
      "A_i_bar:  torch.Size([2]) tensor([1., 0.])\n",
      "not_A_i_bar:  torch.Size([2]) tensor([0., 1.])\n",
      "C_iplus1:  torch.Size([2, 2, 4]) tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.0543, -0.2146,  1.5335, -1.2647]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.0000,  0.0000,  0.0000, -0.0000]]], grad_fn=<CatBackward0>)\n",
      "s_iplus1:  torch.Size([2]) tensor([1, 6])\n",
      "loss:  torch.Size([]) tensor(6.3587, grad_fn=<DivBackward0>)\n",
      "S_iplus1:  torch.Size([2, 2]) tensor([[ 0., 12.],\n",
      "        [ 5.,  6.]])\n",
      "[' I ', 'Every  cloud']\n",
      "not_A_iplus1:  torch.Size([2, 2]) tensor([[1., 0.],\n",
      "        [1., 1.]])\n",
      "********** ITERATION i=2 *************\n",
      "E_prime:  torch.Size([13, 4])\n",
      "T:  torch.Size([2, 2, 4]) tensor([[[ 1.1752,  0.7514, -1.2783, -0.6482],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.8534,  0.0238,  0.7626, -1.6399],\n",
      "         [ 0.0375,  1.0049, -1.6269,  0.5846]]], grad_fn=<EmbeddingBackward0>)\n",
      "X_0:  torch.Size([2, 2, 4])\n",
      "X_f:  torch.Size([2, 2, 4])\n",
      "Y:  torch.Size([2, 4])\n",
      "Z:  torch.Size([2, 12])\n",
      "P:  torch.Size([2, 12]) tensor([1.0000, 1.0000], grad_fn=<SumBackward1>)\n",
      "P_max:  torch.Size([2]) tensor([0.5541, 0.8636], grad_fn=<MaxBackward0>)\n",
      "Gamma_i:  torch.Size([2]) tensor([0.2500, 0.5000])\n",
      "A_i_bar:  torch.Size([2]) tensor([0., 0.])\n",
      "not_A_i_bar:  torch.Size([2]) tensor([1., 1.])\n",
      "C_iplus1:  torch.Size([2, 3, 4]) tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.0543, -0.2146,  1.5335, -1.2647],\n",
      "         [-0.0000, -0.0000, -0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.0000,  0.0000,  0.0000, -0.0000],\n",
      "         [-0.0000, -0.0000, -0.0000,  0.0000]]], grad_fn=<CatBackward0>)\n",
      "not_A_i:  torch.Size([2, 2]) tensor([[1., 0.],\n",
      "        [1., 1.]])\n",
      "not_A_i_hat:  torch.Size([2]) tensor([1., 2.])\n",
      "s_iplus1:  torch.Size([2]) tensor([1, 7])\n",
      "loss:  torch.Size([]) tensor(4.5536, grad_fn=<DivBackward0>)\n",
      "S_iplus1:  torch.Size([2, 3]) tensor([[ 0., 12.,  1.],\n",
      "        [ 5.,  6.,  7.]])\n",
      "[' I   think', 'Every  cloud  has']\n",
      "not_A_iplus1:  torch.Size([2, 3]) tensor([[1., 0., 1.],\n",
      "        [1., 1., 1.]])\n",
      "********** ITERATION i=3 *************\n",
      "E_prime:  torch.Size([13, 4])\n",
      "T:  torch.Size([2, 3, 4]) tensor([[[ 1.1752,  0.7514, -1.2783, -0.6482],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.4043, -0.2201,  1.3867,  0.2376]],\n",
      "\n",
      "        [[ 0.8534,  0.0238,  0.7626, -1.6399],\n",
      "         [ 0.0375,  1.0049, -1.6269,  0.5846],\n",
      "         [ 1.5116,  0.2071, -1.1719, -0.5468]]], grad_fn=<EmbeddingBackward0>)\n",
      "X_0:  torch.Size([2, 3, 4])\n",
      "X_f:  torch.Size([2, 3, 4])\n",
      "Y:  torch.Size([2, 4])\n",
      "Z:  torch.Size([2, 12])\n",
      "P:  torch.Size([2, 12]) tensor([1., 1.], grad_fn=<SumBackward1>)\n",
      "P_max:  torch.Size([2]) tensor([0.2670, 0.4795], grad_fn=<MaxBackward0>)\n",
      "Gamma_i:  torch.Size([2]) tensor([0.2500, 0.5000])\n",
      "A_i_bar:  torch.Size([2]) tensor([0., 1.])\n",
      "not_A_i_bar:  torch.Size([2]) tensor([1., 0.])\n",
      "C_iplus1:  torch.Size([2, 4, 4]) tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.0543, -0.2146,  1.5335, -1.2647],\n",
      "         [-0.0000, -0.0000, -0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.0000,  0.0000,  0.0000, -0.0000],\n",
      "         [-0.0000, -0.0000, -0.0000,  0.0000],\n",
      "         [ 0.3585, -1.7166,  0.7137,  0.6445]]], grad_fn=<CatBackward0>)\n",
      "not_A_i:  torch.Size([2, 3]) tensor([[1., 0., 1.],\n",
      "        [1., 1., 1.]])\n",
      "not_A_i_hat:  torch.Size([2]) tensor([2., 3.])\n",
      "s_iplus1:  torch.Size([2]) tensor([2, 8])\n",
      "loss:  torch.Size([]) tensor(4.1350, grad_fn=<DivBackward0>)\n",
      "S_iplus1:  torch.Size([2, 4]) tensor([[ 0., 12.,  1.,  2.],\n",
      "        [ 5.,  6.,  7., 12.]])\n",
      "[' I   think  there', 'Every  cloud  has ']\n",
      "not_A_iplus1:  torch.Size([2, 4]) tensor([[1., 0., 1., 1.],\n",
      "        [1., 1., 1., 0.]])\n",
      "********** ITERATION i=4 *************\n",
      "E_prime:  torch.Size([13, 4])\n",
      "T:  torch.Size([2, 4, 4]) tensor([[[ 1.1752,  0.7514, -1.2783, -0.6482],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.4043, -0.2201,  1.3867,  0.2376],\n",
      "         [ 1.6376, -0.8880, -0.0221, -0.7275]],\n",
      "\n",
      "        [[ 0.8534,  0.0238,  0.7626, -1.6399],\n",
      "         [ 0.0375,  1.0049, -1.6269,  0.5846],\n",
      "         [ 1.5116,  0.2071, -1.1719, -0.5468],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000]]], grad_fn=<EmbeddingBackward0>)\n",
      "X_0:  torch.Size([2, 4, 4])\n",
      "X_f:  torch.Size([2, 4, 4])\n",
      "Y:  torch.Size([2, 4])\n",
      "Z:  torch.Size([2, 12])\n",
      "P:  torch.Size([2, 12]) tensor([1.0000, 1.0000], grad_fn=<SumBackward1>)\n",
      "P_max:  torch.Size([2]) tensor([0.3503, 0.4338], grad_fn=<MaxBackward0>)\n",
      "Gamma_i:  torch.Size([2]) tensor([0.2500, 0.2500])\n",
      "A_i_bar:  torch.Size([2]) tensor([0., 0.])\n",
      "not_A_i_bar:  torch.Size([2]) tensor([1., 1.])\n",
      "C_iplus1:  torch.Size([2, 5, 4]) tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.0543, -0.2146,  1.5335, -1.2647],\n",
      "         [-0.0000, -0.0000, -0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000, -0.0000, -0.0000],\n",
      "         [ 0.0000, -0.0000, -0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.0000,  0.0000,  0.0000, -0.0000],\n",
      "         [-0.0000, -0.0000, -0.0000,  0.0000],\n",
      "         [ 0.3585, -1.7166,  0.7137,  0.6445],\n",
      "         [-0.0000,  0.0000,  0.0000, -0.0000]]], grad_fn=<CatBackward0>)\n",
      "not_A_i:  torch.Size([2, 4]) tensor([[1., 0., 1., 1.],\n",
      "        [1., 1., 1., 0.]])\n",
      "not_A_i_hat:  torch.Size([2]) tensor([3., 3.])\n",
      "s_iplus1:  torch.Size([2]) tensor([3, 8])\n",
      "loss:  torch.Size([]) tensor(3.8753, grad_fn=<DivBackward0>)\n",
      "S_iplus1:  torch.Size([2, 5]) tensor([[ 0., 12.,  1.,  2.,  3.],\n",
      "        [ 5.,  6.,  7., 12.,  8.]])\n",
      "[' I   think  there fore', 'Every  cloud  has   a']\n",
      "not_A_iplus1:  torch.Size([2, 5]) tensor([[1., 0., 1., 1., 1.],\n",
      "        [1., 1., 1., 0., 1.]])\n",
      "********** ITERATION i=5 *************\n",
      "E_prime:  torch.Size([13, 4])\n",
      "T:  torch.Size([2, 5, 4]) tensor([[[ 1.1752,  0.7514, -1.2783, -0.6482],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.4043, -0.2201,  1.3867,  0.2376],\n",
      "         [ 1.6376, -0.8880, -0.0221, -0.7275],\n",
      "         [-0.5355,  1.5020, -1.1868,  0.2203]],\n",
      "\n",
      "        [[ 0.8534,  0.0238,  0.7626, -1.6399],\n",
      "         [ 0.0375,  1.0049, -1.6269,  0.5846],\n",
      "         [ 1.5116,  0.2071, -1.1719, -0.5468],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3103,  1.4521, -1.2292, -0.5332]]], grad_fn=<EmbeddingBackward0>)\n",
      "X_0:  torch.Size([2, 5, 4])\n",
      "X_f:  torch.Size([2, 5, 4])\n",
      "Y:  torch.Size([2, 4])\n",
      "Z:  torch.Size([2, 12])\n",
      "P:  torch.Size([2, 12]) tensor([1.0000, 1.0000], grad_fn=<SumBackward1>)\n",
      "P_max:  torch.Size([2]) tensor([0.3181, 0.3116], grad_fn=<MaxBackward0>)\n",
      "Gamma_i:  torch.Size([2]) tensor([0.2500, 0.2500])\n",
      "A_i_bar:  torch.Size([2]) tensor([0., 0.])\n",
      "not_A_i_bar:  torch.Size([2]) tensor([1., 1.])\n",
      "C_iplus1:  torch.Size([2, 6, 4]) tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.0543, -0.2146,  1.5335, -1.2647],\n",
      "         [-0.0000, -0.0000, -0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000, -0.0000, -0.0000],\n",
      "         [ 0.0000, -0.0000, -0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000, -0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.0000,  0.0000,  0.0000, -0.0000],\n",
      "         [-0.0000, -0.0000, -0.0000,  0.0000],\n",
      "         [ 0.3585, -1.7166,  0.7137,  0.6445],\n",
      "         [-0.0000,  0.0000,  0.0000, -0.0000],\n",
      "         [ 0.0000,  0.0000, -0.0000,  0.0000]]], grad_fn=<CatBackward0>)\n",
      "not_A_i:  torch.Size([2, 5]) tensor([[1., 0., 1., 1., 1.],\n",
      "        [1., 1., 1., 0., 1.]])\n",
      "not_A_i_hat:  torch.Size([2]) tensor([4., 4.])\n",
      "s_iplus1:  torch.Size([2]) tensor([0, 9])\n",
      "loss:  torch.Size([]) tensor(5.0960, grad_fn=<DivBackward0>)\n",
      "S_iplus1:  torch.Size([2, 6]) tensor([[ 0., 12.,  1.,  2.,  3.,  0.],\n",
      "        [ 5.,  6.,  7., 12.,  8.,  9.]])\n",
      "[' I   think  there fore  I', 'Every  cloud  has   a  silver']\n",
      "not_A_iplus1:  torch.Size([2, 6]) tensor([[1., 0., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 1.]])\n",
      "********** ITERATION i=6 *************\n",
      "E_prime:  torch.Size([13, 4])\n",
      "T:  torch.Size([2, 6, 4]) tensor([[[ 1.1752,  0.7514, -1.2783, -0.6482],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.4043, -0.2201,  1.3867,  0.2376],\n",
      "         [ 1.6376, -0.8880, -0.0221, -0.7275],\n",
      "         [-0.5355,  1.5020, -1.1868,  0.2203],\n",
      "         [ 1.1752,  0.7514, -1.2783, -0.6482]],\n",
      "\n",
      "        [[ 0.8534,  0.0238,  0.7626, -1.6399],\n",
      "         [ 0.0375,  1.0049, -1.6269,  0.5846],\n",
      "         [ 1.5116,  0.2071, -1.1719, -0.5468],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3103,  1.4521, -1.2292, -0.5332],\n",
      "         [-0.8140, -0.0035,  1.6342, -0.8166]]], grad_fn=<EmbeddingBackward0>)\n",
      "X_0:  torch.Size([2, 6, 4])\n",
      "X_f:  torch.Size([2, 6, 4])\n",
      "Y:  torch.Size([2, 4])\n",
      "Z:  torch.Size([2, 12])\n",
      "P:  torch.Size([2, 12]) tensor([1., 1.], grad_fn=<SumBackward1>)\n",
      "P_max:  torch.Size([2]) tensor([0.3452, 0.3755], grad_fn=<MaxBackward0>)\n",
      "Gamma_i:  torch.Size([2]) tensor([0.2500, 0.2500])\n",
      "A_i_bar:  torch.Size([2]) tensor([0., 0.])\n",
      "not_A_i_bar:  torch.Size([2]) tensor([1., 1.])\n",
      "C_iplus1:  torch.Size([2, 7, 4]) tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.0543, -0.2146,  1.5335, -1.2647],\n",
      "         [-0.0000, -0.0000, -0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000, -0.0000, -0.0000],\n",
      "         [ 0.0000, -0.0000, -0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000, -0.0000,  0.0000],\n",
      "         [ 0.0000, -0.0000, -0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.0000,  0.0000,  0.0000, -0.0000],\n",
      "         [-0.0000, -0.0000, -0.0000,  0.0000],\n",
      "         [ 0.3585, -1.7166,  0.7137,  0.6445],\n",
      "         [-0.0000,  0.0000,  0.0000, -0.0000],\n",
      "         [ 0.0000,  0.0000, -0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000, -0.0000,  0.0000]]], grad_fn=<CatBackward0>)\n",
      "not_A_i:  torch.Size([2, 6]) tensor([[1., 0., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 1.]])\n",
      "not_A_i_hat:  torch.Size([2]) tensor([5., 5.])\n",
      "s_iplus1:  torch.Size([2]) tensor([ 4, 10])\n",
      "loss:  torch.Size([]) tensor(3.0981, grad_fn=<DivBackward0>)\n",
      "S_iplus1:  torch.Size([2, 7]) tensor([[ 0., 12.,  1.,  2.,  3.,  0.,  4.],\n",
      "        [ 5.,  6.,  7., 12.,  8.,  9., 10.]])\n",
      "[' I   think  there fore  I  am', 'Every  cloud  has   a  silver  lining']\n",
      "not_A_iplus1:  torch.Size([2, 7]) tensor([[1., 0., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 1., 1.]])\n",
      "********** ITERATION i=7 *************\n",
      "E_prime:  torch.Size([13, 4])\n",
      "T:  torch.Size([2, 7, 4]) tensor([[[ 1.1752,  0.7514, -1.2783, -0.6482],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.4043, -0.2201,  1.3867,  0.2376],\n",
      "         [ 1.6376, -0.8880, -0.0221, -0.7275],\n",
      "         [-0.5355,  1.5020, -1.1868,  0.2203],\n",
      "         [ 1.1752,  0.7514, -1.2783, -0.6482],\n",
      "         [ 1.7089, -0.7020, -0.7032, -0.3037]],\n",
      "\n",
      "        [[ 0.8534,  0.0238,  0.7626, -1.6399],\n",
      "         [ 0.0375,  1.0049, -1.6269,  0.5846],\n",
      "         [ 1.5116,  0.2071, -1.1719, -0.5468],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3103,  1.4521, -1.2292, -0.5332],\n",
      "         [-0.8140, -0.0035,  1.6342, -0.8166],\n",
      "         [-0.4845, -1.0348, -0.1179,  1.6372]]], grad_fn=<EmbeddingBackward0>)\n",
      "X_0:  torch.Size([2, 7, 4])\n",
      "X_f:  torch.Size([2, 7, 4])\n",
      "Y:  torch.Size([2, 4])\n",
      "Z:  torch.Size([2, 12])\n",
      "P:  torch.Size([2, 12]) tensor([1.0000, 1.0000], grad_fn=<SumBackward1>)\n",
      "P_max:  torch.Size([2]) tensor([0.6309, 0.8763], grad_fn=<MaxBackward0>)\n",
      "Gamma_i:  torch.Size([2]) tensor([0.2500, 0.2500])\n",
      "A_i_bar:  torch.Size([2]) tensor([0., 0.])\n",
      "not_A_i_bar:  torch.Size([2]) tensor([1., 1.])\n",
      "C_iplus1:  torch.Size([2, 8, 4]) tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.0543, -0.2146,  1.5335, -1.2647],\n",
      "         [-0.0000, -0.0000, -0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000, -0.0000, -0.0000],\n",
      "         [ 0.0000, -0.0000, -0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000, -0.0000,  0.0000],\n",
      "         [ 0.0000, -0.0000, -0.0000,  0.0000],\n",
      "         [-0.0000,  0.0000,  0.0000, -0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.0000,  0.0000,  0.0000, -0.0000],\n",
      "         [-0.0000, -0.0000, -0.0000,  0.0000],\n",
      "         [ 0.3585, -1.7166,  0.7137,  0.6445],\n",
      "         [-0.0000,  0.0000,  0.0000, -0.0000],\n",
      "         [ 0.0000,  0.0000, -0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000, -0.0000,  0.0000],\n",
      "         [-0.0000, -0.0000, -0.0000,  0.0000]]], grad_fn=<CatBackward0>)\n",
      "not_A_i:  torch.Size([2, 7]) tensor([[1., 0., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 1., 1.]])\n",
      "not_A_i_hat:  torch.Size([2]) tensor([6., 6.])\n",
      "s_iplus1:  torch.Size([2]) tensor([11, 11])\n",
      "loss:  torch.Size([]) tensor(5.6567, grad_fn=<DivBackward0>)\n",
      "S_iplus1:  torch.Size([2, 8]) tensor([[ 0., 12.,  1.,  2.,  3.,  0.,  4., 11.],\n",
      "        [ 5.,  6.,  7., 12.,  8.,  9., 10., 11.]])\n",
      "[' I   think  there fore  I  am <endoftext>', 'Every  cloud  has   a  silver  lining <endoftext>']\n",
      "not_A_iplus1:  torch.Size([2, 8]) tensor([[1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 1., 1., 1.]])\n",
      "********** ITERATION i=8 *************\n",
      "E_prime:  torch.Size([13, 4])\n",
      "T:  torch.Size([2, 8, 4]) tensor([[[ 1.1752,  0.7514, -1.2783, -0.6482],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.4043, -0.2201,  1.3867,  0.2376],\n",
      "         [ 1.6376, -0.8880, -0.0221, -0.7275],\n",
      "         [-0.5355,  1.5020, -1.1868,  0.2203],\n",
      "         [ 1.1752,  0.7514, -1.2783, -0.6482],\n",
      "         [ 1.7089, -0.7020, -0.7032, -0.3037],\n",
      "         [ 1.5916,  0.0875, -0.6827, -0.9964]],\n",
      "\n",
      "        [[ 0.8534,  0.0238,  0.7626, -1.6399],\n",
      "         [ 0.0375,  1.0049, -1.6269,  0.5846],\n",
      "         [ 1.5116,  0.2071, -1.1719, -0.5468],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3103,  1.4521, -1.2292, -0.5332],\n",
      "         [-0.8140, -0.0035,  1.6342, -0.8166],\n",
      "         [-0.4845, -1.0348, -0.1179,  1.6372],\n",
      "         [ 1.5916,  0.0875, -0.6827, -0.9964]]], grad_fn=<EmbeddingBackward0>)\n",
      "X_0:  torch.Size([2, 8, 4])\n",
      "X_f:  torch.Size([2, 8, 4])\n",
      "Y:  torch.Size([2, 4])\n",
      "Z:  torch.Size([2, 12])\n",
      "P:  torch.Size([2, 12]) tensor([1.0000, 1.0000], grad_fn=<SumBackward1>)\n",
      "P_max:  torch.Size([2]) tensor([0.3437, 0.3437], grad_fn=<MaxBackward0>)\n",
      "Gamma_i:  torch.Size([2]) tensor([0.2500, 0.2500])\n",
      "A_i_bar:  torch.Size([2]) tensor([0., 0.])\n",
      "not_A_i_bar:  torch.Size([2]) tensor([1., 1.])\n",
      "C_iplus1:  torch.Size([2, 9, 4]) tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.0543, -0.2146,  1.5335, -1.2647],\n",
      "         [-0.0000, -0.0000, -0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000, -0.0000, -0.0000],\n",
      "         [ 0.0000, -0.0000, -0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000, -0.0000,  0.0000],\n",
      "         [ 0.0000, -0.0000, -0.0000,  0.0000],\n",
      "         [-0.0000,  0.0000,  0.0000, -0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000, -0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.0000,  0.0000,  0.0000, -0.0000],\n",
      "         [-0.0000, -0.0000, -0.0000,  0.0000],\n",
      "         [ 0.3585, -1.7166,  0.7137,  0.6445],\n",
      "         [-0.0000,  0.0000,  0.0000, -0.0000],\n",
      "         [ 0.0000,  0.0000, -0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000, -0.0000,  0.0000],\n",
      "         [-0.0000, -0.0000, -0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000, -0.0000]]], grad_fn=<CatBackward0>)\n",
      "not_A_i:  torch.Size([2, 8]) tensor([[1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 1., 1., 1.]])\n",
      "not_A_i_hat:  torch.Size([2]) tensor([7., 7.])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 7 is out of bounds for dimension 1 with size 7",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 79\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mS_n.size(0): \u001b[39m\u001b[38;5;124m\"\u001b[39m, S_n\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.arange: \u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39marange(S_n\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)))\n\u001b[0;32m---> 79\u001b[0m     s_iplus1 \u001b[38;5;241m=\u001b[39m S_n[torch\u001b[38;5;241m.\u001b[39marange(S_n\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)),not_A_i_hat\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mint32)] \u001b[38;5;66;03m#:\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms_iplus1: \u001b[39m\u001b[38;5;124m\"\u001b[39m, s_iplus1\u001b[38;5;241m.\u001b[39mshape, s_iplus1)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# cross-entropy loss\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 7 is out of bounds for dimension 1 with size 7"
     ]
    }
   ],
   "source": [
    "verbose = False\n",
    "\n",
    "for i in range(1,n+int(t_max)):\n",
    "    print(f\"********** ITERATION i={i} *************\")\n",
    "    \n",
    "    # initialize\n",
    "    if i == 1:\n",
    "        E, S_n, S_i = initialize(double_seq_list, vocabulary_dict)\n",
    "        print(\"S_n: \", S_n.shape, S_n)\n",
    "        print(\"E: \", E.shape)\n",
    "        if verbose:\n",
    "            print(S_n)\n",
    "            print(E)\n",
    "            \n",
    "        X_0 = F.embedding(S_i, E)\n",
    "        print(\"X_0: \", X_0.shape)\n",
    "        if verbose: print(X_0)\n",
    "    else:\n",
    "        S_i, C_i, not_A_i = S_iplus1, C_iplus1, not_A_iplus1\n",
    "        A_iminus1_bar, Gamma_iminus1 = A_i_bar, Gamma_i\n",
    "        \n",
    "        # setting up our augmented E with an empty (zeros) vector for concept vectors\n",
    "        E_prime = torch.cat((E, torch.zeros((1,d))), dim=0)\n",
    "        print(\"E_prime: \", E_prime.shape)\n",
    "\n",
    "        # all of our token embedding vectors\n",
    "        T = F.embedding(S_i.type(torch.int32), E_prime)\n",
    "        print(\"T: \", T.shape, T)\n",
    "\n",
    "        # add correctly placed concept vectors and token embedding vectors to get our first resid\n",
    "        X_0 = C_i + T\n",
    "        print(\"X_0: \", X_0.shape)\n",
    "        if verbose: print(X_0)\n",
    "    \n",
    "\n",
    "    X_f = layers(X_0)\n",
    "    print(\"X_f: \", X_f.shape)\n",
    "    if verbose: print(X_f)\n",
    "\n",
    "    #Y = torch.matmul(X_f,E.T) #<-- regular GPT\n",
    "    Y = X_f[:,-1,:]\n",
    "    print(\"Y: \", Y.shape)\n",
    "    if verbose: print(Y)\n",
    "\n",
    "    #Z = Y[:,-1,:] #<-- regular GPT\n",
    "    Z = torch.matmul(Y,E.T)\n",
    "    print(\"Z: \", Z.shape)\n",
    "    if verbose: print(Z)\n",
    "    \n",
    "    P = F.softmax(Z, dim=-1)\n",
    "    print(\"P: \", P.shape, torch.sum(P,dim=1))\n",
    "    if verbose: print(P)\n",
    "\n",
    "    P_max = torch.max(P, dim=1).values\n",
    "    print(\"P_max: \", P_max.shape, P_max)\n",
    "    \n",
    "    if i == 1:\n",
    "        Gamma_i, A_i_bar, not_A_i_bar = initialize_conditionals(P_max)\n",
    "        C_iplus1 = build_concepts_tensor(Y, A_i_bar)\n",
    "    else:\n",
    "        Gamma_i, A_i_bar, not_A_i_bar = iter_conditionals(P_max, A_iminus1_bar, Gamma_iminus1, t_max, gamma_0)\n",
    "        C_iplus1 = build_concepts_tensor(Y, A_i_bar, C_i)\n",
    "    print(\"Gamma_i: \", Gamma_i.shape, Gamma_i)\n",
    "    print(\"A_i_bar: \", A_i_bar.shape, A_i_bar)\n",
    "    print(\"not_A_i_bar: \", not_A_i_bar.shape, not_A_i_bar)\n",
    "    print(\"C_iplus1: \", C_iplus1.shape, C_iplus1)\n",
    "    \n",
    "    # correct indices to be trained on\n",
    "    if i == 1:\n",
    "        s_iplus1 = S_n[:,i]\n",
    "    else:\n",
    "        print(\"not_A_i: \", not_A_i.shape, not_A_i)\n",
    "        not_A_i_hat = not_A_i.sum(dim=1)\n",
    "        print(\"not_A_i_hat: \", not_A_i_hat.shape, not_A_i_hat)\n",
    "        if verbose:\n",
    "            print(\"S_n: \", S_n.shape, S_n)\n",
    "            print(\"S_n.size(0): \", S_n.size(0))\n",
    "            print(\"torch.arange: \", torch.arange(S_n.size(0)))\n",
    "        s_iplus1 = S_n[torch.arange(S_n.size(0)),not_A_i_hat.type(torch.int32)] #:\n",
    "    print(\"s_iplus1: \", s_iplus1.shape, s_iplus1)\n",
    "    \n",
    "    # cross-entropy loss\n",
    "    loss = custom_loss(Z, s_iplus1, not_A_i_bar)\n",
    "    print(\"loss: \", loss.shape, loss)\n",
    "    \n",
    "    # so i guess i should've named this function something other than inference but whatever\n",
    "    S_iplus1 = inference(A_i_bar, not_A_i_bar, s_iplus1, S_i)\n",
    "    print(\"S_iplus1: \", S_iplus1.shape, S_iplus1)\n",
    "    print(tensor_to_string(S_iplus1, vocabulary_dict_ext))\n",
    "    \n",
    "    if i == 1:\n",
    "        not_A_iplus1 = torch.cat((torch.ones((b,i)), not_A_i_bar.unsqueeze(dim=1)), dim=1)\n",
    "    else:\n",
    "        not_A_iplus1 = torch.cat((not_A_i, not_A_i_bar.unsqueeze(dim=1)), dim=1)\n",
    "    print(\"not_A_iplus1: \", not_A_iplus1.shape, not_A_iplus1)\n",
    "    \n",
    "### really i should add some logic to make it stop once each predicts <endoftext>\n",
    "### and prevent sequences who've already predicted <endoftext> from keeping training\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe9a678",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
