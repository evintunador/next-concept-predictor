{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "780020eb-32cc-4cf5-bd24-9d824f936aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99d14d41-c829-4bdd-8c6f-50dcf075b967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5891, -0.2638],\n",
       "        [ 0.1915, -1.1254],\n",
       "        [-0.3462, -0.9490],\n",
       "        [-0.2087,  0.7255]], requires_grad=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = 4\n",
    "d = 2\n",
    "E = torch.randn(v,d, requires_grad=True)\n",
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb088b1d-a2c4-4583-8bd7-712d87d68c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triangular(n):\n",
    "    return n*(n+1)//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2283959f-a531-49da-b712-eb25b98bed8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "v2 = triangular(v)\n",
    "print(v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9aacb34c-1cd4-4528-a14e-a3e491ace685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E2 = torch.zeros((int(v2), d), requires_grad=False)\n",
    "E2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a991e655-e5e1-4665-a586-3e7b649cc1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0\n",
      "j: 0\n",
      "vec: tensor([-1.5891, -0.2638], grad_fn=<UnbindBackward0>)\n",
      "expanded vec: torch.Size([4, 2])\n",
      "tensor([[-1.5891, -0.2638],\n",
      "        [-1.5891, -0.2638],\n",
      "        [-1.5891, -0.2638],\n",
      "        [-1.5891, -0.2638]], grad_fn=<ExpandBackward0>)\n",
      "spliced 2nd matrix: torch.Size([4, 2])\n",
      "tensor([[-1.5891, -0.2638],\n",
      "        [ 0.1915, -1.1254],\n",
      "        [-0.3462, -0.9490],\n",
      "        [-0.2087,  0.7255]], grad_fn=<SliceBackward0>)\n",
      "[0:4,:]\n",
      "E2: torch.Size([10, 2])\n",
      "tensor([[-3.1781, -0.5276],\n",
      "        [-1.3976, -1.3892],\n",
      "        [-1.9353, -1.2127],\n",
      "        [-1.7978,  0.4617],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000]], grad_fn=<CopySlices>)\n",
      "i: 1\n",
      "j: 4\n",
      "vec: tensor([ 0.1915, -1.1254], grad_fn=<UnbindBackward0>)\n",
      "expanded vec: torch.Size([3, 2])\n",
      "tensor([[ 0.1915, -1.1254],\n",
      "        [ 0.1915, -1.1254],\n",
      "        [ 0.1915, -1.1254]], grad_fn=<ExpandBackward0>)\n",
      "spliced 2nd matrix: torch.Size([3, 2])\n",
      "tensor([[ 0.1915, -1.1254],\n",
      "        [-0.3462, -0.9490],\n",
      "        [-0.2087,  0.7255]], grad_fn=<SliceBackward0>)\n",
      "[4:7,:]\n",
      "E2: torch.Size([10, 2])\n",
      "tensor([[-3.1781, -0.5276],\n",
      "        [-1.3976, -1.3892],\n",
      "        [-1.9353, -1.2127],\n",
      "        [-1.7978,  0.4617],\n",
      "        [ 0.3829, -2.2507],\n",
      "        [-0.1547, -2.0743],\n",
      "        [-0.0172, -0.3999],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000]], grad_fn=<CopySlices>)\n",
      "i: 2\n",
      "j: 7\n",
      "vec: tensor([-0.3462, -0.9490], grad_fn=<UnbindBackward0>)\n",
      "expanded vec: torch.Size([2, 2])\n",
      "tensor([[-0.3462, -0.9490],\n",
      "        [-0.3462, -0.9490]], grad_fn=<ExpandBackward0>)\n",
      "spliced 2nd matrix: torch.Size([2, 2])\n",
      "tensor([[-0.3462, -0.9490],\n",
      "        [-0.2087,  0.7255]], grad_fn=<SliceBackward0>)\n",
      "[7:9,:]\n",
      "E2: torch.Size([10, 2])\n",
      "tensor([[-3.1781, -0.5276],\n",
      "        [-1.3976, -1.3892],\n",
      "        [-1.9353, -1.2127],\n",
      "        [-1.7978,  0.4617],\n",
      "        [ 0.3829, -2.2507],\n",
      "        [-0.1547, -2.0743],\n",
      "        [-0.0172, -0.3999],\n",
      "        [-0.6924, -1.8979],\n",
      "        [-0.5549, -0.2234],\n",
      "        [ 0.0000,  0.0000]], grad_fn=<CopySlices>)\n",
      "i: 3\n",
      "j: 9\n",
      "vec: tensor([-0.2087,  0.7255], grad_fn=<UnbindBackward0>)\n",
      "expanded vec: torch.Size([1, 2])\n",
      "tensor([[-0.2087,  0.7255]], grad_fn=<ExpandBackward0>)\n",
      "spliced 2nd matrix: torch.Size([1, 2])\n",
      "tensor([[-0.2087,  0.7255]], grad_fn=<SliceBackward0>)\n",
      "[9:10,:]\n",
      "E2: torch.Size([10, 2])\n",
      "tensor([[-3.1781, -0.5276],\n",
      "        [-1.3976, -1.3892],\n",
      "        [-1.9353, -1.2127],\n",
      "        [-1.7978,  0.4617],\n",
      "        [ 0.3829, -2.2507],\n",
      "        [-0.1547, -2.0743],\n",
      "        [-0.0172, -0.3999],\n",
      "        [-0.6924, -1.8979],\n",
      "        [-0.5549, -0.2234],\n",
      "        [-0.4174,  1.4510]], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "j = 0\n",
    "for i, vec in enumerate(E):\n",
    "    print(f\"i: {i}\")\n",
    "    print(f\"j: {j}\")\n",
    "    print(f\"vec: {vec}\")\n",
    "    print(f\"expanded vec: {vec.expand(v-i,d).shape}\\n{vec.expand(v-i,d)}\")\n",
    "    print(f\"spliced 2nd matrix: {E[i:,:].shape}\\n{E[i:,:]}\")\n",
    "    print(f\"[{j}:{j+v-i},:]\")\n",
    "    E2[j:j+v-i,:] += E[i,:].expand(v-i,d) + E[i:,:]\n",
    "    print(f\"E2: {E2.shape}\\n{E2}\")\n",
    "    j += v-i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82515f03-6417-41c3-b930-12e286042d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combination_embedding(E):\n",
    "    v, d = E.shape\n",
    "    E2 = torch.zeros((v*(v+1)//2, d), requires_grad=False)\n",
    "    j = 0\n",
    "    for i, vec in enumerate(E):\n",
    "        E2[j:j+v-i,:] += E[i,:].expand(v-i,d) + E[i:,:]\n",
    "        j += v-i\n",
    "\n",
    "    return E2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "135337f9-e81f-4289-94b1-f4bd95edb96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1466, -1.3016, -0.5108, -1.2226],\n",
      "        [-0.6532,  0.5549,  1.5002,  1.1963],\n",
      "        [ 1.8579,  0.4681, -0.5286,  0.6174],\n",
      "        [-0.2927,  0.8926,  0.9477,  0.5534],\n",
      "        [-1.7084, -0.5552, -1.0848, -0.7154],\n",
      "        [-0.1111, -0.8939,  0.8378, -1.3440],\n",
      "        [ 0.1130, -0.8787,  0.3059,  0.1857],\n",
      "        [-0.1648,  0.7757,  0.7516, -0.4718],\n",
      "        [ 0.3098,  2.3650,  0.7328, -1.1582],\n",
      "        [ 0.2621,  0.4907,  0.7943, -1.1838],\n",
      "        [ 0.3898,  0.0448,  1.4809,  2.1894],\n",
      "        [-1.1570, -0.5844,  1.6015, -1.2931],\n",
      "        [-1.3966, -0.2929, -2.1745,  0.3359],\n",
      "        [-0.7169,  1.2680, -0.0851, -0.9281],\n",
      "        [ 0.6049, -1.2090, -0.1902,  1.7755],\n",
      "        [ 0.2276, -0.4579,  0.6435, -0.9937],\n",
      "        [ 0.4440,  0.4491, -0.7782, -0.4786],\n",
      "        [ 1.3719,  0.0576, -0.2437, -0.9158],\n",
      "        [ 0.6693,  0.2217, -1.1978, -2.0264],\n",
      "        [-0.5351,  0.1084,  1.2621,  2.3545],\n",
      "        [-1.6203, -0.4430,  0.4694,  2.3911],\n",
      "        [-0.1002, -0.9977, -0.3544, -0.2514],\n",
      "        [ 1.0566, -0.6489, -0.1732, -0.4588],\n",
      "        [ 0.9232, -1.7844, -0.8470,  1.9350],\n",
      "        [-1.4660,  1.0863,  0.0992,  0.4550],\n",
      "        [-0.3293, -0.0920,  1.3986, -0.0042],\n",
      "        [-0.1912,  1.7430,  2.3938, -1.6160],\n",
      "        [-0.0722,  0.9974,  0.1170, -1.1751],\n",
      "        [-1.0692,  1.0495,  2.6438,  1.5432],\n",
      "        [ 1.2254,  0.2580,  0.0057,  1.5011],\n",
      "        [-0.9785,  1.0834,  0.6280,  1.5423],\n",
      "        [ 0.1595, -0.2288,  0.1592, -0.6248],\n",
      "        [ 0.1665, -0.0384,  1.1630,  1.4046],\n",
      "        [ 0.4242, -0.3644, -1.0553, -1.1302],\n",
      "        [-0.4086,  0.3297, -0.4825, -0.2971],\n",
      "        [-0.8994,  0.8831, -1.2243,  0.5496],\n",
      "        [-0.3785, -0.7259, -0.0190,  0.5130],\n",
      "        [ 0.5949, -0.6160, -0.5057, -0.5219],\n",
      "        [-0.1576, -0.5917, -0.0987, -0.2467],\n",
      "        [ 0.1267,  0.2805, -1.4023,  1.3445],\n",
      "        [-1.7402,  0.7414,  0.7810, -1.3231],\n",
      "        [-0.4421, -0.1026,  1.0154,  0.9592],\n",
      "        [-1.7632, -0.3180,  1.4942,  0.0190],\n",
      "        [-0.7147, -1.2194,  0.2037,  0.1552],\n",
      "        [-0.5635,  1.4446, -0.5989, -0.2458],\n",
      "        [ 1.1250,  1.2342, -0.4902, -0.5036],\n",
      "        [-1.5870, -0.2849, -0.9106, -1.1836],\n",
      "        [-1.3644, -0.7296, -0.3465,  0.3960],\n",
      "        [-0.6268, -1.4816,  0.0085,  1.1784],\n",
      "        [ 0.2570, -0.1254, -2.1022, -1.0062],\n",
      "        [-1.9045, -0.4178,  1.3286, -0.4687],\n",
      "        [ 1.9532,  0.2255, -1.4756, -1.0982],\n",
      "        [ 0.9267, -0.3637, -0.6720, -0.5049],\n",
      "        [-0.8376,  0.7578,  0.1789, -0.0804],\n",
      "        [-2.2660,  0.0480, -1.4981,  0.2681],\n",
      "        [-0.3590,  0.2819, -0.4929,  0.8481],\n",
      "        [-0.2681, -0.0713,  1.5231, -1.6104],\n",
      "        [-2.5657,  0.1864,  0.7825, -0.2568],\n",
      "        [ 0.1878, -0.7258, -1.9102,  0.7520],\n",
      "        [-0.7577,  0.7825,  0.9211,  0.7260],\n",
      "        [-2.3152,  0.1864,  1.9912, -0.0965],\n",
      "        [ 0.9376, -0.3746, -1.7580, -0.2839],\n",
      "        [ 1.0720,  0.8747, -0.0106,  0.4437],\n",
      "        [-0.6835, -1.1161,  0.3647, -1.3992],\n",
      "        [-0.4996, -1.0696,  0.6186,  0.7648]])\n",
      "2145\n"
     ]
    }
   ],
   "source": [
    "test = torch.randn(65,4)\n",
    "print(test)\n",
    "E2 = combination_embedding(test)\n",
    "print(E2.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d436b4e2-a585-4305-a631-adacfd7fe2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2301585\n"
     ]
    }
   ],
   "source": [
    "E3 = combination_embedding(E2)\n",
    "print(E3.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae8eb3a-49db-4aba-b62d-d66aa7d9c091",
   "metadata": {},
   "source": [
    "I'm thinking maybe i can start off with some absurdly huge supertoken setup made from bytes and then use a method to trim it down to more interesting token combinations like in that one paper i just read [tokenization is more than compression](https://arxiv.org/pdf/2402.18376.pdf) to make the size actually manageable\n",
    "\n",
    "ok let's try a different idea i had recently that kinda brings us back to NCP. let's say at the bottom level we have some list of byte or token or wahtever embedding vectors. at a higher level model let's do a kind of mechanism where we grab the topk results from multiplying by the embedding matrix, then take those topk embedding vectors and add & RMSNorm them together, then temporarily concatenate those new options to the end of our embedding matrix, then multiply by the embedding again and see if any of our new results are better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cf40bd8-2e0a-4167-9f09-51d2efb70fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3421,  0.4649, -0.0076, -0.7923],\n",
       "        [ 0.1688,  1.9326,  0.1201, -0.2860],\n",
       "        [ 0.8903,  0.4975, -1.1466,  0.3244],\n",
       "        [-0.1451,  0.9796, -1.3276,  2.2594],\n",
       "        [ 0.9164,  0.4881,  1.0235, -0.0794],\n",
       "        [-0.7313, -1.4907, -0.9651, -1.4982],\n",
       "        [ 0.2095, -1.0771,  1.6429, -0.5998],\n",
       "        [-1.0176, -0.6422,  0.0439,  1.1870]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "v = 8\n",
    "d = 4\n",
    "E = F.layer_norm(torch.randn(v,d), normalized_shape=(v,d), weight=None, bias=None) # layernorm is safe as RMSNorm for our purposes rn\n",
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2db7256-9343-4f24-a278-0c9dcb2f35ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4521, -1.2818,  1.3657, -0.5361]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = F.layer_norm(torch.randn(1,1,d), normalized_shape=(1,1,d), weight=None, bias=None)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e4bf1e6-e1b3-4e5c-a8d8-b4e273a47aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7883, -2.0835, -1.9749, -4.3456,  1.2290,  1.0652,  4.0406,\n",
       "          -0.2133]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = x @ E.t()\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d22cf3cc-331b-47dd-bbb0-4b2d8c04a3d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[6, 4, 5]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 3\n",
    "top = torch.topk(z, k).indices\n",
    "top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b49f7090-5c67-4ca4-a332-876f56fa21c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triangular(n):\n",
    "    return n * (n+1) // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69203f55-4ca4-4cf8-a7b6-54602e21841c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E_temp = torch.zeros((v+triangular(k-1), d))\n",
    "E_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7118fd4a-fad5-46a7-b43c-6ba402dfbbaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3421,  0.4649, -0.0076, -0.7923],\n",
       "        [ 0.1688,  1.9326,  0.1201, -0.2860],\n",
       "        [ 0.8903,  0.4975, -1.1466,  0.3244],\n",
       "        [-0.1451,  0.9796, -1.3276,  2.2594],\n",
       "        [ 0.9164,  0.4881,  1.0235, -0.0794],\n",
       "        [-0.7313, -1.4907, -0.9651, -1.4982],\n",
       "        [ 0.2095, -1.0771,  1.6429, -0.5998],\n",
       "        [-1.0176, -0.6422,  0.0439,  1.1870],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E_temp[:v,:] += E\n",
    "E_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a09b688-8bcf-49ab-800e-043908db7df6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1260, -0.5890,  2.6663, -0.6793],\n",
       "        [-0.5217, -2.5678,  0.6778, -2.0981],\n",
       "        [ 0.1852, -1.0026,  0.0583, -1.5776]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_tensor_x_no_self_addition(E, indices):\n",
    "    k = len(indices)\n",
    "    d = E.size(1)\n",
    "    X_size = (k - 1) * k // 2\n",
    "    X = torch.empty((X_size, d), dtype=E.dtype)\n",
    "\n",
    "    count = 0\n",
    "    for i in range(k):\n",
    "        for j in range(i + 1, k):\n",
    "            X[count] = E[indices[i]] + E[indices[j]]\n",
    "            count += 1\n",
    "\n",
    "    return X\n",
    "\n",
    "# Now you can use this function to create your tensor X without self additions\n",
    "E_comb = create_tensor_x_no_self_addition(E, top.squeeze(0).squeeze(0))\n",
    "E_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae520126-6974-49f8-9963-4113a39b5824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3421,  0.4649, -0.0076, -0.7923],\n",
       "        [ 0.1688,  1.9326,  0.1201, -0.2860],\n",
       "        [ 0.8903,  0.4975, -1.1466,  0.3244],\n",
       "        [-0.1451,  0.9796, -1.3276,  2.2594],\n",
       "        [ 0.9164,  0.4881,  1.0235, -0.0794],\n",
       "        [-0.7313, -1.4907, -0.9651, -1.4982],\n",
       "        [ 0.2095, -1.0771,  1.6429, -0.5998],\n",
       "        [-1.0176, -0.6422,  0.0439,  1.1870],\n",
       "        [ 1.1260, -0.5890,  2.6663, -0.6793],\n",
       "        [-0.5217, -2.5678,  0.6778, -2.0981],\n",
       "        [ 0.1852, -1.0026,  0.0583, -1.5776]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E_temp[v:,:] += E_comb\n",
    "E_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45f55603-124a-4871-80f1-67d76e438a5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7883, -2.0835, -1.9749, -4.3456,  1.2290,  1.0652,  4.0406,\n",
       "          -0.2133,  5.2696,  5.1058,  2.2942]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_ = x @ E_temp.t()\n",
    "z_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b39896-636d-4012-8869-882d948f76a5",
   "metadata": {},
   "source": [
    "yoooo this is cool so now we have a way of checking against combination vectors without having to keep some absurdly large tensor in memory.\n",
    "\n",
    "so the question is, do we only ever keep the lowest level in memory and then our combinations are only ever from that base level? Or do we let new vectors that get used actually be added to that level's matrix so that it can dynamically grow over time in a kind of byte-pair tokenization style? idk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d05327c-c2c0-4e29-ac26-de9d98d981e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
