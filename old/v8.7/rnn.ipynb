{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "929f6889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 2814k  100 2814k    0     0  7792k      0 --:--:-- --:--:-- --:--:-- 7796k\n",
      "Archive:  data.zip\n",
      "   creating: data/\n",
      "  inflating: data/eng-fra.txt        \n",
      "   creating: data/names/\n",
      "  inflating: data/names/Arabic.txt   \n",
      "  inflating: data/names/Chinese.txt  \n",
      "  inflating: data/names/Czech.txt    \n",
      "  inflating: data/names/Dutch.txt    \n",
      "  inflating: data/names/English.txt  \n",
      "  inflating: data/names/French.txt   \n",
      "  inflating: data/names/German.txt   \n",
      "  inflating: data/names/Greek.txt    \n",
      "  inflating: data/names/Irish.txt    \n",
      "  inflating: data/names/Italian.txt  \n",
      "  inflating: data/names/Japanese.txt  \n",
      "  inflating: data/names/Korean.txt   \n",
      "  inflating: data/names/Polish.txt   \n",
      "  inflating: data/names/Portuguese.txt  \n",
      "  inflating: data/names/Russian.txt  \n",
      "  inflating: data/names/Scottish.txt  \n",
      "  inflating: data/names/Spanish.txt  \n",
      "  inflating: data/names/Vietnamese.txt  \n"
     ]
    }
   ],
   "source": [
    "!curl -O https://download.pytorch.org/tutorial/data.zip; unzip data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d75c7336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from string import ascii_letters\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from unidecode import unidecode\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "357bca03",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = torch.manual_seed(69)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "#device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de17aeb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Czech': tensor([0]),\n",
       " 'German': tensor([1]),\n",
       " 'Arabic': tensor([2]),\n",
       " 'Japanese': tensor([3]),\n",
       " 'Chinese': tensor([4]),\n",
       " 'Vietnamese': tensor([5]),\n",
       " 'Russian': tensor([6]),\n",
       " 'French': tensor([7]),\n",
       " 'Irish': tensor([8]),\n",
       " 'English': tensor([9]),\n",
       " 'Spanish': tensor([10]),\n",
       " 'Greek': tensor([11]),\n",
       " 'Italian': tensor([12]),\n",
       " 'Portuguese': tensor([13]),\n",
       " 'Scottish': tensor([14]),\n",
       " 'Dutch': tensor([15]),\n",
       " 'Korean': tensor([16]),\n",
       " 'Polish': tensor([17])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = \"rnn_data/names\"\n",
    "\n",
    "# construct a dictionary that maps a language to a numerical label\n",
    "lang2label = {\n",
    "    file_name.split(\".\")[0]: torch.tensor([i], dtype=torch.long) # torch.long is the same as torch.int64\n",
    "    for i, file_name in enumerate(os.listdir(data_dir))\n",
    "}\n",
    "lang2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d92a9ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_langs = len(lang2label)\n",
    "num_langs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e77c30d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We first want to use unidecode to standardize all names and remove any acute symbols or the likes\n",
    "char2idx = {letter: i for i, letter in enumerate(ascii_letters + \" .,:;-'\")}\n",
    "num_letters = len(char2idx)\n",
    "num_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf7b8066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This means that each name will now be expressed as a tensor of size (num_char, 59)\n",
    "# in other words, each character will be a tensor of size (59,)`\n",
    "def name2tensor(name):\n",
    "    tensor = torch.zeros(len(name), 1, num_letters) # batch size of 1\n",
    "    for i, char in enumerate(name):\n",
    "        tensor[i][0][char2idx[char]] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c83762d",
   "metadata": {},
   "source": [
    "RNN layers expect the input tensor to be of size (seq_len, batch_size, input_size)\n",
    "Since every name is going to have a different length, we don’t batch the inputs for simplicity purposes and simply use each input as a single batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e46ffabe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name2tensor(\"abc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2794369",
   "metadata": {},
   "source": [
    "Now we need to build a our dataset with all the preprocessing steps. Let’s collect all the decoded and converted tensors in a list, with accompanying labels. The labels can be obtained easily from the file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20f4cdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_names = []\n",
    "target_langs = []\n",
    "\n",
    "for file in os.listdir(data_dir):\n",
    "    with open(os.path.join(data_dir, file)) as f:\n",
    "        lang = file.split(\".\")[0]\n",
    "        names = [unidecode(line.rstrip()) for line in f] # rstrip() removes extra spaces at end of a word, but leaves one\n",
    "        for name in names:\n",
    "            try:\n",
    "                tensor_names.append(name2tensor(name))\n",
    "                target_langs.append(lang2label[lang])\n",
    "            except KeyError:\n",
    "                pass\n",
    "            \n",
    "# We could wrap this in a PyTorch Dataset class, but for simplicity sake let’s just use a good old for loop to feed this data into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c9b2406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we are dealing with normal lists, we can easily use sklearn’s train_test_split() \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_idx, test_idx = train_test_split(\n",
    "    range(len(target_langs)),\n",
    "    test_size=0.1,\n",
    "    shuffle=True,\n",
    "    stratify=np.array([tensor.item() for tensor in target_langs])\n",
    ")\n",
    "\n",
    "train_dataset = [(tensor_names[i], target_langs[i]) for i in train_idx]\n",
    "test_dataset = [(tensor_names[i], target_langs[i]) for i in test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85ce7c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 18063\n",
      "Test: 2007\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train: {len(train_dataset)}\")\n",
    "print(f\"Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b6908f",
   "metadata": {},
   "source": [
    "We will be building two models: a simple RNN, which is going to be built from scratch, and a GRU-based model using PyTorch’s layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f8d4e9",
   "metadata": {},
   "source": [
    "This is a very simple RNN that takes a single character tensor representation as input and produces some prediction and a hidden state, which can be used in the next iteration. Notice that it is just some fully connected layers with a sigmoid non-linearity applied during the hidden state computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "865fd4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MyRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.in2hidden = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.in2output = nn.Linear(input_size + hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x, hidden_state):\n",
    "        combined = torch.cat((x, hidden_state), 1)\n",
    "        hidden = torch.sigmoid(self.in2hidden(combined))\n",
    "        output = self.in2output(combined)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return nn.init.kaiming_uniform_(torch.empty(1, self.hidden_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8fe7e9",
   "metadata": {},
   "source": [
    "We call init_hidden() at the start of every new batch. For easier training and learning, I decided to use kaiming_uniform_() to initialize these hidden states.\n",
    "\n",
    "We can now build our model and start training it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21694852",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = MyRNN(num_letters, hidden_size, num_langs)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c822d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [3000/18063], Loss: 0.1325\n",
      "Epoch [1/2], Step [6000/18063], Loss: 3.4649\n",
      "Epoch [1/2], Step [9000/18063], Loss: 0.6217\n",
      "Epoch [1/2], Step [12000/18063], Loss: 0.0161\n",
      "Epoch [1/2], Step [15000/18063], Loss: 1.2954\n",
      "Epoch [1/2], Step [18000/18063], Loss: 0.6237\n",
      "Epoch [2/2], Step [3000/18063], Loss: 0.0003\n",
      "Epoch [2/2], Step [6000/18063], Loss: 0.0030\n",
      "Epoch [2/2], Step [9000/18063], Loss: 4.1942\n",
      "Epoch [2/2], Step [12000/18063], Loss: 1.5010\n",
      "Epoch [2/2], Step [15000/18063], Loss: 1.5929\n",
      "Epoch [2/2], Step [18000/18063], Loss: 3.4665\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "print_interval = 3000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    random.shuffle(train_dataset)\n",
    "    for i, (name, label) in enumerate(train_dataset):\n",
    "        hidden_state = model.init_hidden()\n",
    "        for char in name:\n",
    "            output, hidden_state = model(char, hidden_state)\n",
    "        loss = criterion(output, label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i + 1) % print_interval == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "                f\"Step [{i + 1}/{len(train_dataset)}], \"\n",
    "                f\"Loss: {loss.item():.4f}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f228311f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 71.8984%\n"
     ]
    }
   ],
   "source": [
    "num_correct = 0\n",
    "num_samples = len(test_dataset)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for name, label in test_dataset:\n",
    "        hidden_state = model.init_hidden()\n",
    "        for char in name:\n",
    "            output, hidden_state = model(char, hidden_state)\n",
    "        _, pred = torch.max(output, dim=1)\n",
    "        num_correct += bool(pred == label)\n",
    "\n",
    "print(f\"Accuracy: {num_correct / num_samples * 100:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e244005",
   "metadata": {},
   "source": [
    "concrete examples\n",
    "\n",
    "I don’t know if any of these names were actually in the training or testing set; these are just some random names I came up with that I thought would be pretty reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85d971c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2lang = {label.item(): lang for lang, label in lang2label.items()}\n",
    "\n",
    "def myrnn_predict(name):\n",
    "    model.eval()\n",
    "    tensor_name = name2tensor(name)\n",
    "    with torch.no_grad():\n",
    "        hidden_state = model.init_hidden()\n",
    "        for char in tensor_name:\n",
    "            output, hidden_state = model(char, hidden_state)\n",
    "        _, pred = torch.max(output, dim=1)\n",
    "    model.train()    \n",
    "    return label2lang[pred.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da6fcd32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('English', 'Chinese', 'Russian')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myrnn_predict(\"Randy\"), myrnn_predict(\"Qin\"), myrnn_predict(\"Vladamir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6a1c0b",
   "metadata": {},
   "source": [
    "This is cool and all, and I could probably stop here, but I wanted to see how this custom model fares in comparison to, say, a model using PyTorch layers. Gated Recurrent Unit is probably not fair game for our simple RNN, but let’s see how well it does\n",
    "\n",
    "A Gated Recurrent Unit (GRU) is a type of recurrent neural network (RNN) architecture introduced by Kyunghyun Cho et al. in 2014. It aims to solve the vanishing gradient problem inherent in traditional RNNs, making it more effective for learning from long sequences. \n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "Given an input sequence \\( X = \\{x_1, x_2, \\ldots, x_T\\} \\), the GRU updates its hidden state \\( h_t \\) at each time step \\( t \\) using the following equations:\n",
    "\n",
    "1. **Update Gate $z_t$**\n",
    "\\[\n",
    "z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t] + b_z)\n",
    "\\]\n",
    "\n",
    "2. **Reset Gate \\( r_t \\)**\n",
    "\\[\n",
    "r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t] + b_r)\n",
    "\\]\n",
    "\n",
    "3. **Candidate Hidden State \\( \\tilde{h}_t \\)**\n",
    "\\[\n",
    "\\tilde{h}_t = \\tanh(W \\cdot [r_t \\odot h_{t-1}, x_t] + b)\n",
    "\\]\n",
    "\n",
    "4. **Hidden State \\( h_t \\)**\n",
    "\\[\n",
    "h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t\n",
    "\\]\n",
    "\n",
    "Here:\n",
    "\n",
    "- \\( \\sigma \\) is the sigmoid activation function.\n",
    "- \\( \\odot \\) denotes element-wise multiplication.\n",
    "- \\( W_z, W_r, W \\) and \\( b_z, b_r, b \\) are trainable parameters.\n",
    "- \\( [h_{t-1}, x_t] \\) denotes the concatenation of \\( h_{t-1} \\) and \\( x_t \\).\n",
    "\n",
    "### Key Features\n",
    "\n",
    "1. **Gating Mechanism**: Update and reset gates regulate the flow of information, allowing the model to learn long-term dependencies.\n",
    "2. **Efficiency**: Fewer parameters than its LSTM counterpart, which makes it computationally more efficient.\n",
    "3. **Vanishing Gradient**: Mitigates but does not completely eliminate the vanishing gradient problem.\n",
    "\n",
    "GRUs are widely used in natural language processing, time-series analysis, and other sequence modeling tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b1f84c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, num_layers, hidden_size):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=num_letters, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, num_langs)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        hidden_state = self.init_hidden()\n",
    "        output, hidden_state = self.gru(x, hidden_state)\n",
    "        output = self.fc(output[-1])\n",
    "        return output\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(self.num_layers, 1, self.hidden_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16f2111d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GRUModel(num_layers=2, hidden_size=hidden_size)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c69d4d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [3000/18063], Loss: 0.0886\n",
      "Epoch [1/2], Step [6000/18063], Loss: 2.4803\n",
      "Epoch [1/2], Step [9000/18063], Loss: 0.0501\n",
      "Epoch [1/2], Step [12000/18063], Loss: 0.2171\n",
      "Epoch [1/2], Step [15000/18063], Loss: 0.1890\n",
      "Epoch [1/2], Step [18000/18063], Loss: 0.0100\n",
      "Epoch [2/2], Step [3000/18063], Loss: 1.3591\n",
      "Epoch [2/2], Step [6000/18063], Loss: 0.0076\n",
      "Epoch [2/2], Step [9000/18063], Loss: 0.4325\n",
      "Epoch [2/2], Step [12000/18063], Loss: 0.2355\n",
      "Epoch [2/2], Step [15000/18063], Loss: 3.4074\n",
      "Epoch [2/2], Step [18000/18063], Loss: 0.0001\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "print_interval = 3000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    random.shuffle(train_dataset)\n",
    "    for i, (name, label) in enumerate(train_dataset):\n",
    "        output = model(name)\n",
    "        loss = criterion(output, label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "         \n",
    "        if (i + 1) % print_interval == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "                f\"Step [{i + 1}/{len(train_dataset)}], \"\n",
    "                f\"Loss: {loss.item():.4f}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f8e5a9",
   "metadata": {},
   "source": [
    "The training appeared somewhat more stable at first, but we do see a weird jump near the end of the second epoch. This is partially because I didn’t use gradient clipping for this GRU model, and we might see better results with clipping applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d97f453c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 81.4150%\n"
     ]
    }
   ],
   "source": [
    "num_correct = 0\n",
    "num_samples = len(test_dataset)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for name, label in test_dataset:\n",
    "        output = model(name)\n",
    "        _, pred = torch.max(output, dim=1)\n",
    "        num_correct += bool(pred == label)\n",
    "\n",
    "print(f\"Accuracy: {num_correct / num_samples * 100:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c7cf8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pytorch_predict(name):\n",
    "    model.eval()\n",
    "    tensor_name = name2tensor(name)\n",
    "    with torch.no_grad():\n",
    "        output = model(tensor_name)\n",
    "        _, pred = torch.max(output, dim=1)\n",
    "    model.train()\n",
    "    return label2lang[pred.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a14f8cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'label2lang' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pytorch_predict(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandy\u001b[39m\u001b[38;5;124m\"\u001b[39m), pytorch_predict(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQin\u001b[39m\u001b[38;5;124m\"\u001b[39m), pytorch_predict(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVladamir\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 8\u001b[0m, in \u001b[0;36mpytorch_predict\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m      6\u001b[0m     _, pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m label2lang[pred\u001b[38;5;241m.\u001b[39mitem()]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'label2lang' is not defined"
     ]
    }
   ],
   "source": [
    "pytorch_predict(\"Randy\"), pytorch_predict(\"Qin\"), pytorch_predict(\"Vladamir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef5f487",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
