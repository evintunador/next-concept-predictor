{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# GOAL\n",
    "\n",
    "so for this first implementation of a concept head, we're just gonna attempt to extend the context length of the model by giving the GPT a kind of recurrent hidden state that basically summarizes the previous length t window that it was looking at. to make this work imma have to train on the last context length chunk of the total sequence first, and then work backwards. each time i'll be passing the concept vector that was wanted to the previous chunk, and then regression / cosine similarity training on it. if i had a dataset other than tinyshakespeare that actually had \\<endoftext> tokens then it'd be interesting to train that last context length chunk of the sequence to predict that token. Also I might need a learnable token to separate the concept from the rest of the sequence\n",
    "\n",
    "a limitation of this approach is that it only works in chunks of the context length. if it works well then this is useful bc it means we could use a shorter context length which is important because attention is quadratic in its use of compute as a function of context length. however it's not clear whether the current approach will work with varying context lengths or if it'll only give us a useable context vector for the pre-determined context length t\n",
    "\n",
    "yoooo so to speed up inference what if i ran a bunch of inferences on a bunch of sequences & got all the concept vectors that came out, then trained a GPT using cosine similarity regression rather than CEL classification to predict future concept vectors. then what i'd be able to do is after the first t tokens are created this meta-model could use the concept vector that's outputted to create all the future concept vectors, then i could run the actual model in parallel for inference\n",
    "\n",
    "in a future version i would like to allow the model to carry over all previous concept vectors it has made from this sequence rather than just the most recent one\n",
    "\n",
    "at some point i'd like to solve that dynamic context length problem. one idea i've got is to use a dataset with actual documents that end in \\<endoftext> tokens. basically i'd take the total sequence length of the document and divide by k, then do my batch training on context lengths of that size. i'd have to match up documents of the same size. and i'd have to switch from learned pos embeddings to RoPE. \n",
    "\n",
    "Another idea to potentially explore is to use the unused outputs of the regression head for something. maybe hyperparameter control or creating higher-level concept vectors\n",
    "\n",
    "after that i would like to train the model on a dataset of separate documents where each document provides its own concept vector, so this model can essentially be used to extend my obsidian graph\n",
    "\n",
    "a big assumption with this model is that the embedding space is actually capable of carrying surprisingly complicated concepts, far more than the number of tokens\n",
    "\n",
    "another half-baked idea for the generation process is to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### !!!! DO NOT RUN THIS FIRST CELL UNLESS YOU HAVE THE SAME VENV PATH ISSUE THAT I DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/tunadorable/local-repos/learning_medusa/venv/lib/python3.11/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### !!!! ONLY FOR APPLE SILICON\n",
    "make your own if u use cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fjjvMifYZf7x"
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "b = 4 # how many independent sequences will we process in parallel?\n",
    "t = 8 # what is the maximum context length for predictions?\n",
    "max_iters = 100\n",
    "eval_interval = 10\n",
    "lr = 3e-4 # learning rate for each backprop step\n",
    "eval_iters = 10\n",
    "d = 16 # embedding aka hidden dimension\n",
    "h = 4 # number of attention heads\n",
    "l = 4 # number of transormer layers\n",
    "dropout = 0.2 # % of parameters to ignore every iteration\n",
    "l2 = 0.01 # multiplier for our L2 norm to encourage sparsity\n",
    "\n",
    "k = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataset is TinyShakespeare\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', ''] 66\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "# we'll be using individual characters instead of tokens\n",
    "chars = sorted(list(set(text)))\n",
    "chars.append('') # a learnable token we'll use later\n",
    "v = len(chars)\n",
    "print(chars, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from characters to integers & vice versa\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1003854 111540\n"
     ]
    }
   ],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(len(train_data), len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split, k=k, b=b, t=t):\n",
    "    # Assume train_data and val_data are defined outside this function\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    max_index = len(data) - k * t\n",
    "    ix = torch.randint(max_index, (b,))\n",
    "\n",
    "    # Initialize x and y\n",
    "    x = torch.zeros((k, b, t), dtype=data.dtype).to(device)\n",
    "    y = torch.zeros((k, b, t), dtype=data.dtype).to(device)\n",
    "\n",
    "    # Fill in x and y\n",
    "    for i in range(k):\n",
    "        x[i] = torch.stack([data[j+i*t:j+i*t+t] for j in ix])\n",
    "        y[i] = torch.stack([data[j+i*t+1:j+i*t+t+1] for j in ix])\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x  torch.Size([8, 4, 8]) \n",
      " tensor([[[63,  8,  0, 14, 56, 43, 39, 49],\n",
      "         [43,  6,  1, 41, 53, 51, 43,  6],\n",
      "         [21, 27, 10,  0, 19, 53,  6,  1],\n",
      "         [43, 56, 43, 58, 53,  1, 47, 44]],\n",
      "\n",
      "        [[ 1, 53, 44, 44,  1, 58, 46, 43],\n",
      "         [ 1, 42, 47, 57, 54, 39, 58, 41],\n",
      "         [58, 46, 43, 52, 11,  1, 44, 53],\n",
      "         [ 1, 63, 53, 59,  5, 50, 50,  1]],\n",
      "\n",
      "        [[ 1, 54, 39, 56, 50, 43, 63, 11],\n",
      "         [46, 11,  1,  5, 58, 47, 57,  1],\n",
      "         [56,  1,  5, 58, 47, 57,  1, 47],\n",
      "         [39,  1, 61, 47, 50, 50, 47, 52]],\n",
      "\n",
      "        [[ 1, 44, 53, 56,  1, 57, 41, 39],\n",
      "         [40, 53, 53, 58, 50, 43, 57, 57],\n",
      "         [52,  1, 60, 39, 47, 52,  0, 32],\n",
      "         [45,  1, 43, 39, 56,  1, 47, 52]],\n",
      "\n",
      "        [[56, 41, 43,  1, 21,  1, 41, 39],\n",
      "         [ 1, 58, 53,  1, 43, 62, 41, 50],\n",
      "         [53,  1, 57, 43, 43, 49,  1, 46],\n",
      "         [41, 50, 47, 52, 43,  6,  0, 35]],\n",
      "\n",
      "        [[52,  1, 56, 43, 44, 56, 39, 47],\n",
      "         [39, 47, 51,  8,  0,  0, 20, 13],\n",
      "         [47, 51,  1, 46, 43, 56, 43,  1],\n",
      "         [46, 39, 58,  5, 57,  1, 51, 47]],\n",
      "\n",
      "        [[52,  0, 32, 46, 43,  1, 43, 62],\n",
      "         [31, 32, 21, 26, 19, 31, 10,  0],\n",
      "         [58, 46, 39, 58,  1, 51, 43, 39],\n",
      "         [52, 43,  1, 47, 57,  1, 63, 53]],\n",
      "\n",
      "        [[43, 41, 59, 58, 47, 53, 52,  1],\n",
      "         [27,  1, 40, 50, 53, 53, 42, 63],\n",
      "         [52, 57,  1, 52, 53, 58,  1, 58],\n",
      "         [59, 56, 57,  1, 39, 52, 42,  1]]], device='mps:0')\n",
      "y  torch.Size([8, 4, 8]) \n",
      " tensor([[[ 8,  0, 14, 56, 43, 39, 49,  1],\n",
      "         [ 6,  1, 41, 53, 51, 43,  6,  1],\n",
      "         [27, 10,  0, 19, 53,  6,  1, 58],\n",
      "         [56, 43, 58, 53,  1, 47, 44,  1]],\n",
      "\n",
      "        [[53, 44, 44,  1, 58, 46, 43,  1],\n",
      "         [42, 47, 57, 54, 39, 58, 41, 46],\n",
      "         [46, 43, 52, 11,  1, 44, 53, 56],\n",
      "         [63, 53, 59,  5, 50, 50,  1, 39]],\n",
      "\n",
      "        [[54, 39, 56, 50, 43, 63, 11,  1],\n",
      "         [11,  1,  5, 58, 47, 57,  1, 40],\n",
      "         [ 1,  5, 58, 47, 57,  1, 47, 52],\n",
      "         [ 1, 61, 47, 50, 50, 47, 52, 45]],\n",
      "\n",
      "        [[44, 53, 56,  1, 57, 41, 39, 56],\n",
      "         [53, 53, 58, 50, 43, 57, 57,  1],\n",
      "         [ 1, 60, 39, 47, 52,  0, 32, 53],\n",
      "         [ 1, 43, 39, 56,  1, 47, 52, 41]],\n",
      "\n",
      "        [[41, 43,  1, 21,  1, 41, 39, 52],\n",
      "         [58, 53,  1, 43, 62, 41, 50, 39],\n",
      "         [ 1, 57, 43, 43, 49,  1, 46, 47],\n",
      "         [50, 47, 52, 43,  6,  0, 35, 46]],\n",
      "\n",
      "        [[ 1, 56, 43, 44, 56, 39, 47, 52],\n",
      "         [47, 51,  8,  0,  0, 20, 13, 31],\n",
      "         [51,  1, 46, 43, 56, 43,  1, 58],\n",
      "         [39, 58,  5, 57,  1, 51, 47, 52]],\n",
      "\n",
      "        [[ 0, 32, 46, 43,  1, 43, 62, 43],\n",
      "         [32, 21, 26, 19, 31, 10,  0, 27],\n",
      "         [46, 39, 58,  1, 51, 43, 39, 52],\n",
      "         [43,  1, 47, 57,  1, 63, 53, 59]],\n",
      "\n",
      "        [[41, 59, 58, 47, 53, 52,  1, 53],\n",
      "         [ 1, 40, 50, 53, 53, 42, 63,  1],\n",
      "         [57,  1, 52, 53, 58,  1, 58, 53],\n",
      "         [56, 57,  1, 39, 52, 42,  1, 61]]], device='mps:0')\n",
      "the spaces are messed up only bc of how print() works: \n",
      " y.\n",
      "Break  off the  parley;  for sca\n"
     ]
    }
   ],
   "source": [
    "# so you can see what the tokenized data looks like\n",
    "x,y = get_batch('train')\n",
    "print(\"x \", x.shape, \"\\n\", x) # instead of (b,t) it's (k,b,t)\n",
    "print(\"y \", y.shape, \"\\n\", y)\n",
    "t1, t2, t3, t4 = decode(x[0,0].tolist()), decode(x[1,0].tolist()), decode(x[2,0].tolist()), decode(x[3,0].tolist())\n",
    "print(\"the spaces are messed up only bc of how print() works: \\n\", t1,t2,t3,t4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval() # sets model to eval mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for j in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "\n",
    "            # how we'll keep track of this iteration's total loss\n",
    "            loss_sum = torch.tensor(0.0, device=device)\n",
    "            \n",
    "            # initlaize c_vecs so that the model will use the empty token '' on first go\n",
    "            c_vecs=None\n",
    "            \n",
    "            for i in range(k):\n",
    "                # notice how we can get loss without testing on c_hat bc at the end of the day only NTP loss matters\n",
    "                # c_hat loss is just a means to an end\n",
    "                logits, c_vecs, loss = model(idx=X[i,...], targets=Y[i,...], c_vecs=c_vecs)\n",
    "                loss_sum += loss\n",
    "\n",
    "            losses[j] = (loss_sum/k).item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # just resets to training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, d):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d, 4 * d), # the 4 is arbitrary, but i wouldn't go smaller\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(4 * d, d),\n",
    "            nn.Dropout(dropout))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(d, head_size, bias=False)\n",
    "        self.query = nn.Linear(d, head_size, bias=False)\n",
    "        self.value = nn.Linear(d, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(1+t,1+t))) # mask future timestesps # 1+ for the prepended concept vec\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        b,t,d = x.shape\n",
    "        k = self.key(x)   # (b,t,d/h)\n",
    "        q = self.query(x) # (b,t,d/h)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (b, t, d/h) @ (b, d/h, t) -> (b, t, t)\n",
    "        wei = wei.masked_fill(self.tril[:t, :t] == 0, float('-inf')) # (b, t, t)\n",
    "        wei = F.softmax(wei, dim=-1) # (b, t, t)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (b,t,d/h)\n",
    "        out = wei @ v # (b, t, t) @ (b, t, d/h) -> (b, t, d/h)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, h, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(h)])\n",
    "        self.proj = nn.Linear(head_size * h, d)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, d, h):\n",
    "        # d: embedding dimension, h: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = d // h # the double backslash just makes the output an int instead of float\n",
    "        self.sa = MultiHeadAttention(h, head_size)\n",
    "        self.ffwd = FeedForward(d)\n",
    "        self.ln = nn.LayerNorm(d, elementwise_affine=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln(x))\n",
    "        x = x + self.ffwd(self.ln(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conceptGPTv9(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(v, d)\n",
    "        self.vocab_len = v\n",
    "        \n",
    "        # simple learned positional encodings rather than sine or RoPE\n",
    "        self.position_embedding_table = nn.Embedding(t+1, d) # +1 for c or the learnable token\n",
    "        \n",
    "        # bulk of the beast\n",
    "        self.blocks = nn.Sequential(*[Block(d, h) for _ in range(l)]) \n",
    "        \n",
    "        # output head\n",
    "        self.lm_head = nn.Linear(d, v) \n",
    "        \n",
    "        # apparently layernorm by default actually adds a linear layer & bias unless you specificaly specify false\n",
    "        # if you're gonna re-use the same layernorm object then you should specify false\n",
    "        self.ln = nn.LayerNorm(d, elementwise_affine=False) # final layer norm\n",
    "        \n",
    "        # the concept head\n",
    "        self.conc_head = FeedForward(d)\n",
    "\n",
    "        # Initialize Cosine Similarity module here\n",
    "        self.cosine_similarity = nn.CosineSimilarity(dim=1)\n",
    "\n",
    "        # according to Andrej Karpathy this _init_weights method is better than default\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, idx=None, targets=None, c_vecs=None, c_hat=None, verbose=False):\n",
    "        # c_hat is your target concept vectors\n",
    "        \n",
    "        # ik it's crazy but you can in fact not pass in a sequence & still have it perform inference\n",
    "        b, t = (idx.shape[0],idx.shape[1]) if idx is not None else (1,0)\n",
    "        # in theory you should only do this for v9.2 where we use c_vecs to massively parallelize inference\n",
    "            \n",
    "        # if there's no input concept vector we use the learned token ''\n",
    "        if c_vecs is None:\n",
    "            # v-1 is the index of the '' token we added\n",
    "            c_ind = (self.vocab_len-1)*torch.ones((b,1),device=device,dtype=torch.long) # (b,1)\n",
    "\n",
    "            # turn it into the vector parts for residual later\n",
    "            c_vecs = self.token_embedding_table(c_ind) # (b,1,d)\n",
    "\n",
    "        # regular GPT pos embeddings but with 1+t as our context length\n",
    "        pos_emb = self.position_embedding_table(torch.arange(1+t, device=device)) # (1+t,d)\n",
    "\n",
    "        # most scenarios idx should not be None. later when we do v9.2 it will be tho\n",
    "        if idx is not None:\n",
    "            tok_emb = self.token_embedding_table(idx) # (b,t,d)\n",
    "            x = self.ln(torch.cat((c_vecs,tok_emb),dim=1) + pos_emb) # (b,1+t,d)\n",
    "        else:\n",
    "            # if there's no idx inputted, that means we're predicting based off either\n",
    "            # - the learned token '' in the case that we're at the beginning of the sequence\n",
    "            # - a c_vec that was passed in, meaning we're at the beginning of a chunk & no initial token was provided\n",
    "            x = self.ln(c_vecs + pos_emb)\n",
    "        \n",
    "        # the bulk of the beast\n",
    "        x = self.ln(self.blocks(x)) # (b,1+t,d) -> (b,1+t,d)\n",
    "        \n",
    "        # the regular next-token prediction head\n",
    "        logits = self.lm_head(x)[:,1:,:] # ((b,1+t,d)@(d,v))[:,1:,:] -> (b,t,v)\n",
    "\n",
    "        # the concept head is just a 2-layer feedforward, a splice to make it 1 vector per sequence, then a layernorm\n",
    "        c_out = self.ln(self.conc_head(x)[:,0,:]).unsqueeze(1) # (b,1+t,d) -> (b,1,d)\n",
    "        # the other indicies could be used for something else in the future\n",
    "\n",
    "        if targets is None:\n",
    "            # If we're not training at all, we can ignore loss\n",
    "            loss = None\n",
    "        else:\n",
    "            # Regular NTP loss\n",
    "            b, t, v = logits.shape\n",
    "            ntp_loss = F.cross_entropy(logits.reshape(b*t, v), targets.reshape(b*t))\n",
    "            \n",
    "            if c_hat is not None:\n",
    "                # Cosine similarity loss for concept vectors\n",
    "                # this is similar to doing a regression since all layernormed vectors have roughly the same radius anyways\n",
    "                # except unlike MSE or MAE, it lets us interpret all c's as vectors in embedding space\n",
    "                similarity = self.cosine_similarity(c_out, c_hat)\n",
    "\n",
    "                # Maximizing cosine similarity is equivalent to minimizing 1 - cosine similarity\n",
    "                c_loss = 1 - similarity.mean()  \n",
    "        \n",
    "                # might try a different way to balance the two in the future. for now we'll multiply by sqrt(t)\n",
    "                loss = ntp_loss + (c_loss*(t**-0.5))\n",
    "            else:\n",
    "                # If we're on the first run of training\n",
    "                # aka don't have a c_hat\n",
    "                # aka we're at the end of the sequence\n",
    "                loss = ntp_loss\n",
    "        \n",
    "        return logits, c_out, loss\n",
    "\n",
    "    def generate_subsequence(self, idx, c_vecs, inp, start, end, temperature):\n",
    "        \"\"\"\n",
    "        so this is the thing that generates subsequences of length <= t\n",
    "        \"\"\"\n",
    "        for j in range(start, end):\n",
    "            # now we're looking for that next token\n",
    "            logits, c_vecs, loss = model(idx=inp, c_vecs=c_vecs)\n",
    "            \n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (b, d)\n",
    "            \n",
    "            # scale logits by the temperature\n",
    "            logits = logits / (temperature+1e-10)\n",
    "            \n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (b, d)\n",
    "            \n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (b, 1)\n",
    "            \n",
    "            # to be inputted next inference run\n",
    "            inp = torch.concat((inp, idx_next),dim=1)\n",
    "            \n",
    "            # keeping track of our total sequence\n",
    "            idx = torch.concat((idx, idx_next),dim=1)\n",
    "        \n",
    "        return idx, c_vecs\n",
    "        \n",
    "    def generate(self, idx, max_new_tokens=250, temperature=1.0):\n",
    "        \"\"\"\n",
    "        the # notes assume we passed in as input \"JULIET:\\nO Romeo, Romeo! wherefore art thou R\" with a context length of 8\n",
    "        \"\"\"\n",
    "        b, i = idx.shape\n",
    "    \n",
    "        ##############################################\n",
    "        #### getting the first concept vector(s) ####\n",
    "        ##############################################\n",
    "\n",
    "        # if our provided context is longer than our context length\n",
    "        if i >= t:\n",
    "            # the subsequence we'll be performing inference on to get the first concept vec\n",
    "            inp = idx[:,:t] # \"JULIET:\\n\"\n",
    "            # we want that first outputted concept vector\n",
    "            logits, c_vecs, loss = model(idx=inp)\n",
    "        \n",
    "            # getting concept vectors for the next few length t parts of the input context\n",
    "            context_chunks = i // t\n",
    "            for j in range(1,context_chunks): # \"O Romeo,\" -> \" Romeo! \" -> \"wherefor\" -> \"e art th\" \n",
    "                # the subsequence we'll be performing inference on\n",
    "                inp = idx[:,j*t:(j+1)*t]\n",
    "            \n",
    "                # all we care about is that next concept vector\n",
    "                logits, c_vecs, loss = model(idx=inp, c_vecs=c_vecs)\n",
    "            \n",
    "            # defining c_inp since we won't want to use the newest c_vecs every time,\n",
    "            # we only want to use c_vecs generated from a full length t subsequence since \n",
    "            # that's how the model was trained\n",
    "            c_inp = c_vecs\n",
    "        else:\n",
    "            c_inp = None\n",
    "    \n",
    "        ###########################\n",
    "        #### Actual generation ####\n",
    "        ###########################\n",
    "        \n",
    "        # beginning generation wherever the context leaves off\n",
    "        partial_final_context_chunk = i % t\n",
    "        # if it ==0 then that means the context is divisible by t so there's no partial chunk to finish, so we skip\n",
    "        if partial_final_context_chunk != 0:\n",
    "            # we'll use c_inp plus this as our context to generate off of\n",
    "            inp = idx[:,-partial_final_context_chunk:]\n",
    "            \n",
    "            idx, c_inp = self.generate_subsequence(idx=idx, inp=inp, c_vecs=c_inp, start=partial_final_context_chunk, end=t+1, temperature=temperature)\n",
    "    \n",
    "        # all of the remaining full chunks as defined by max_new_tokens\n",
    "        full_chunks = (i + max_new_tokens) // t\n",
    "        for k in range(context_chunks + 1, full_chunks):\n",
    "            # the last step gave us both c_vecs and another NTP token\n",
    "            inp = idx[:,-1].unsqueeze(0)\n",
    "\n",
    "            idx, c_inp = self.generate_subsequence(idx=idx, inp=inp, c_vecs=c_inp, start=0, end=t, temperature=temperature)\n",
    "    \n",
    "        # the final remainder as defined by the number of chunks & max_new_tokens\n",
    "        final_remainder = (i + max_new_tokens) % t\n",
    "        if final_remainder != 0:\n",
    "            # the last step gave us both c_vecs and another NTP token\n",
    "            inp = idx[:,-1].unsqueeze(0)\n",
    "\n",
    "            idx, c_vecs = self.generate_subsequence(idx=idx, inp=inp, c_vecs=c_inp, start=0, end=final_remainder, temperature=temperature)\n",
    "    \n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "if you don't want to do your own training just scroll down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.122 K parameters\n"
     ]
    }
   ],
   "source": [
    "model = conceptGPTv9().to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.1852, val loss 4.1835, time elapsed: 2.37 seconds\n",
      "step 10: train loss 3.9518, val loss 3.9495, time elapsed: 20.66 seconds\n",
      "step 20: train loss 3.7157, val loss 3.7139, time elapsed: 39.56 seconds\n",
      "step 30: train loss 3.5511, val loss 3.5417, time elapsed: 58.03 seconds\n",
      "step 40: train loss 3.4309, val loss 3.4355, time elapsed: 75.95 seconds\n",
      "step 50: train loss 3.3346, val loss 3.3586, time elapsed: 94.02 seconds\n",
      "step 60: train loss 3.2637, val loss 3.2613, time elapsed: 112.38 seconds\n",
      "step 70: train loss 3.1786, val loss 3.1855, time elapsed: 130.09 seconds\n",
      "step 80: train loss 3.1309, val loss 3.1646, time elapsed: 147.78 seconds\n",
      "step 90: train loss 3.0918, val loss 3.1200, time elapsed: 165.48 seconds\n",
      "step 99: train loss 3.0117, val loss 3.0727, time elapsed: 181.69 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for iter in range(max_iters):\n",
    "    \n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # initialize it. when this happens the system starts with a learned token '' where the concept vecs will be later\n",
    "    c_hat = None\n",
    "    \n",
    "    # k is the number of subsequences we're training through\n",
    "    for j in range(k-1):\n",
    "        # temporarily set model to evaluate mode since we're not backpropogating on this part\n",
    "        model.eval()\n",
    "        \n",
    "        # initlaize c_vecs so that the model will use the empty token '' on first go\n",
    "        c_vecs=None\n",
    "        \n",
    "        # forward pass to get list of concept vectors\n",
    "        for i in range(k-1-j): # -1 bc i don't think we care about the last one\n",
    "            logits, c_vecs, loss = model(xb[i,...], targets=yb[i,...], c_vecs=c_vecs)\n",
    "        \n",
    "        # saving a version to train with that's not attached to the gradient graph\n",
    "        c_vecs_input = c_vecs.clone().detach().requires_grad_(True)\n",
    "        \n",
    "        # put the model back into train mode\n",
    "        model.train()\n",
    "        \n",
    "        # so now we can use that c_hat we made earlier to train on\n",
    "        logits, c_vecs, loss = model(xb[k-1-j,...], targets=yb[k-1-j,...], c_vecs=c_vecs_input, c_hat=c_hat)\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        \n",
    "        # creating this set of \"ideal\" concept vectors to train on\n",
    "        with torch.no_grad():\n",
    "            c_prime = c_vecs_input - lr * c_vecs_input.grad  # Simple gradient descent step\n",
    "        \n",
    "        # making absolutely sure the gradient graphs are not connected for sake of memory savings\n",
    "        # we'll be using this to train on\n",
    "        c_hat = c_prime.clone().detach().requires_grad_(False)\n",
    "        \n",
    "        # so this actually implements the gradients\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "    # the last iter doesn't need the model.eval() forward pass stuff, we just pass in empty c_vecs & train on c_hat\n",
    "    logits, c_vecs, loss = model(xb[k-1-j,...], targets=yb[k-1-j,...], c_vecs=None, c_hat=c_hat) # xb[0,...]\n",
    "    \n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - start_time\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, time elapsed: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'models/{model.__class__.__name__}_b{b}_t{t}_d{d}_h{h}_l{l}_lr{lr}_drop{dropout}_l2-{l2}_k{k}_{time.strftime(\"%Y-%m-%d|%H-%M-%S\")}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "conceptGPTv9(\n",
       "  (token_embedding_table): Embedding(66, 16)\n",
       "  (position_embedding_table): Embedding(9, 16)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=16, out_features=4, bias=False)\n",
       "            (query): Linear(in_features=16, out_features=4, bias=False)\n",
       "            (value): Linear(in_features=16, out_features=4, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=16, out_features=64, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=64, out_features=16, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln): LayerNorm((16,), eps=1e-05, elementwise_affine=False)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=16, out_features=4, bias=False)\n",
       "            (query): Linear(in_features=16, out_features=4, bias=False)\n",
       "            (value): Linear(in_features=16, out_features=4, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=16, out_features=64, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=64, out_features=16, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln): LayerNorm((16,), eps=1e-05, elementwise_affine=False)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=16, out_features=4, bias=False)\n",
       "            (query): Linear(in_features=16, out_features=4, bias=False)\n",
       "            (value): Linear(in_features=16, out_features=4, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=16, out_features=64, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=64, out_features=16, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln): LayerNorm((16,), eps=1e-05, elementwise_affine=False)\n",
       "    )\n",
       "    (3): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=16, out_features=4, bias=False)\n",
       "            (query): Linear(in_features=16, out_features=4, bias=False)\n",
       "            (value): Linear(in_features=16, out_features=4, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=16, out_features=64, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=64, out_features=16, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln): LayerNorm((16,), eps=1e-05, elementwise_affine=False)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=16, out_features=66, bias=True)\n",
       "  (ln): LayerNorm((16,), eps=1e-05, elementwise_affine=False)\n",
       "  (conc_head): FeedForward(\n",
       "    (net): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=64, out_features=16, bias=True)\n",
       "      (3): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (cosine_similarity): CosineSimilarity()\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = conceptGPTv9().to(device)  # Initialize a model with the same architecture\n",
    "\n",
    "# Load the saved state dictionary\n",
    "model.load_state_dict(torch.load('models/conceptGPTv9_b4_t8_d16_h4_l4_lr0.0003_drop0.2_l2-0.01_k8_2024-02-02|02-54-22.pth'))\n",
    "# that's the better model of the two that I trained. The extra heads were useless tho\n",
    "\n",
    "# If you plan to continue training the model, switch to training mode\n",
    "#model.train()\n",
    "\n",
    "# If you only plan to do inference, switch to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou RQlacHtsoveoiloeeeudOBeg\n",
      "RUQaD LCMbh hsEget fon ndowaco!kony matBimmee hluphdes-Idori w ule ndt RJag\n"
     ]
    }
   ],
   "source": [
    "#%%time # to keep track of how long it takes\n",
    "input_str = \"JULIET:\\nO Romeo, Romeo! wherefore art thou R\" # the classic line\n",
    "context_tensor = torch.tensor([encode(input_str)], dtype=torch.long, device=device)\n",
    "output = model.generate(context_tensor, max_new_tokens=100)\n",
    "output_str = decode(output[0].tolist())\n",
    "print(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
