{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4be31fdd",
   "metadata": {},
   "source": [
    "# Next-Concept Prediction v11.2\n",
    "\n",
    "So the main plan here is to\n",
    "1. [x] turn minGemma into a kind of MatFormer\n",
    "2. [x] have the smallest level deal with tokens, medium with first-level concepts, and large with 2nd-level concepts\n",
    "3. [x] dynamically generate concept vectors based on regressive cosine similarity rather than storing huge vector combination vocabularies\n",
    "4. [x] allow lower levels to attend to the vector representations at the levels above them using cross-attention\n",
    "5. [x] ~~with inference, higher levels will run first, lower levels attend to them, and then each time a lowest level finishes its predictions of length config.combo then that value gets pushed up to the higher levels which now have to recalculate their predictions~~ -- ~~i'm an idiot, the code written for training actually does work perfectly well for performing inference. i mean it's not as efficient as it could be but it is peak quality given how the model was trained.~~ \n",
    "    - [ ] no i'm an idiot again. so i think when you pump it through it only predicts one concept vector ahead and then suddenly uses that for lower levels. this is made worse by the fact that i've been training it to do exactly that rather than training it to finish to the full context length just because i was concerned about dynamic RoPE. what i need to do is precompute RoPE, return get_batch() to its previous state where it always called a full length, and then mess with i guess Body to make it call our cosine similarity search thing and build out the full future sequence\n",
    "    - [ ] rather than actually use the raw regression outputs, we can use cosine similarity to search for the token combination at that level that is most similar to the current regression output, and then add that to the sequence for our next run of the decoder instead. That way the model is always working in-distribution. Would be interesting to see which one performs better. Also, if this option is chosen then config.noise_sd would be useless bc we'd no longer have to deal with lower levels attending to upper level concepts that aren't in-distribution. this is a similarly big project to the above bullet point; really they should be implemented at the same time\n",
    "6. [ ] I think the moving goalposts problem is likely too difficult for the model to have to deal with straight from the get-go. Rather, i should set it up to train the token level first and then progressively engage higher levels. i think all i have to do to make this happen is pull the loss additions out of the model and into the training loop, no? \n",
    "    - [ ] ~~honestly now that i'm thinking about it, it might make sense to pull the concept loss out entirely since the model can train just through the lowest level's backpropogation through crossMQA.~~ no that makes no sense because then how would i do autoregressive decoding of concepts\n",
    "    - [ ] Also, might it make sense to project the final output of a conceptual layer to the dimension size below? that way it's really focusing on the task at hand rather than having this entire second half of the vector who's goal is to just predict itself after starting from a random initialization. doing this would require that i mess with crossMQA. \n",
    "\n",
    "less important todo list\n",
    "- [x] clean up the primary forward() by putting things into their own classes\n",
    "- [ ] ~~change the get_batch() function to give us sequence lengths that are not multiples of config.combo so that the thing generalizes to prompts that aren't a perfect length. likely need to add a <|padding|> embedding vector for this to work well. should it be learnable or maybe set it to all zeros?~~ nvm i don't think that's a good idea bc i don't want to encourage the model to predict simpler concepts (concepts made up of fewer tokens/concepts) so i think instead i have to just hope that if they give me a prompt that's off number than it can generalize.\n",
    "    - [x] yoooo what if instead of padding the end i pad the beginning???? models tend to use the first embedding in the sequence as an attention sink anyways and that way it'd never be encouraged to predict dumb shit. ok i think this is the way to go. should be a simple edit to the combine embeddings class and then i can do the thing where i give varying sequence lengths. wait but how would that mess with what gets paid attention to? i think it doesn't matter with our current version of cross-attention. so just adjust the current implementation of padding and then the get_batch() function\n",
    "- [x] add an option to norm the initial residual state instead of scaling it by sqrt(embed_dim). would it be interesting to both norm and scale? for example, with cosine norm that would put the initial vectors on a sqrt(d) radius hypersphere centered at the origin i guess\n",
    "- [ ] add noise to the upper-triangular part of the cross-attention; maybe anothe option where this \"noise mask\" is stronger over the course of each row to represent the likely inaccuracy that the model will experience during inference for progressive concept embeddings. or alternatively, add a mask that only allows the model to see the concept it's currently working on, so kinda like a mask of diagonal 1's except with blocks of size config.combo rather than entries. this won't be necessary if i go down the route of replacing the model's guesses with the nearest cosine similarity concept combos.\n",
    "- [x] pump `training` variable throughout for stuff like dropout\n",
    "- [x] add a ~~config.verbose dictionary~~ debug/demonstrate logger\n",
    "- [ ] ~~switch the cross-attention to be inbetween self-attention and MLP (more like vaswani, which i assume is how encoder-decoder structures usually work). while we're making it more like vaswani, i should just let every layer use cross-attention instead of doing it in intervals. not sure if they used the same cross-attention mechanism weights at every layer or created an nn.ModuleList~~ \n",
    "    - [x] actually i'm thinking it shouldn't matter and doing this would require me to tear up `Layer` and make `Body` more important which i think isn't worth the effort. like it really shouldn't matter at all tbh. I should prolly set it to activate every layer tho.\n",
    "- [x] the code in the norms have gotten so similar it'd prolly be more visually appealing for me to combine them all into one object plus then i wouldn't have to be picky about where in the stack of cells `config` gets defined\n",
    "- [ ] write a more efficient inference algorithm. specifically, have each level predict config.combo vectors rather than 1 vector. this *i think* should be just as ideal in terms of accuracy but obviously more efficient \n",
    "\n",
    "further ideas for v11.3 (maybe call it 11.2b?):\n",
    "- [ ] do it without the matryoshka-ness\n",
    "    - [ ] add multiple <|bos|> tokens to put at the beginning of each sequence, one for each level. So that way the model can know what level it's working with and have somewhere to sink attention.\n",
    "- [ ] setup `effective_seq_len_mult` and `seq_len_list` and a bunch of downstream stuff like weird batching at lower sub-levels to allow for hella long effective context lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c9f55d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my virtual environments are rarely properly connected to jupyter so this fixes that\n",
    "import sys\n",
    "import os\n",
    "current_dir = os.getcwd()  # Get the current working directory\n",
    "venv_dir = os.path.join(current_dir, 'venv') \n",
    "python_version = str(sys.version_info.major) + '.' + str(sys.version_info.minor)\n",
    "site_packages_path = os.path.join(venv_dir, 'lib', 'python' + python_version, 'site-packages')\n",
    "sys.path.append(site_packages_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7f31c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# imports for the debugging/demonstration setup\n",
    "import functools\n",
    "import inspect\n",
    "\n",
    "# imports for the tokenizer\n",
    "from tokenizer import SimpleTokenizer, loaded_stoi, loaded_merges\n",
    "\n",
    "# Imports used for the config\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "# Imports used for the model\n",
    "import re\n",
    "from typing import Any, List, Sequence, Tuple, Union\n",
    "\n",
    "# used for training\n",
    "import random\n",
    "import time\n",
    "\n",
    "# used to save & load models\n",
    "import json\n",
    "from dataclasses import asdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85662081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_io(func):\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(self, *args, **kwargs):\n",
    "        if not self.logging_enabled:\n",
    "            return func(self, *args, **kwargs)\n",
    "\n",
    "        def log_item(item, name, level=0, is_root=False):\n",
    "            indent = \"    \" * level\n",
    "            if isinstance(item, torch.Tensor):\n",
    "                print(f\"{indent}Tensor '{name}' shape: {item.shape}\")\n",
    "            elif isinstance(item, tuple):\n",
    "                if is_root and level == 0:\n",
    "                    # Root level tuple, don't print it as a tuple unless it's a \"true\" tuple\n",
    "                    for idx, sub_item in enumerate(item):\n",
    "                        log_item(sub_item, f\"{name}[{idx}]\", level)\n",
    "                else:\n",
    "                    print(f\"{indent}Tuple '{name}':\")\n",
    "                    for idx, sub_item in enumerate(item):\n",
    "                        log_item(sub_item, f\"{name}[{idx}]\", level + 1)\n",
    "            elif isinstance(item, int):\n",
    "                print(f\"{indent}Integer '{name}': Value={item}\")\n",
    "            else:\n",
    "                print(f\"{indent}Other-type '{name}': Type={type(item).__name__}, Value={item}\")\n",
    "\n",
    "        print(f\"\\n{'='*10}Entering {self.__class__.__name__}.{func.__name__}{'='*10}\")\n",
    "        print(\"Inputs:\")\n",
    "        arg_names = inspect.getfullargspec(func).args[1:]  # Excluding 'self'\n",
    "        arg_values = args + tuple(kwargs.values())\n",
    "        for name, value in zip(arg_names, arg_values):\n",
    "            log_item(value, name)\n",
    "\n",
    "        result = func(self, *args, **kwargs)\n",
    "        print(\"\\nOutputs:\")\n",
    "        if isinstance(result, tuple):\n",
    "            log_item(result, \"output\", is_root=True)\n",
    "        else:\n",
    "            log_item(result, \"output\")\n",
    "\n",
    "        print(f\"{'='*10}Exiting {self.__class__.__name__}.{func.__name__}{'='*10}\")\n",
    "        return result\n",
    "    return wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a73fa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# and the tokenizer\n",
    "tokenizer = SimpleTokenizer(loaded_stoi, loaded_merges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30979583",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18206ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(vocab_size=128, max_seq_len=256, num_layers=6, sa_q_heads=2, sa_kv_heads=1, embed_dim=256, mlp_multiplier=4, sa_head_dim=64, theta=100.0, dropout_rate=0.05, norm_init_embed=True, scale_init_embed=True, init_scale_degree=2, norm_final_embed=True, norm_affine=False, level_loss_weight=2.0, ca_connections=6, ca_q_heads=2, ca_kv_heads=1, shared_ca=False, ca_use_RoPE=False, ca_noise_sd=None)\n",
      "embedding dimension size of each model: [128, 256]\n",
      "attention head size of each model: [32, 64]\n",
      "sequence length of each model: [256, 64]\n",
      "base head dimension of cross-attention to higher levels: 32\n",
      "head size of each cross-attention connection: [32]\n",
      "cross-attention to a higher level will be applied every 1 layers\n",
      "loss discounts starting from lowest level: [1.0, 2.0]\n"
     ]
    }
   ],
   "source": [
    "@dataclass # a class meant specifically to just hold data\n",
    "class Config:\n",
    "    \"\"\" \n",
    "    The default configuration & hyperparameters for my next-concept predictor\n",
    "    \"\"\"\n",
    "    ### boring hyperparameters\n",
    "    vocab_size: int = tokenizer.vocab_len\n",
    "    max_seq_len: int = 256\n",
    "    num_layers: int = 6\n",
    "    sa_q_heads: int = 2\n",
    "    sa_kv_heads: int = 1\n",
    "    embed_dim: int = 256\n",
    "    mlp_multiplier: int = 4\n",
    "    sa_head_dim: int = 64\n",
    "    theta: float = 100.0\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    dropout_rate: float = 0.05\n",
    "    eps = 1e-6\n",
    "\n",
    "    ### Normalization related (also boring)\n",
    "    norm_init_embed: bool = True # whether or not to normalize right after turning input indices into the first residual state\n",
    "    scale_init_embed: bool = True # whether or not to scale the initial residual state by the nth root of embed_dim. default True\n",
    "    init_scale_degree: int = 2 # what nth root to take of embed_dim to use as the scale for the initial residual. default 2\n",
    "    norm_final_embed: bool = True # whether or not to normalize the embeddings at the output layer before multiplying for logits\n",
    "    norm_affine: bool = False # whether norms should have a linear & bias after them\n",
    "    norm_type = \"CosineNorm\"  # Options are RMSNorm, CosineNorm and LayerNorm\n",
    "\n",
    "    ### MatFormer+ related\n",
    "    levels = 2\n",
    "    split = 2\n",
    "    @property\n",
    "    def embed_dim_list(self):\n",
    "        return [self.embed_dim // (self.split ** (i-1)) for i in range(self.levels, 0, -1)]\n",
    "    @property\n",
    "    def sa_head_dim_list(self):\n",
    "        return [self.sa_head_dim // (self.split ** (i-1)) for i in range(self.levels, 0, -1)]\n",
    "\n",
    "    ### Concept embedding vectors\n",
    "    combo = 4\n",
    "    concept_loss = \"cos\" # options are 'mae', 'mse', and 'cos'(default)\n",
    "    @property\n",
    "    def seq_len_list(self):\n",
    "        return [(self.max_seq_len // (self.combo ** (i-1))) for i in range(1, self.levels + 1)]\n",
    "    # how much to discount each higher level in the loss function compared to the last. gives the smaller models more say in the gradient\n",
    "    level_loss_weight: float = 2.0\n",
    "\n",
    "    ### Dualcoder cross-attention\n",
    "    # how many times to do the cross-attention\n",
    "    ca_connections: int = num_layers # for simplicity let's just keep them the same & not mess with it\n",
    "    ca_q_heads: int = sa_q_heads\n",
    "    ca_kv_heads: int = sa_kv_heads\n",
    "    shared_ca: bool = False # True: same weights shared for each ca connection. False: separarely instantiated module for each connection\n",
    "    ca_use_RoPE: bool = False # True: expands out k & v tensors to be usable with rope. False: leaves k & v same size but no positional encodings\n",
    "    ca_noise_sd: float = None # float: adds gaussian noise with that sd to higher-level concept vecs. None: no noise added\n",
    "    # i don't think i can change this one because\n",
    "    @property\n",
    "    def ca_head_dim(self): \n",
    "        return self.sa_head_dim // self.split \n",
    "    @property\n",
    "    def ca_head_dim_list(self):\n",
    "        return [self.ca_head_dim // (self.split ** (i-1)) for i in range(self.levels-1, 0, -1)]\n",
    "    @property\n",
    "    def ca_interval(self):\n",
    "        return self.num_layers // self.ca_connections\n",
    "\n",
    "    ### assertions\n",
    "    assert sa_q_heads % sa_kv_heads == 0, 'the number of query heads must be divisible by the number of key-value heads in self-attention'\n",
    "    assert ca_q_heads % ca_kv_heads == 0, 'the number of query heads must be divisible by the number of key-value heads in cross-attention'\n",
    "    assert max_seq_len % (split ** levels) == 0, 'your context length must be a multiple of split ** levels'\n",
    "    assert embed_dim % (split ** levels) == 0, 'your embedding dimension must be a multiple of split ** levels'\n",
    "    assert sa_head_dim % (split ** levels) == 0, 'your self-attentionhead dimension must be divisible by split ** levels'\n",
    "    #assert self.ca_head_dim % (split ** levels) == 0, 'your cross-attentionhead dimension must be divisible by split ** levels'\n",
    "    assert num_layers % ca_connections == 0, 'the total number of layers must be divisible by the number of cross-attention connections'        \n",
    "        \n",
    "config = Config()\n",
    "print(config)\n",
    "print(f\"embedding dimension size of each model: {config.embed_dim_list}\")\n",
    "print(f\"attention head size of each model: {config.sa_head_dim_list}\")\n",
    "print(f\"sequence length of each model: {config.seq_len_list}\")\n",
    "print(f\"base head dimension of cross-attention to higher levels: {config.ca_head_dim}\")\n",
    "print(f\"head size of each cross-attention connection: {config.ca_head_dim_list}\")\n",
    "print(f\"cross-attention to a higher level will be applied every {config.ca_interval} layers\")\n",
    "print(f\"loss discounts starting from lowest level: {[config.level_loss_weight**i for i in range(config.levels)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ea723b",
   "metadata": {},
   "source": [
    "# Norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5ee62ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Norm(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.eps = config.eps\n",
    "        self.affine = config.norm_affine\n",
    "        self.dropout_rate = config.dropout_rate\n",
    "        self.type = config.norm_type\n",
    "\n",
    "        # Initialize weight and bias parameters for affine transformation\n",
    "        # We start with ones for weight to keep the original scale initially, and zeros for bias.\n",
    "        self.w = nn.Parameter(torch.ones(config.embed_dim))\n",
    "        self.b = nn.Parameter(torch.zeros(config.embed_dim))\n",
    "        \n",
    "        self.logging_enabled = False\n",
    "    def enable_logging(self):\n",
    "        self.logging_enabled = True\n",
    "    def disable_logging(self):\n",
    "        self.logging_enabled = False\n",
    "\n",
    "    @log_io\n",
    "    def forward(self, x: torch.Tensor, training: bool = False) -> torch.Tensor:\n",
    "        # Normalize the input tensor\n",
    "        if self.type == \"CosineNorm\":\n",
    "            x = self.CosineNorm(x)\n",
    "        elif self.type == \"LayerNorm\":\n",
    "            x = self.LayerNorm(x)\n",
    "        else: # defaults to RMSNorm bc that's the most commonly used nowadays\n",
    "            x = self.RMSNorm(x)\n",
    "\n",
    "        if self.affine: # Optionally apply the affine transformation with splicing\n",
    "            w, b = self.splice_affine(self.w, self.b, x.shape[-1])\n",
    "            x = x * w + b\n",
    "            # and dropout the linear projection if we're training\n",
    "            x = F.dropout(x, p=self.dropout_rate, training=training) \n",
    "            \n",
    "        return x \n",
    "\n",
    "    @log_io\n",
    "    def CosineNorm(self, x):\n",
    "        # normalize x by dividing by its L2 norm along the last dimension.\n",
    "        # this places x on the unit hypersphere centered at the origin\n",
    "        # Add a small constant to the denominator to avoid division by zero.\n",
    "        return x / torch.norm(x, p=2, dim=-1, keepdim=True).clamp(min=self.eps)\n",
    "\n",
    "    @log_io\n",
    "    def LayerNorm(self, x):\n",
    "        # normalize x by subtracting by its mean then dividing by its variance\n",
    "        # this places x on a hypersphere of radius sqrt(dimension) centered at the origin\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        return (x - mean) / torch.sqrt(var + self.eps)\n",
    "\n",
    "    @log_io\n",
    "    def RMSNorm(self, x):\n",
    "        # normalize x by dividing by its root-mean-square along the last dimension\n",
    "        # this places x on a hypersphere of radius sqrt(dimension) with no certain center\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    @log_io\n",
    "    def splice_affine(self, weight, bias, d_i):\n",
    "        return weight[:d_i], bias[:d_i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4d5ae3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### demonstration/debugging\n",
    "I've setup these little snippets after each nn.Module to help you see what's happening and for my own debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5ec58b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering Norm.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 256, 64])\n",
      "\n",
      "==========Entering Norm.CosineNorm==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 256, 64])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 256, 64])\n",
      "==========Exiting Norm.CosineNorm==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 256, 64])\n",
      "==========Exiting Norm.forward==========\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of RMSNorm\n",
    "module = Norm(config)\n",
    "\n",
    "# Initially, logging is disabled\n",
    "# Enable logging\n",
    "module.enable_logging()\n",
    "\n",
    "# Call the forward method - logging will occur\n",
    "output = module(torch.randn(32, config.max_seq_len, config.embed_dim // config.combo))\n",
    "\n",
    "# Disable logging. \n",
    "# This isn't actually necessary since we won't be using this object again but that's how you'd do it\n",
    "module.disable_logging()\n",
    "\n",
    "# clearing up ram jic we're training later\n",
    "del module, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f6e32f",
   "metadata": {},
   "source": [
    "# RoPE\n",
    "\n",
    "i like the idea of pre-computing RoPE embeddings but at this point i don't think it's worth the effort bc i'd have to not only use this code but also pipe the two instantiations of this class through from `Body` all the way to `selfMQA` and `crossMQA` and I'm not even sure if it matters. I really should learn more about RoPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5316c0e",
   "metadata": {},
   "source": [
    "class RoPE(nn.Module):\n",
    "    def __init__(self, \n",
    "                 dim: int, \n",
    "                 max_seq_len:int = config.max_seq_len, \n",
    "                 device: str = config.device):\n",
    "        super().__init__()\n",
    "        # Validate that dim is even since we split it by 2 for real and imaginary parts\n",
    "        if dim % 2 != 0: raise ValueError(\"Dimension 'dim' must be an even number.\")\n",
    "            \n",
    "        # Precompute frequencies based on configuration\n",
    "        theta = config.theta if hasattr(config, 'theta') else 10000.0\n",
    "        \n",
    "        freqs = 1.0 / (theta ** (torch.arange(0, config.dim, 2, device=config.device).float() / config.dim))\n",
    "        t = torch.arange(config.max_seq_len, device=config.device)\n",
    "        freqs = torch.outer(t, freqs).to(config.device).float()\n",
    "        \n",
    "        # Register as buffer to prevent gradient tracking\n",
    "        self.register_buffer('freqs_cis', torch.polar(torch.ones_like(freqs), freqs)) # complex64\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply rotary embeddings to the input tensor\n",
    "        x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "        x_out = torch.view_as_real(x_ * self.freqs_cis.unsqueeze(0)).type_as(x)\n",
    "        x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "        x_out = x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81b8b88",
   "metadata": {},
   "source": [
    "\\# demonstration/debugging\n",
    "module = RoPE(dim=10)\n",
    "module.enable_logging()\n",
    "output = module(torch.randn(, 5))\n",
    "del module, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "741b4837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RoPE(x: torch.Tensor, dim: int, theta: float = 10000.0) -> torch.Tensor:\n",
    "    \"\"\"Applies the rotary embedding to the inputted query or key tensor\"\"\"\n",
    "    # Validate that dim is even since we split it by 2 for real and imaginary parts\n",
    "    if dim % 2 != 0: raise ValueError(\"Dimension 'dim' must be an even number.\")\n",
    "            \n",
    "    # Get sequence length\n",
    "    seq_len = x.size(1)\n",
    "    device = x.device\n",
    "\n",
    "    # Dynamically compute frequency cis based on the input sequence length\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2, device=device).float() / dim))\n",
    "    t = torch.arange(seq_len, device=device)\n",
    "    freqs = torch.outer(t, freqs).float()\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    # it's important to train on a wide variety of sequence lengths within your context length so that the model learns to generalize\n",
    "\n",
    "    # Apply rotary embeddings to the input tensor\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    x_out = x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "    return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e524f063",
   "metadata": {},
   "source": [
    "# selfMQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1547ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class selfMQA(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.sa_q_heads = config.sa_q_heads\n",
    "        self.sa_kv_heads = config.sa_kv_heads\n",
    "        assert self.sa_q_heads % self.sa_kv_heads == 0\n",
    "        self.num_queries_per_kv = self.sa_q_heads // self.sa_kv_heads\n",
    "\n",
    "        self.embed_dim = config.embed_dim\n",
    "        self.sa_head_dim = config.sa_head_dim\n",
    "        self.theta = config.theta\n",
    "        self.dropout_rate = config.dropout_rate\n",
    "\n",
    "        self.Wqkv = nn.Parameter(torch.Tensor(self.embed_dim, (self.sa_q_heads + 2 * self.sa_kv_heads) * self.sa_head_dim))\n",
    "        nn.init.uniform_(self.Wqkv, -((1 / self.embed_dim) ** 0.5), (1 / self.embed_dim) ** 0.5)\n",
    "\n",
    "        self.Wo = nn.Parameter(torch.Tensor(self.sa_q_heads * self.sa_head_dim, self.embed_dim))\n",
    "        nn.init.uniform_(self.Wo, -((1 / (self.sa_q_heads * self.sa_head_dim)) ** 0.5), (1 / (self.sa_q_heads * self.sa_head_dim)) ** 0.5)\n",
    "\n",
    "        # for our attention mask we'll create a boolean mask that'll later be turned into large negative values\n",
    "        self.mask = torch.tril(torch.ones((config.max_seq_len, config.max_seq_len), dtype=torch.uint8)\n",
    "                              ).view(1, 1, config.max_seq_len, config.max_seq_len).to(dtype=torch.bool)\n",
    "        \n",
    "        self.logging_enabled = False\n",
    "    def enable_logging(self):\n",
    "        self.logging_enabled = True\n",
    "    def disable_logging(self):\n",
    "        self.logging_enabled = False\n",
    "\n",
    "    @log_io\n",
    "    def forward(self, x: torch.Tensor, training: bool = False) -> torch.Tensor:\n",
    "        # Extracts batch size and input sequence length from the hidden states tensor.\n",
    "        batch_size, input_len, d_i = x.shape\n",
    "        h_i = self.sa_head_dim // (self.embed_dim // d_i)\n",
    "\n",
    "        # splicing our primary projection to get the correct sub-matrices\n",
    "        Wq, Wk, Wv, Wo = self.weight_splicing(self.Wqkv, self.Wo, d_i, h_i)\n",
    "        # technically self.weight_splicing has access to self.Wqkv & Wo but this way our debugger can see them\n",
    "\n",
    "        # Applies the linear projection to the hidden state to retrieve our q, k & v projections\n",
    "        xq = F.dropout(x @ Wq, p=self.dropout_rate, training=training) # also dropout if we're training\n",
    "        xk = F.dropout(x @ Wk, p=self.dropout_rate, training=training)\n",
    "        xv = F.dropout(x @ Wv, p=self.dropout_rate, training=training)\n",
    "\n",
    "        # Reshapes each of the Q, K, and V tensors to separate the heads and align the dimensions for attention operations.\n",
    "        xq = xq.view(batch_size, -1, self.sa_q_heads, h_i)\n",
    "        xk = xk.view(batch_size, -1, self.sa_kv_heads, h_i)\n",
    "        xv = xv.view(batch_size, -1, self.sa_kv_heads, h_i)\n",
    "\n",
    "        # Applies rotary positional embeddings to queries and keys to incorporate positional information.\n",
    "        xq = self.RoPE(xq, h_i)\n",
    "        xk = self.RoPE(xk, h_i)\n",
    "\n",
    "        # If the number of KV heads is different from the number of query heads, adjusts keys and values to match the query heads count.\n",
    "        if self.sa_kv_heads != self.sa_q_heads:\n",
    "            xk = self.match_headcount(xk) # [batch_size, input_len, n_local_heads, sa_head_dim]\n",
    "            xv = self.match_headcount(xv)\n",
    "\n",
    "        # Transposes Q, K, and V tensors to align them for the batch matrix multiplication in attention calculation.\n",
    "        xq = xq.transpose(1, 2) # [batch_size, n_local_heads, input_len, sa_head_dim]\n",
    "        xk = xk.transpose(1, 2)\n",
    "        xv = xv.transpose(1, 2)\n",
    "\n",
    "        # Calculates attention scores by performing a batch matrix multiplication between queries and keys, followed by scaling.\n",
    "        logits = self.attend(xq, xk, h_i) # [batch_size, n_local_heads, input_len, input_len]\n",
    "        \n",
    "        # Applies the lower-triangular mask to the attention logits\n",
    "        logits = self.apply_mask(logits, input_len)\n",
    "\n",
    "        # applies values to get final output\n",
    "        output = self.calc_output(logits, xv, batch_size, input_len) \n",
    "\n",
    "        # Applies the final linear projection to the attention output, mapping it back to d_i\n",
    "        return F.dropout(output @ Wo, p=self.dropout_rate, training=training) # also dropout if we're training\n",
    "\n",
    "    @log_io\n",
    "    def weight_splicing(self, Wqkv, Wo, d_i, h_i):\n",
    "        Wq, Wk, Wv = Wqkv.split([self.sa_q_heads * self.sa_head_dim,\n",
    "                                 self.sa_kv_heads * self.sa_head_dim,\n",
    "                                 self.sa_kv_heads * self.sa_head_dim],dim = -1)\n",
    "        Wq = torch.cat([Wq[:d_i, j*self.sa_head_dim:j*self.sa_head_dim + h_i] for j in range(self.sa_q_heads)], dim = 1)\n",
    "        Wk = torch.cat([Wk[:d_i, j*self.sa_head_dim:j*self.sa_head_dim + h_i] for j in range(self.sa_kv_heads)], dim = 1)\n",
    "        Wv = torch.cat([Wv[:d_i, j*self.sa_head_dim:j*self.sa_head_dim + h_i] for j in range(self.sa_kv_heads)], dim = 1)\n",
    "        Wo = torch.cat([Wo[j*self.sa_head_dim :j*self.sa_head_dim + h_i, :d_i] for j in range(self.sa_q_heads)], dim=0)\n",
    "        return Wq, Wk, Wv, Wo\n",
    "\n",
    "    @log_io\n",
    "    def RoPE(self, x, h_i):\n",
    "        return RoPE(x, h_i, self.theta)\n",
    "\n",
    "    @log_io\n",
    "    def match_headcount(self, xn):\n",
    "        return torch.repeat_interleave(xn, self.num_queries_per_kv, dim=2)\n",
    "\n",
    "    @log_io\n",
    "    def attend(self, xq, xk, h_i):\n",
    "        return torch.matmul(xq, xk.transpose(2, 3)) * (h_i ** -0.5)\n",
    "        \n",
    "    @log_io\n",
    "    def apply_mask(self, logits, input_len):\n",
    "        return torch.where(self.mask[..., :input_len, :input_len].expand_as(logits),\n",
    "                           logits,\n",
    "                           torch.tensor(-1e30, device=logits.device, dtype=logits.dtype))\n",
    "    \n",
    "    @log_io\n",
    "    def calc_output(self, logits, xv, batch_size, input_len):\n",
    "        # Applies softmax to the logits to obtain attention probabilities\n",
    "        scores = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Computes the weighted sum of values based on the attention scores to obtain the output of the attention mechanism.\n",
    "        output = scores @ xv # [batch_size, n_local_heads, input_len, sa_head_dim]\n",
    "\n",
    "        # Reshapes the attention output to match the expected output dimensions, combining the heads back into the hidden dimension.\n",
    "        return output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de1beb5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### demonstration/debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a53abe27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering selfMQA.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 256, 256])\n",
      "\n",
      "==========Entering selfMQA.weight_splicing==========\n",
      "Inputs:\n",
      "Tensor 'Wqkv' shape: torch.Size([256, 256])\n",
      "Tensor 'Wo' shape: torch.Size([128, 256])\n",
      "Integer 'd_i': Value=256\n",
      "Integer 'h_i': Value=64\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([256, 128])\n",
      "Tensor 'output[1]' shape: torch.Size([256, 64])\n",
      "Tensor 'output[2]' shape: torch.Size([256, 64])\n",
      "Tensor 'output[3]' shape: torch.Size([128, 256])\n",
      "==========Exiting selfMQA.weight_splicing==========\n",
      "\n",
      "==========Entering selfMQA.RoPE==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 256, 2, 64])\n",
      "Integer 'h_i': Value=64\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 256, 2, 64])\n",
      "==========Exiting selfMQA.RoPE==========\n",
      "\n",
      "==========Entering selfMQA.RoPE==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 256, 1, 64])\n",
      "Integer 'h_i': Value=64\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 256, 1, 64])\n",
      "==========Exiting selfMQA.RoPE==========\n",
      "\n",
      "==========Entering selfMQA.match_headcount==========\n",
      "Inputs:\n",
      "Tensor 'xn' shape: torch.Size([32, 256, 1, 64])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 256, 2, 64])\n",
      "==========Exiting selfMQA.match_headcount==========\n",
      "\n",
      "==========Entering selfMQA.match_headcount==========\n",
      "Inputs:\n",
      "Tensor 'xn' shape: torch.Size([32, 256, 1, 64])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 256, 2, 64])\n",
      "==========Exiting selfMQA.match_headcount==========\n",
      "\n",
      "==========Entering selfMQA.attend==========\n",
      "Inputs:\n",
      "Tensor 'xq' shape: torch.Size([32, 2, 256, 64])\n",
      "Tensor 'xk' shape: torch.Size([32, 2, 256, 64])\n",
      "Integer 'h_i': Value=64\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 2, 256, 256])\n",
      "==========Exiting selfMQA.attend==========\n",
      "\n",
      "==========Entering selfMQA.apply_mask==========\n",
      "Inputs:\n",
      "Tensor 'logits' shape: torch.Size([32, 2, 256, 256])\n",
      "Integer 'input_len': Value=256\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 2, 256, 256])\n",
      "==========Exiting selfMQA.apply_mask==========\n",
      "\n",
      "==========Entering selfMQA.calc_output==========\n",
      "Inputs:\n",
      "Tensor 'logits' shape: torch.Size([32, 2, 256, 256])\n",
      "Tensor 'xv' shape: torch.Size([32, 2, 256, 64])\n",
      "Integer 'batch_size': Value=32\n",
      "Integer 'input_len': Value=256\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 256, 128])\n",
      "==========Exiting selfMQA.calc_output==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 256, 256])\n",
      "==========Exiting selfMQA.forward==========\n"
     ]
    }
   ],
   "source": [
    "module = selfMQA(config)\n",
    "module.enable_logging()\n",
    "output = module(torch.randn(32,config.max_seq_len,config.embed_dim))\n",
    "del module, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e486b9c5",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "edc5018a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_dim: int,\n",
    "                 mlp_multiplier: int,\n",
    "                 dropout_rate: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.mlp_multiplier = mlp_multiplier\n",
    "        self.hidden_size = embed_dim * mlp_multiplier\n",
    "\n",
    "        # the gate\n",
    "        self.Wgate = nn.Parameter(torch.Tensor(embed_dim, self.hidden_size))\n",
    "        self.Bgate = nn.Parameter(torch.Tensor(self.hidden_size))\n",
    "        torch.nn.init.uniform_(self.Wgate, -((1/embed_dim) ** 0.5), (1/embed_dim) ** 0.5)\n",
    "        torch.nn.init.uniform_(self.Bgate, -((1/embed_dim) ** 0.5), (1/embed_dim) ** 0.5)\n",
    "\n",
    "        # the up projection\n",
    "        self.Wup = nn.Parameter(torch.Tensor(embed_dim, self.hidden_size))\n",
    "        self.Bup = nn.Parameter(torch.Tensor(self.hidden_size))\n",
    "        torch.nn.init.uniform_(self.Wup, -((1/embed_dim) ** 0.5), (1/embed_dim) ** 0.5)\n",
    "        torch.nn.init.uniform_(self.Bup, -((1/embed_dim) ** 0.5), (1/embed_dim) ** 0.5)\n",
    "\n",
    "        # the down projection\n",
    "        self.Wdown = nn.Parameter(torch.Tensor(self.hidden_size, embed_dim))\n",
    "        self.Bdown = nn.Parameter(torch.Tensor(embed_dim))\n",
    "        torch.nn.init.uniform_(self.Wdown, -((1/self.hidden_size) ** 0.5), (1/self.hidden_size) ** 0.5)\n",
    "        torch.nn.init.uniform_(self.Bdown, -((1/self.hidden_size) ** 0.5), (1/self.hidden_size) ** 0.5)\n",
    "        \n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.logging_enabled = False\n",
    "    def enable_logging(self):\n",
    "        self.logging_enabled = True\n",
    "    def disable_logging(self):\n",
    "        self.logging_enabled = False\n",
    "        \n",
    "    @log_io\n",
    "    def forward(self,\n",
    "                x: torch.Tensor,\n",
    "                training: bool = False\n",
    "               ) -> torch.Tensor:\n",
    "        d_i = x.shape[-1]\n",
    "        gate = x @ self.Wgate[:d_i, :d_i * self.mlp_multiplier] + self.Bgate[:d_i * self.mlp_multiplier]\n",
    "        up = x @ self.Wup[:d_i, :d_i * self.mlp_multiplier] + self.Bup[:d_i * self.mlp_multiplier]\n",
    "        fuse = F.dropout(F.gelu(gate) * up, p=self.dropout_rate, training=training)\n",
    "        down = fuse @ self.Wdown[:d_i * self.mlp_multiplier, :d_i] + self.Bdown[:d_i]\n",
    "        return F.dropout(down, p=self.dropout_rate, training=training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abaabb6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### demonstration/debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e98dbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering MLP.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 256, 256])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 256, 256])\n",
      "==========Exiting MLP.forward==========\n"
     ]
    }
   ],
   "source": [
    "module = MLP(config.embed_dim, config.mlp_multiplier, config.dropout_rate)\n",
    "module.enable_logging()\n",
    "output = module(torch.randn(32,config.max_seq_len,config.embed_dim))\n",
    "del module, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe3552c",
   "metadata": {},
   "source": [
    "# Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e30fc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mqa = selfMQA(config)\n",
    "        self.mlp = MLP(config.embed_dim, config.mlp_multiplier, config.dropout_rate)\n",
    "\n",
    "        # Initialize normalization layers using the class reference from config\n",
    "        self.pre_mqa_norm = Norm(config)\n",
    "        self.post_mqa_norm = Norm(config)\n",
    "        self.pre_mlp_norm = Norm(config)\n",
    "        self.post_mlp_norm = Norm(config)\n",
    "        \n",
    "        self.logging_enabled = False\n",
    "    def enable_logging(self):\n",
    "        self.logging_enabled = True\n",
    "    def disable_logging(self):\n",
    "        self.logging_enabled = False\n",
    "\n",
    "    @log_io\n",
    "    def forward(self, \n",
    "                x: torch.Tensor,\n",
    "                training: bool = False\n",
    "               ) -> torch.Tensor:\n",
    "        x = x + self.mqa_connection(x, training)\n",
    "        x = x + self.mlp_connection(x, training)\n",
    "        return x\n",
    "\n",
    "    @log_io\n",
    "    def mqa_connection(self, x, training):\n",
    "        return self.post_mqa_norm(self.mqa(self.pre_mqa_norm(x, training), training), training)\n",
    "\n",
    "    @log_io\n",
    "    def mlp_connection(self, x, training):\n",
    "        return self.post_mlp_norm(self.mlp(self.pre_mlp_norm(x, training), training), training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5315f6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### demonstration/debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e125ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering Layer.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 256, 256])\n",
      "\n",
      "==========Entering Layer.mqa_connection==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 256, 256])\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 256, 256])\n",
      "==========Exiting Layer.mqa_connection==========\n",
      "\n",
      "==========Entering Layer.mlp_connection==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 256, 256])\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 256, 256])\n",
      "==========Exiting Layer.mlp_connection==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 256, 256])\n",
      "==========Exiting Layer.forward==========\n"
     ]
    }
   ],
   "source": [
    "module = Layer(config)\n",
    "module.enable_logging()\n",
    "#module.pre_mqa_norm.enable_logging()\n",
    "#module.post_mqa_norm.enable_logging()\n",
    "#module.pre_mlp_norm.enable_logging()\n",
    "#module.post_mlp_norm.enable_logging()\n",
    "output = module(torch.randn(32,config.max_seq_len,config.embed_dim))\n",
    "del module, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c1d15c",
   "metadata": {},
   "source": [
    "# crossMQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7251408e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class crossMQA(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ca_q_heads = config.ca_q_heads\n",
    "        self.ca_kv_heads = config.ca_kv_heads\n",
    "        assert self.ca_q_heads % self.ca_kv_heads == 0\n",
    "        self.num_queries_per_kv = self.ca_q_heads // self.ca_kv_heads\n",
    "\n",
    "        self.embed_dim = config.embed_dim\n",
    "        self.ca_head_dim = config.ca_head_dim\n",
    "        self.sa_head_dim = config.sa_head_dim # used only for an assertion to make sure sizes will fit\n",
    "        self.combo = config.combo # used only for an assertion to make sure sizes will fit\n",
    "        self.split = config.split # used only for an assertion to make sure sizes will fit\n",
    "        self.theta = config.theta\n",
    "        self.use_RoPE = config.ca_use_RoPE\n",
    "        self.noise_sd = config.ca_noise_sd\n",
    "        self.dropout_rate = config.dropout_rate\n",
    "\n",
    "        self.Wqkv = nn.Parameter(torch.Tensor(self.embed_dim, (self.ca_q_heads + 2 * self.ca_kv_heads) * self.ca_head_dim))\n",
    "        nn.init.uniform_(self.Wqkv, -((1 / self.embed_dim) ** 0.5), (1 / self.embed_dim) ** 0.5)\n",
    "        \n",
    "        self.Wo = nn.Parameter(torch.Tensor(self.ca_q_heads * self.ca_head_dim, self.embed_dim))\n",
    "        nn.init.uniform_(self.Wo, -((1 / (self.ca_q_heads * self.ca_head_dim)) ** 0.5), (1 / (self.ca_q_heads * self.ca_head_dim)) ** 0.5)\n",
    "\n",
    "        # for now we'll try no attention mask and then maybe (probably) edit that later \n",
    "        \n",
    "        self.logging_enabled = False\n",
    "    def enable_logging(self):\n",
    "        self.logging_enabled = True\n",
    "    def disable_logging(self):\n",
    "        self.logging_enabled = False\n",
    "\n",
    "    @log_io\n",
    "    def forward(self, \n",
    "                x: torch.Tensor, # the lower level tensor, sometimes a resid state full of tokens & sometimes concepts\n",
    "                c: torch.Tensor, # the upper level tensor, always a resid state full of concept vecs\n",
    "                training: bool = False\n",
    "               ) -> torch.Tensor:\n",
    "        # optionally adds some random gaussian noise to the upper-level concepts to simulate how bad they'll prolly be\n",
    "        # i think this should only be implemented during training, but imma have to go & spread that bool througout later\n",
    "        # this version does the entire txt matrix but later i'd like switch it to just upper-triangular\n",
    "        if self.noise_sd is not None:\n",
    "            noise = torch.randn_like(c, requires_grad = False)\n",
    "            c = c + noise * self.noise_sd\n",
    "        \n",
    "        # Extracts batch size and input sequence length from the hidden states tensor.\n",
    "        batch_size, input_len_x, xd_i = x.shape\n",
    "        batch_size_c, input_len_c, cd_i = c.shape\n",
    "        assert batch_size == batch_size_c\n",
    "        \n",
    "        # finds the appropriate head dimension and ensures that they match\n",
    "        h_i = self.ca_head_dim // (self.embed_dim // cd_i)\n",
    "        assert h_i == self.sa_head_dim // (self.embed_dim // xd_i), 'head_dim was not same bw levels in cross attention'\n",
    "        \n",
    "        # splicing our projection to get the correct sub-matrices\n",
    "        Wq, Wk, Wv, Wo = self.weight_splicing(self.Wqkv, self.Wo, xd_i, cd_i, h_i)\n",
    "\n",
    "        # Applies the linear projection to the hidden state to retrieve our q, k & v projections\n",
    "        xq = F.dropout(x @ Wq, p=self.dropout_rate, training=training) # also applies dropout if we're training\n",
    "        ck = F.dropout(c @ Wk, p=self.dropout_rate, training=training)\n",
    "        cv = F.dropout(c @ Wv, p=self.dropout_rate, training=training)\n",
    "\n",
    "        # Reshapes each of the Q, K, and V tensors to separate the heads and align the dimensions for attention operations.\n",
    "        xq = xq.view(batch_size, -1, self.ca_q_heads, h_i)\n",
    "        ck = ck.view(batch_size, -1, self.ca_kv_heads, h_i)\n",
    "        cv = cv.view(batch_size, -1, self.ca_kv_heads, h_i)\n",
    "\n",
    "        # IF we want to use RoPE (doesn't fully make sense to)\n",
    "        if self.use_RoPE:\n",
    "            expand = input_len_x // input_len_c\n",
    "            ck = ck.repeat_interleave(expand, dim=1) \n",
    "            cv = cv.repeat_interleave(expand, dim=1) # values need to be expanded for their use later on if we do this\n",
    "\n",
    "            # Applies rotary positional embeddings to queries and keys to incorporate positional information.\n",
    "            xq = self.RoPE(xq, h_i) \n",
    "            ck = self.RoPE(ck, h_i)\n",
    "\n",
    "        # If the number of KV heads is different from the number of query heads, adjusts keys and values to match the query heads count.\n",
    "        if self.ca_kv_heads != self.ca_q_heads:\n",
    "            ck = self.match_headcount(ck)\n",
    "            cv = self.match_headcount(cv) # [batch_size, input_len, n_local_heads, ca_head_dim]\n",
    "\n",
    "        # Transposes Q, K, and V tensors to align them for the batch matrix multiplication in attention calculation.\n",
    "        xq = xq.transpose(1, 2) # [batch_size, n_local_heads, input_len, ca_head_dim]\n",
    "        ck = ck.transpose(1, 2)\n",
    "        cv = cv.transpose(1, 2)\n",
    "\n",
    "        # Calculates attention scores by performing a batch matrix multiplication between queries and keys, followed by scaling.\n",
    "        logits = self.attend(xq, ck, h_i) # [batch_size, n_local_heads, input_len, input_len]\n",
    "\n",
    "        # applies values to get final output\n",
    "        output = self.calc_output(logits, cv, batch_size, input_len_x)\n",
    "\n",
    "        # Applies the final linear projection to the attention output, mapping it back to d_i. \n",
    "        return F.dropout(output @ Wo, p=self.dropout_rate, training=training) # and dropout if we're training\n",
    "\n",
    "    @log_io\n",
    "    def weight_splicing(self, Wqkv, Wo, xd_i, cd_i, h_i):\n",
    "        Wq, Wk, Wv = Wqkv.split([self.ca_q_heads * self.ca_head_dim,\n",
    "                                 self.ca_kv_heads * self.ca_head_dim,\n",
    "                                 self.ca_kv_heads * self.ca_head_dim],dim = -1)\n",
    "        Wq = torch.cat([Wq[:xd_i, j*self.ca_head_dim:j*self.ca_head_dim + h_i] for j in range(self.ca_q_heads)], dim = 1)\n",
    "        Wk = torch.cat([Wk[:cd_i, j*self.ca_head_dim:j*self.ca_head_dim + h_i] for j in range(self.ca_kv_heads)], dim = 1)\n",
    "        Wv = torch.cat([Wv[:cd_i, j*self.ca_head_dim:j*self.ca_head_dim + h_i] for j in range(self.ca_kv_heads)], dim = 1)\n",
    "        Wo = torch.cat([Wo[j*self.ca_head_dim:j*self.ca_head_dim + h_i, :xd_i] for j in range(self.ca_q_heads)], dim=0)\n",
    "        return Wq, Wk, Wv, Wo\n",
    "\n",
    "    @log_io\n",
    "    def RoPE(self, x, h_i):\n",
    "        return RoPE(x, h_i, self.theta)\n",
    "\n",
    "    @log_io\n",
    "    def match_headcount(self, xn):\n",
    "        return torch.repeat_interleave(xn, self.num_queries_per_kv, dim=2)\n",
    "\n",
    "    @log_io\n",
    "    def attend(self, xq, ck, h_i):\n",
    "        return torch.matmul(xq, ck.transpose(2, 3)) * (h_i ** -0.5)\n",
    "        \n",
    "    @log_io\n",
    "    def apply_mask(self, logits, input_len):\n",
    "        return torch.where(self.mask[..., :input_len, :input_len].expand_as(logits),\n",
    "                           logits,\n",
    "                           torch.tensor(-1e30, device=logits.device, dtype=logits.dtype))\n",
    "    \n",
    "    @log_io\n",
    "    def calc_output(self, logits, cv, batch_size, input_len_x):\n",
    "        # Applies softmax to the logits to obtain attention probabilities\n",
    "        scores = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Computes the weighted sum of values based on the attention scores to obtain the output of the attention mechanism.\n",
    "        output = scores @ cv # [batch_size, n_local_heads, input_len, sa_head_dim]\n",
    "\n",
    "        # Reshapes the attention output to match the expected output dimensions, combining the heads back into the hidden dimension.\n",
    "        return output.transpose(1, 2).contiguous().view(batch_size, input_len_x, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6853c90b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### demonstration/debugging\n",
    "\n",
    "at one point this one had been giving me trouble so here's multiple different config.levels setups for ya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25425bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering crossMQA.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 256, 128])\n",
      "Tensor 'c' shape: torch.Size([32, 64, 256])\n",
      "\n",
      "==========Entering crossMQA.weight_splicing==========\n",
      "Inputs:\n",
      "Tensor 'Wqkv' shape: torch.Size([256, 128])\n",
      "Tensor 'Wo' shape: torch.Size([64, 256])\n",
      "Integer 'xd_i': Value=128\n",
      "Integer 'cd_i': Value=256\n",
      "Integer 'h_i': Value=32\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([128, 64])\n",
      "Tensor 'output[1]' shape: torch.Size([256, 32])\n",
      "Tensor 'output[2]' shape: torch.Size([256, 32])\n",
      "Tensor 'output[3]' shape: torch.Size([64, 128])\n",
      "==========Exiting crossMQA.weight_splicing==========\n",
      "\n",
      "==========Entering crossMQA.match_headcount==========\n",
      "Inputs:\n",
      "Tensor 'xn' shape: torch.Size([32, 64, 1, 32])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 64, 2, 32])\n",
      "==========Exiting crossMQA.match_headcount==========\n",
      "\n",
      "==========Entering crossMQA.match_headcount==========\n",
      "Inputs:\n",
      "Tensor 'xn' shape: torch.Size([32, 64, 1, 32])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 64, 2, 32])\n",
      "==========Exiting crossMQA.match_headcount==========\n",
      "\n",
      "==========Entering crossMQA.attend==========\n",
      "Inputs:\n",
      "Tensor 'xq' shape: torch.Size([32, 2, 256, 32])\n",
      "Tensor 'ck' shape: torch.Size([32, 2, 64, 32])\n",
      "Integer 'h_i': Value=32\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 2, 256, 64])\n",
      "==========Exiting crossMQA.attend==========\n",
      "\n",
      "==========Entering crossMQA.calc_output==========\n",
      "Inputs:\n",
      "Tensor 'logits' shape: torch.Size([32, 2, 256, 64])\n",
      "Tensor 'cv' shape: torch.Size([32, 2, 64, 32])\n",
      "Integer 'batch_size': Value=32\n",
      "Integer 'input_len_x': Value=256\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 256, 64])\n",
      "==========Exiting crossMQA.calc_output==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 256, 128])\n",
      "==========Exiting crossMQA.forward==========\n"
     ]
    }
   ],
   "source": [
    "hold = config.levels\n",
    "config.levels = 2\n",
    "module = crossMQA(config)\n",
    "module.enable_logging()\n",
    "x0 = torch.randn(32, config.max_seq_len, config.embed_dim // config.split)\n",
    "c1 = torch.randn(32, config.max_seq_len // config.combo, config.embed_dim)\n",
    "output = module(x0, c1)\n",
    "config.levels = hold\n",
    "del hold, module, x0, c1, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "340a07df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering crossMQA.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 256, 64])\n",
      "Tensor 'c' shape: torch.Size([32, 64, 128])\n",
      "\n",
      "==========Entering crossMQA.weight_splicing==========\n",
      "Inputs:\n",
      "Tensor 'Wqkv' shape: torch.Size([256, 128])\n",
      "Tensor 'Wo' shape: torch.Size([64, 256])\n",
      "Integer 'xd_i': Value=64\n",
      "Integer 'cd_i': Value=128\n",
      "Integer 'h_i': Value=16\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([64, 32])\n",
      "Tensor 'output[1]' shape: torch.Size([128, 16])\n",
      "Tensor 'output[2]' shape: torch.Size([128, 16])\n",
      "Tensor 'output[3]' shape: torch.Size([32, 64])\n",
      "==========Exiting crossMQA.weight_splicing==========\n",
      "\n",
      "==========Entering crossMQA.match_headcount==========\n",
      "Inputs:\n",
      "Tensor 'xn' shape: torch.Size([32, 64, 1, 16])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 64, 2, 16])\n",
      "==========Exiting crossMQA.match_headcount==========\n",
      "\n",
      "==========Entering crossMQA.match_headcount==========\n",
      "Inputs:\n",
      "Tensor 'xn' shape: torch.Size([32, 64, 1, 16])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 64, 2, 16])\n",
      "==========Exiting crossMQA.match_headcount==========\n",
      "\n",
      "==========Entering crossMQA.attend==========\n",
      "Inputs:\n",
      "Tensor 'xq' shape: torch.Size([32, 2, 256, 16])\n",
      "Tensor 'ck' shape: torch.Size([32, 2, 64, 16])\n",
      "Integer 'h_i': Value=16\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 2, 256, 64])\n",
      "==========Exiting crossMQA.attend==========\n",
      "\n",
      "==========Entering crossMQA.calc_output==========\n",
      "Inputs:\n",
      "Tensor 'logits' shape: torch.Size([32, 2, 256, 64])\n",
      "Tensor 'cv' shape: torch.Size([32, 2, 64, 16])\n",
      "Integer 'batch_size': Value=32\n",
      "Integer 'input_len_x': Value=256\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 256, 32])\n",
      "==========Exiting crossMQA.calc_output==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 256, 64])\n",
      "==========Exiting crossMQA.forward==========\n",
      "\n",
      "==========Entering crossMQA.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 64, 128])\n",
      "Tensor 'c' shape: torch.Size([32, 16, 256])\n",
      "\n",
      "==========Entering crossMQA.weight_splicing==========\n",
      "Inputs:\n",
      "Tensor 'Wqkv' shape: torch.Size([256, 128])\n",
      "Tensor 'Wo' shape: torch.Size([64, 256])\n",
      "Integer 'xd_i': Value=128\n",
      "Integer 'cd_i': Value=256\n",
      "Integer 'h_i': Value=32\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([128, 64])\n",
      "Tensor 'output[1]' shape: torch.Size([256, 32])\n",
      "Tensor 'output[2]' shape: torch.Size([256, 32])\n",
      "Tensor 'output[3]' shape: torch.Size([64, 128])\n",
      "==========Exiting crossMQA.weight_splicing==========\n",
      "\n",
      "==========Entering crossMQA.match_headcount==========\n",
      "Inputs:\n",
      "Tensor 'xn' shape: torch.Size([32, 16, 1, 32])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 16, 2, 32])\n",
      "==========Exiting crossMQA.match_headcount==========\n",
      "\n",
      "==========Entering crossMQA.match_headcount==========\n",
      "Inputs:\n",
      "Tensor 'xn' shape: torch.Size([32, 16, 1, 32])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 16, 2, 32])\n",
      "==========Exiting crossMQA.match_headcount==========\n",
      "\n",
      "==========Entering crossMQA.attend==========\n",
      "Inputs:\n",
      "Tensor 'xq' shape: torch.Size([32, 2, 64, 32])\n",
      "Tensor 'ck' shape: torch.Size([32, 2, 16, 32])\n",
      "Integer 'h_i': Value=32\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 2, 64, 16])\n",
      "==========Exiting crossMQA.attend==========\n",
      "\n",
      "==========Entering crossMQA.calc_output==========\n",
      "Inputs:\n",
      "Tensor 'logits' shape: torch.Size([32, 2, 64, 16])\n",
      "Tensor 'cv' shape: torch.Size([32, 2, 16, 32])\n",
      "Integer 'batch_size': Value=32\n",
      "Integer 'input_len_x': Value=64\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 64, 64])\n",
      "==========Exiting crossMQA.calc_output==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 64, 128])\n",
      "==========Exiting crossMQA.forward==========\n"
     ]
    }
   ],
   "source": [
    "hold = config.levels\n",
    "config.levels = 3\n",
    "module = crossMQA(config)\n",
    "module.enable_logging()\n",
    "x0 = torch.randn(32, config.max_seq_len, config.embed_dim // (config.split**2))\n",
    "c1 = torch.randn(32, config.max_seq_len // config.combo, config.embed_dim // config.split)\n",
    "c2 = torch.randn(32, config.max_seq_len // (config.combo**2), config.embed_dim)\n",
    "output = module(x0, c1)\n",
    "output = module(c1, c2)\n",
    "config.levels = hold\n",
    "del hold, module, x0, c1, c2, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "105a7825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering crossMQA.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 256, 32])\n",
      "Tensor 'c' shape: torch.Size([32, 64, 64])\n",
      "\n",
      "==========Entering crossMQA.weight_splicing==========\n",
      "Inputs:\n",
      "Tensor 'Wqkv' shape: torch.Size([256, 128])\n",
      "Tensor 'Wo' shape: torch.Size([64, 256])\n",
      "Integer 'xd_i': Value=32\n",
      "Integer 'cd_i': Value=64\n",
      "Integer 'h_i': Value=8\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([32, 16])\n",
      "Tensor 'output[1]' shape: torch.Size([64, 8])\n",
      "Tensor 'output[2]' shape: torch.Size([64, 8])\n",
      "Tensor 'output[3]' shape: torch.Size([16, 32])\n",
      "==========Exiting crossMQA.weight_splicing==========\n",
      "\n",
      "==========Entering crossMQA.match_headcount==========\n",
      "Inputs:\n",
      "Tensor 'xn' shape: torch.Size([32, 64, 1, 8])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 64, 2, 8])\n",
      "==========Exiting crossMQA.match_headcount==========\n",
      "\n",
      "==========Entering crossMQA.match_headcount==========\n",
      "Inputs:\n",
      "Tensor 'xn' shape: torch.Size([32, 64, 1, 8])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 64, 2, 8])\n",
      "==========Exiting crossMQA.match_headcount==========\n",
      "\n",
      "==========Entering crossMQA.attend==========\n",
      "Inputs:\n",
      "Tensor 'xq' shape: torch.Size([32, 2, 256, 8])\n",
      "Tensor 'ck' shape: torch.Size([32, 2, 64, 8])\n",
      "Integer 'h_i': Value=8\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 2, 256, 64])\n",
      "==========Exiting crossMQA.attend==========\n",
      "\n",
      "==========Entering crossMQA.calc_output==========\n",
      "Inputs:\n",
      "Tensor 'logits' shape: torch.Size([32, 2, 256, 64])\n",
      "Tensor 'cv' shape: torch.Size([32, 2, 64, 8])\n",
      "Integer 'batch_size': Value=32\n",
      "Integer 'input_len_x': Value=256\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 256, 16])\n",
      "==========Exiting crossMQA.calc_output==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 256, 32])\n",
      "==========Exiting crossMQA.forward==========\n",
      "\n",
      "==========Entering crossMQA.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 64, 64])\n",
      "Tensor 'c' shape: torch.Size([32, 16, 128])\n",
      "\n",
      "==========Entering crossMQA.weight_splicing==========\n",
      "Inputs:\n",
      "Tensor 'Wqkv' shape: torch.Size([256, 128])\n",
      "Tensor 'Wo' shape: torch.Size([64, 256])\n",
      "Integer 'xd_i': Value=64\n",
      "Integer 'cd_i': Value=128\n",
      "Integer 'h_i': Value=16\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([64, 32])\n",
      "Tensor 'output[1]' shape: torch.Size([128, 16])\n",
      "Tensor 'output[2]' shape: torch.Size([128, 16])\n",
      "Tensor 'output[3]' shape: torch.Size([32, 64])\n",
      "==========Exiting crossMQA.weight_splicing==========\n",
      "\n",
      "==========Entering crossMQA.match_headcount==========\n",
      "Inputs:\n",
      "Tensor 'xn' shape: torch.Size([32, 16, 1, 16])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 16, 2, 16])\n",
      "==========Exiting crossMQA.match_headcount==========\n",
      "\n",
      "==========Entering crossMQA.match_headcount==========\n",
      "Inputs:\n",
      "Tensor 'xn' shape: torch.Size([32, 16, 1, 16])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 16, 2, 16])\n",
      "==========Exiting crossMQA.match_headcount==========\n",
      "\n",
      "==========Entering crossMQA.attend==========\n",
      "Inputs:\n",
      "Tensor 'xq' shape: torch.Size([32, 2, 64, 16])\n",
      "Tensor 'ck' shape: torch.Size([32, 2, 16, 16])\n",
      "Integer 'h_i': Value=16\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 2, 64, 16])\n",
      "==========Exiting crossMQA.attend==========\n",
      "\n",
      "==========Entering crossMQA.calc_output==========\n",
      "Inputs:\n",
      "Tensor 'logits' shape: torch.Size([32, 2, 64, 16])\n",
      "Tensor 'cv' shape: torch.Size([32, 2, 16, 16])\n",
      "Integer 'batch_size': Value=32\n",
      "Integer 'input_len_x': Value=64\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 64, 32])\n",
      "==========Exiting crossMQA.calc_output==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 64, 64])\n",
      "==========Exiting crossMQA.forward==========\n",
      "\n",
      "==========Entering crossMQA.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 16, 128])\n",
      "Tensor 'c' shape: torch.Size([32, 4, 256])\n",
      "\n",
      "==========Entering crossMQA.weight_splicing==========\n",
      "Inputs:\n",
      "Tensor 'Wqkv' shape: torch.Size([256, 128])\n",
      "Tensor 'Wo' shape: torch.Size([64, 256])\n",
      "Integer 'xd_i': Value=128\n",
      "Integer 'cd_i': Value=256\n",
      "Integer 'h_i': Value=32\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([128, 64])\n",
      "Tensor 'output[1]' shape: torch.Size([256, 32])\n",
      "Tensor 'output[2]' shape: torch.Size([256, 32])\n",
      "Tensor 'output[3]' shape: torch.Size([64, 128])\n",
      "==========Exiting crossMQA.weight_splicing==========\n",
      "\n",
      "==========Entering crossMQA.match_headcount==========\n",
      "Inputs:\n",
      "Tensor 'xn' shape: torch.Size([32, 4, 1, 32])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 4, 2, 32])\n",
      "==========Exiting crossMQA.match_headcount==========\n",
      "\n",
      "==========Entering crossMQA.match_headcount==========\n",
      "Inputs:\n",
      "Tensor 'xn' shape: torch.Size([32, 4, 1, 32])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 4, 2, 32])\n",
      "==========Exiting crossMQA.match_headcount==========\n",
      "\n",
      "==========Entering crossMQA.attend==========\n",
      "Inputs:\n",
      "Tensor 'xq' shape: torch.Size([32, 2, 16, 32])\n",
      "Tensor 'ck' shape: torch.Size([32, 2, 4, 32])\n",
      "Integer 'h_i': Value=32\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 2, 16, 4])\n",
      "==========Exiting crossMQA.attend==========\n",
      "\n",
      "==========Entering crossMQA.calc_output==========\n",
      "Inputs:\n",
      "Tensor 'logits' shape: torch.Size([32, 2, 16, 4])\n",
      "Tensor 'cv' shape: torch.Size([32, 2, 4, 32])\n",
      "Integer 'batch_size': Value=32\n",
      "Integer 'input_len_x': Value=16\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 16, 64])\n",
      "==========Exiting crossMQA.calc_output==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 16, 128])\n",
      "==========Exiting crossMQA.forward==========\n"
     ]
    }
   ],
   "source": [
    "hold = config.levels\n",
    "config.levels = 4\n",
    "module = crossMQA(config)\n",
    "module.enable_logging()\n",
    "x0 = torch.randn(32, config.max_seq_len, config.embed_dim // (config.split**3))\n",
    "c1 = torch.randn(32, config.max_seq_len // config.combo, config.embed_dim // (config.split**2))\n",
    "c2 = torch.randn(32, config.max_seq_len // (config.combo**2), config.embed_dim // config.split)\n",
    "c3 = torch.randn(32, config.max_seq_len // (config.combo**3), config.embed_dim)\n",
    "output = module(x0, c1)\n",
    "output = module(c1, c2)\n",
    "output = module(c2, c3)\n",
    "config.levels = hold\n",
    "del hold, module, x0, c1, c2, c3, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5917e8d4",
   "metadata": {},
   "source": [
    "# crossLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "35e022a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class crossLayer(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.mqa = crossMQA(config)\n",
    "\n",
    "        # jic i want to add an MLP later\n",
    "        #self.mlp = MLP(config.embed_dim, config.mlp_multiplier)\n",
    "\n",
    "        # Initialize normalization layers using the class reference from config\n",
    "        self.pre_mqa_norm_x = Norm(config)\n",
    "        self.pre_mqa_norm_c = Norm(config)\n",
    "        self.post_mqa_norm = Norm(config)\n",
    "        #self.pre_mlp_norm = Norm(config)\n",
    "        #self.post_mlp_norm = Norm(config)\n",
    "        \n",
    "        self.logging_enabled = False\n",
    "    def enable_logging(self):\n",
    "        self.logging_enabled = True\n",
    "    def disable_logging(self):\n",
    "        self.logging_enabled = False\n",
    "        \n",
    "    @log_io\n",
    "    def forward(self,\n",
    "                x: torch.Tensor,\n",
    "                c: torch.Tensor,\n",
    "                training: bool = False\n",
    "               ) -> torch.Tensor:\n",
    "        x = x + self.mqa_connection(x, c, training)\n",
    "        #x = x + self.mlp_connection(x, training)\n",
    "        return x\n",
    "\n",
    "    @log_io\n",
    "    def mqa_connection(self, x, c, training):\n",
    "        return self.post_mqa_norm(self.mqa(x = self.pre_mqa_norm_x(x, training), \n",
    "                                           c = self.pre_mqa_norm_c(c, training), training=training), training)\n",
    "        return x\n",
    "\n",
    "    #@log_io\n",
    "    #def mlp_connection(self, x, training):\n",
    "        #return self.post_mlp_norm(self.mlp(self.pre_mlp_norm(x, training), training), training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c358bd1f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### demonstration/debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9dd4e595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering crossLayer.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 256, 128])\n",
      "Tensor 'c' shape: torch.Size([32, 64, 256])\n",
      "\n",
      "==========Entering crossLayer.mqa_connection==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 256, 128])\n",
      "Tensor 'c' shape: torch.Size([32, 64, 256])\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 256, 128])\n",
      "==========Exiting crossLayer.mqa_connection==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 256, 128])\n",
      "==========Exiting crossLayer.forward==========\n"
     ]
    }
   ],
   "source": [
    "module = crossLayer(config)\n",
    "module.enable_logging()\n",
    "#module.mqa.enable_logging()\n",
    "#module.pre_mqa_norm_x.enable_logging()\n",
    "#module.pre_mqa_norm_c.enable_logging()\n",
    "#module.post_mqa_norm.enable_logging()\n",
    "x = torch.randn(32, config.max_seq_len, config.embed_dim // config.split)\n",
    "c = torch.randn(32, config.max_seq_len // config.combo, config.embed_dim)\n",
    "output = module(x, c)\n",
    "del module, x, c, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6743c07e",
   "metadata": {},
   "source": [
    "# Body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "47494bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to prevent the warning statement from printing hella times\n",
    "cvec_warning = False\n",
    "\n",
    "class Body(nn.Module):\n",
    "    def __init__(self, config: Config, embedder: torch.Tensor):\n",
    "        super().__init__()\n",
    "        self.ca_interval = config.ca_interval\n",
    "        self.max_seq_len = config.max_seq_len\n",
    "        self.combo = config.combo\n",
    "        self.levels = config.levels\n",
    "        self.embedding = embedder.weight\n",
    "        \n",
    "        # Initialize a sequence of Layer instances as specified by the number of hidden layers in the config\n",
    "        self.layers = nn.ModuleList(Layer(config) for _ in range(config.num_layers))\n",
    "        # initialize the (single / sequence of) cross-attention layer(s)\n",
    "        self.ca_layers = nn.ModuleList(crossLayer(config)) if config.shared_ca else nn.ModuleList(crossLayer(config) for _ in range(config.ca_connections))\n",
    "        \n",
    "        # Initialize normalization layers to be applied after the last decoder layers, stabilizing the output\n",
    "        self.final_norm = Norm(config)\n",
    "        \n",
    "        self.logging_enabled = False\n",
    "    def enable_logging(self):\n",
    "        self.logging_enabled = True\n",
    "    def disable_logging(self):\n",
    "        self.logging_enabled = False\n",
    "\n",
    "    @log_io\n",
    "    def forward(self, \n",
    "                x0s: Tuple[torch.Tensor], # ordered from tokens -> highest concepts\n",
    "                cvec_samples: int = None,\n",
    "                cvec_greedy: bool = False,\n",
    "                cvec_temp: float = 1.0,\n",
    "                training: bool = False,\n",
    "               ) -> Tuple[torch.Tensor]:\n",
    "        # initiate tuple to hold final residual states\n",
    "        xfs = ()\n",
    "        # iterate through model levels, starting from highest level concepts & ending at lowest level tokens\n",
    "        for i, x in enumerate(reversed(x0s)): # reversed() makes us start at highest level\n",
    "            \n",
    "            effective_max_seq_len = self.max_seq_len // (self.combo ** (self.levels-1-i))\n",
    "            assert x.shape[1] <= effective_max_seq_len, f'somehow a too-long sequence ({x.shape[1]} vs {effective_max_seq_len}) made it all the way to Body'\n",
    "            extra_runs = effective_max_seq_len - x.shape[1]\n",
    "            for k in range(extra_runs): # if extra_runs == 0 then this just won't do anything\n",
    "                # run\n",
    "                x_ = self.layers_loop(x, i, x0s, training)\n",
    "                # subset -1\n",
    "                x_ = x_[:,-1,:]\n",
    "                # norm? no, i'm worried about affine transformations messing with it since this process wasn't present during training\n",
    "                # select most similar concept vectors to be appended to the sequence\n",
    "                c = self.concept_matchup(x_, cvec_samples, cvec_greedy, cvec_temp)\n",
    "                # append to x\n",
    "                x = torch.concat([x, c.unsqueeze(1)], dim=1)\n",
    "\n",
    "            # the final run. this one will actually be logits to get used with crossMQA\n",
    "            x = self.layers_loop(x, i, x0s, training)\n",
    "            \n",
    "            # add the final residual state of the level to our tuple, normed\n",
    "            xfs += (self.final_norm(x, training),) # should i be using separate final norms? nah\n",
    "\n",
    "        return xfs # now it's ordered from highest concepts -> token\n",
    "\n",
    "    @log_io\n",
    "    def layers_loop(self, x, i, x0s, training):\n",
    "        \n",
    "        # Iteratively process the input through each Layer of the model\n",
    "        for j in range(len(self.layers)):\n",
    "            \n",
    "            # our occasional cross-attention connection to the upper level\n",
    "            if (i != 0) & (j % self.ca_interval == 0): # i can't equal zero bc there'd be no higher level model to pay attention to\n",
    "                ca_layer = self.ca_layers[j // self.ca_interval]\n",
    "                x = ca_layer(x, x0s[len(x0s)-i], training)\n",
    "\n",
    "            # our current-level model's work\n",
    "            layer = self.layers[j]\n",
    "            x = layer(x, training)\n",
    "        return x\n",
    "\n",
    "    @log_io\n",
    "    def concept_matchup(self,\n",
    "                        c: torch.Tensor,\n",
    "                        cvec_samples: int,\n",
    "                        cvec_greedy: bool,\n",
    "                        cvec_temp: float,\n",
    "                        ) -> torch.Tensor:\n",
    "        global cvec_warning\n",
    "        batch_size, d = c.size()\n",
    "        vocab_size = self.embedding.size(0)\n",
    "    \n",
    "        # Batch cosine similarity\n",
    "        # Reshape c: (batch_size x 1 x embedding_dim)\n",
    "        # Reshape embedding: (1 x vocab_size x embedding_dim)\n",
    "        # Resulting similarity: (batch_size x vocab_size)\n",
    "        token_similarities = F.cosine_similarity(c.unsqueeze(1), self.embedding.unsqueeze(0), dim=-1)\n",
    "        \n",
    "        # how many tokens will we sample to build up our chosen concept vector?\n",
    "        if cvec_samples is None:\n",
    "            cvec_samples = self.combo ** (self.levels-1)\n",
    "            if (cvec_warning == False) or (cvec_warning is None):\n",
    "                print(f\"cvec_samples not defined. defaulting to highest level's minimum size: combo**(levels-1) = {cvec_samples}\")\n",
    "                cvec_warning = True\n",
    "        assert cvec_samples >= self.combo ** (self.levels-1)\n",
    "        \n",
    "        # Select top-k token embeddings for each concept vector\n",
    "        topk_token_indices = torch.topk(token_similarities, k=cvec_samples, dim=1).indices  # (batch_size x sample)\n",
    "    \n",
    "        # Generate concept embeddings for each set of top-k token embeddings\n",
    "        concept_embeddings_batch = []\n",
    "        X_sizes_batch = []\n",
    "        for i in range(batch_size):\n",
    "            # Pass the list of indices for each concept\n",
    "            concept_embeddings, X_sizes = self.create_concept_embeddings(self.embedding, \n",
    "                                                                         [topk_token_indices[i].tolist()])\n",
    "            concept_embeddings_batch.append(concept_embeddings.squeeze(0))  # Remove the extra batch dimension\n",
    "            X_sizes_batch.append(X_sizes)\n",
    "    \n",
    "        # Convert list of tensors to a tensor\n",
    "        concept_embeddings_batch = torch.stack(concept_embeddings_batch)  # (batch_size x max_X_size x d)\n",
    "    \n",
    "        # Calculate concept similarities for each concept in the batch\n",
    "        concept_similarities_batch = F.cosine_similarity(c.unsqueeze(1), concept_embeddings_batch, dim=-1)\n",
    "    \n",
    "        # Select the best matching concept embedding for each concept vector in the batch\n",
    "        if cvec_greedy:\n",
    "            best_concept_indices = concept_similarities_batch.argmax(dim=1)\n",
    "            matched_concepts = concept_embeddings_batch[torch.arange(batch_size), best_concept_indices]\n",
    "        else:\n",
    "            # Apply softmax with temperature and sample\n",
    "            topk_concept_probs = F.softmax(concept_similarities_batch / cvec_temp, dim=1)\n",
    "            concept_topk_idx = torch.multinomial(topk_concept_probs, num_samples=1).squeeze(1)\n",
    "            matched_concepts = concept_embeddings_batch[torch.arange(batch_size), concept_topk_idx]\n",
    "    \n",
    "        return matched_concepts\n",
    "\n",
    "    @log_io\n",
    "    def create_concept_embeddings(self, E: torch.Tensor, indices: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Create concept embeddings for a batch of indices.\n",
    "    \n",
    "        E: Embedding matrix (vocab_size x embedding_dim)\n",
    "        indices: A list of lists of indices (batch_size x num_indices)\n",
    "        \"\"\"\n",
    "        batch_size = len(indices)\n",
    "        d = E.size(1)\n",
    "        X_sizes = [(len(ind) - 1) * len(ind) // 2 for ind in indices]\n",
    "        max_X_size = max(X_sizes)\n",
    "        X = torch.empty((batch_size, max_X_size, d), dtype=E.dtype)\n",
    "        \n",
    "        # this could prolly be done way more efficiently with tensor operations\n",
    "        for b in range(batch_size):\n",
    "            count = 0\n",
    "            for i in range(len(indices[b])):\n",
    "                for j in range(i + 1, len(indices[b])):\n",
    "                    X[b, count] = E[indices[b][i]] + E[indices[b][j]]\n",
    "                    count += 1\n",
    "            # Padding the rest if necessary\n",
    "            if count < max_X_size:\n",
    "                X[b, count:] = torch.zeros((max_X_size - count, d))\n",
    "        \n",
    "        # X_sizes is not useful rn but i think it may be later when we switch away from TinyShakespeare\n",
    "        # and over to data that actually has variable sequence lengths\n",
    "        return X, X_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa19e1b5",
   "metadata": {},
   "source": [
    "### demonstration/debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5d4ba86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering Body.forward==========\n",
      "Inputs:\n",
      "Tuple 'x0s':\n",
      "    Tensor 'x0s[0]' shape: torch.Size([32, 256, 128])\n",
      "    Tensor 'x0s[1]' shape: torch.Size([32, 64, 256])\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 64, 256])\n",
      "Integer 'i': Value=0\n",
      "Tuple 'x0s':\n",
      "    Tensor 'x0s[0]' shape: torch.Size([32, 256, 128])\n",
      "    Tensor 'x0s[1]' shape: torch.Size([32, 64, 256])\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 64, 256])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 256, 128])\n",
      "Integer 'i': Value=1\n",
      "Tuple 'x0s':\n",
      "    Tensor 'x0s[0]' shape: torch.Size([32, 256, 128])\n",
      "    Tensor 'x0s[1]' shape: torch.Size([32, 64, 256])\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 256, 128])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([32, 64, 256])\n",
      "Tensor 'output[1]' shape: torch.Size([32, 256, 128])\n",
      "==========Exiting Body.forward==========\n"
     ]
    }
   ],
   "source": [
    "# first let's do 2 levels with full sequence length\n",
    "embedding = nn.Embedding(config.vocab_size, config.embed_dim)\n",
    "hold = config.levels\n",
    "config.levels = 2\n",
    "module = Body(config, embedding)\n",
    "module.enable_logging()\n",
    "#module.layers.enable_logging()\n",
    "#module.ca_layers.enable_logging()\n",
    "#module.final_norm.enable_logging()\n",
    "x = torch.randn(32, config.max_seq_len, config.embed_dim // config.split)\n",
    "c = torch.randn(32, config.max_seq_len // config.combo, config.embed_dim)\n",
    "x0s = (x,c)\n",
    "output = module(x0s)\n",
    "config.levels = hold\n",
    "del embedding, hold, module, x, c, x0s, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b2f1f3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering Body.forward==========\n",
      "Inputs:\n",
      "Tuple 'x0s':\n",
      "    Tensor 'x0s[0]' shape: torch.Size([32, 256, 64])\n",
      "    Tensor 'x0s[1]' shape: torch.Size([32, 64, 128])\n",
      "    Tensor 'x0s[2]' shape: torch.Size([32, 16, 256])\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 16, 256])\n",
      "Integer 'i': Value=0\n",
      "Tuple 'x0s':\n",
      "    Tensor 'x0s[0]' shape: torch.Size([32, 256, 64])\n",
      "    Tensor 'x0s[1]' shape: torch.Size([32, 64, 128])\n",
      "    Tensor 'x0s[2]' shape: torch.Size([32, 16, 256])\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 16, 256])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 64, 128])\n",
      "Integer 'i': Value=1\n",
      "Tuple 'x0s':\n",
      "    Tensor 'x0s[0]' shape: torch.Size([32, 256, 64])\n",
      "    Tensor 'x0s[1]' shape: torch.Size([32, 64, 128])\n",
      "    Tensor 'x0s[2]' shape: torch.Size([32, 16, 256])\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 64, 128])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 256, 64])\n",
      "Integer 'i': Value=2\n",
      "Tuple 'x0s':\n",
      "    Tensor 'x0s[0]' shape: torch.Size([32, 256, 64])\n",
      "    Tensor 'x0s[1]' shape: torch.Size([32, 64, 128])\n",
      "    Tensor 'x0s[2]' shape: torch.Size([32, 16, 256])\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 256, 64])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([32, 16, 256])\n",
      "Tensor 'output[1]' shape: torch.Size([32, 64, 128])\n",
      "Tensor 'output[2]' shape: torch.Size([32, 256, 64])\n",
      "==========Exiting Body.forward==========\n"
     ]
    }
   ],
   "source": [
    "# now 3 levels full sequence length\n",
    "embedding = nn.Embedding(config.vocab_size, config.embed_dim)\n",
    "hold = config.levels\n",
    "config.levels = 3\n",
    "module = Body(config, embedding)\n",
    "module.enable_logging()\n",
    "#module.layers.enable_logging()\n",
    "#module.ca_layers.enable_logging()\n",
    "#module.final_norm.enable_logging()\n",
    "x0 = torch.randn(32, config.max_seq_len // (config.combo ** 0), config.embed_dim // (config.split ** 2))\n",
    "c1 = torch.randn(32, config.max_seq_len // (config.combo ** 1), config.embed_dim // (config.split ** 1))\n",
    "c2 = torch.randn(32, config.max_seq_len // (config.combo ** 2), config.embed_dim // (config.split ** 0))\n",
    "x0s = (x0, c1, c2)\n",
    "output = module(x0s)\n",
    "config.levels = hold\n",
    "del embedding, hold, module, x0, c1, c2, x0s, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "86644a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering Body.forward==========\n",
      "Inputs:\n",
      "Tuple 'x0s':\n",
      "    Tensor 'x0s[0]' shape: torch.Size([32, 240, 128])\n",
      "    Tensor 'x0s[1]' shape: torch.Size([32, 60, 256])\n",
      "Integer 'cvec_samples': Value=16\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 60, 256])\n",
      "Integer 'i': Value=0\n",
      "Tuple 'x0s':\n",
      "    Tensor 'x0s[0]' shape: torch.Size([32, 240, 128])\n",
      "    Tensor 'x0s[1]' shape: torch.Size([32, 60, 256])\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 60, 256])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "==========Entering Body.concept_matchup==========\n",
      "Inputs:\n",
      "Tensor 'c' shape: torch.Size([32, 256])\n",
      "Integer 'cvec_samples': Value=16\n",
      "Integer 'cvec_greedy': Value=False\n",
      "Other-type 'cvec_temp': Type=float, Value=1.0\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[80, 97, 85, 23, 51, 118, 110, 73, 44, 33, 86, 46, 87, 106, 103, 38]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[121, 12, 42, 52, 57, 72, 108, 91, 73, 94, 3, 112, 30, 5, 124, 48]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[127, 8, 20, 21, 121, 83, 62, 103, 14, 99, 5, 96, 40, 38, 73, 11]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[49, 59, 79, 80, 114, 24, 16, 123, 18, 118, 119, 75, 113, 39, 85, 69]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[106, 31, 94, 122, 82, 20, 57, 87, 114, 29, 97, 3, 22, 117, 108, 74]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[92, 110, 21, 17, 97, 112, 27, 19, 13, 31, 64, 115, 93, 15, 109, 33]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[24, 39, 78, 61, 2, 40, 84, 43, 53, 113, 54, 10, 72, 29, 55, 35]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[127, 47, 121, 60, 56, 13, 62, 101, 57, 43, 7, 104, 42, 36, 49, 94]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[127, 117, 81, 93, 50, 115, 82, 102, 109, 116, 74, 85, 34, 3, 106, 24]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[59, 73, 62, 20, 77, 71, 86, 24, 36, 32, 49, 125, 118, 31, 94, 53]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[107, 28, 36, 54, 56, 109, 91, 70, 3, 101, 69, 32, 39, 99, 105, 24]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[103, 33, 2, 89, 115, 36, 11, 63, 92, 96, 46, 10, 43, 23, 65, 1]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[35, 26, 47, 79, 75, 51, 71, 104, 49, 23, 6, 24, 58, 69, 33, 9]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[21, 43, 122, 51, 47, 109, 46, 19, 119, 50, 111, 23, 20, 22, 125, 118]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[15, 76, 114, 127, 69, 31, 49, 93, 90, 32, 23, 22, 9, 108, 35, 60]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[76, 121, 47, 127, 110, 19, 50, 81, 112, 74, 106, 57, 11, 93, 13, 90]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[34, 125, 77, 57, 10, 81, 92, 33, 84, 63, 120, 23, 103, 72, 75, 110]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[28, 95, 13, 54, 4, 69, 42, 108, 80, 82, 123, 47, 21, 58, 8, 30]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[92, 80, 73, 83, 65, 94, 7, 102, 44, 28, 87, 108, 119, 31, 96, 21]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[101, 87, 116, 104, 72, 69, 35, 117, 23, 40, 108, 111, 94, 31, 60, 33]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[73, 72, 42, 19, 63, 41, 8, 76, 32, 124, 62, 61, 35, 60, 91, 121]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[26, 50, 85, 20, 24, 122, 34, 101, 69, 102, 21, 19, 28, 65, 111, 79]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[31, 15, 113, 84, 112, 93, 55, 17, 49, 39, 26, 97, 76, 1, 85, 83]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[7, 78, 12, 106, 51, 71, 80, 64, 69, 27, 41, 66, 97, 83, 91, 127]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[1, 78, 17, 4, 81, 115, 35, 32, 114, 6, 59, 127, 94, 103, 55, 38]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[48, 51, 29, 44, 101, 22, 108, 55, 124, 112, 103, 99, 80, 38, 119, 77]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[101, 75, 73, 122, 52, 41, 2, 127, 86, 60, 54, 126, 112, 77, 62, 63]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[119, 33, 70, 100, 37, 126, 7, 82, 17, 80, 63, 14, 45, 57, 11, 65]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[127, 20, 122, 8, 37, 100, 103, 9, 101, 17, 50, 74, 52, 18, 40, 21]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[49, 109, 121, 125, 4, 16, 105, 0, 62, 85, 32, 118, 100, 59, 86, 6]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[26, 50, 2, 121, 51, 29, 46, 9, 107, 43, 32, 70, 63, 17, 47, 38]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[43, 72, 49, 37, 125, 82, 70, 34, 5, 7, 87, 94, 89, 2, 100, 93]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 256])\n",
      "==========Exiting Body.concept_matchup==========\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 61, 256])\n",
      "Integer 'i': Value=0\n",
      "Tuple 'x0s':\n",
      "    Tensor 'x0s[0]' shape: torch.Size([32, 240, 128])\n",
      "    Tensor 'x0s[1]' shape: torch.Size([32, 60, 256])\n",
      "Integer 'training': Value=False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 61, 256])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "==========Entering Body.concept_matchup==========\n",
      "Inputs:\n",
      "Tensor 'c' shape: torch.Size([32, 256])\n",
      "Integer 'cvec_samples': Value=16\n",
      "Integer 'cvec_greedy': Value=False\n",
      "Other-type 'cvec_temp': Type=float, Value=1.0\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[51, 85, 111, 71, 61, 69, 106, 29, 73, 38, 65, 72, 113, 43, 75, 33]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[112, 5, 93, 78, 81, 66, 105, 95, 15, 121, 22, 58, 20, 68, 110, 52]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[21, 121, 110, 81, 63, 38, 14, 28, 127, 92, 27, 108, 47, 112, 17, 49]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[123, 80, 40, 118, 100, 9, 108, 11, 96, 58, 45, 5, 120, 67, 86, 102]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[31, 94, 114, 54, 116, 119, 1, 7, 43, 72, 21, 113, 82, 106, 102, 33]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[110, 21, 121, 52, 14, 89, 81, 38, 24, 17, 5, 19, 43, 50, 92, 77]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[35, 29, 51, 43, 74, 69, 112, 63, 92, 42, 103, 88, 12, 0, 44, 105]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[43, 101, 69, 96, 126, 29, 20, 50, 68, 35, 56, 16, 47, 41, 62, 30]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[24, 85, 1, 97, 39, 68, 113, 65, 73, 111, 18, 76, 61, 11, 107, 37]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[125, 94, 2, 34, 8, 116, 73, 40, 33, 21, 68, 25, 1, 82, 54, 38]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[105, 69, 106, 35, 110, 60, 89, 108, 30, 50, 51, 29, 49, 112, 120, 61]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[33, 92, 108, 17, 115, 53, 27, 77, 100, 7, 20, 125, 10, 84, 110, 103]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[51, 23, 35, 29, 73, 124, 33, 10, 122, 108, 62, 0, 65, 69, 106, 77]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[119, 118, 47, 43, 94, 123, 70, 17, 42, 14, 20, 120, 97, 83, 92, 72]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[15, 90, 42, 9, 20, 108, 52, 2, 98, 126, 5, 17, 93, 104, 53, 4]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[106, 90, 46, 11, 69, 126, 118, 36, 31, 57, 34, 78, 50, 114, 54, 99]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[34, 77, 110, 14, 10, 33, 72, 42, 30, 23, 55, 84, 52, 20, 106, 26]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[123, 4, 100, 118, 127, 40, 30, 9, 88, 116, 115, 120, 43, 103, 58, 42]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[83, 73, 44, 97, 12, 124, 32, 62, 1, 121, 60, 78, 48, 34, 40, 98]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[101, 111, 117, 22, 79, 71, 122, 6, 56, 125, 60, 75, 100, 69, 48, 20]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[76, 61, 85, 12, 99, 57, 69, 71, 107, 63, 96, 22, 11, 68, 72, 93]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[122, 79, 65, 63, 26, 28, 25, 111, 8, 29, 7, 23, 101, 1, 71, 93]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[97, 26, 46, 99, 57, 15, 44, 105, 63, 37, 96, 75, 83, 24, 62, 93]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[97, 51, 62, 29, 103, 70, 106, 43, 73, 63, 108, 35, 10, 124, 69, 26]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[115, 17, 15, 92, 21, 127, 9, 33, 37, 66, 119, 36, 65, 27, 74, 109]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[99, 101, 69, 96, 56, 16, 122, 126, 60, 48, 97, 62, 117, 124, 103, 76]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[112, 75, 57, 105, 38, 86, 116, 78, 25, 42, 52, 55, 60, 1, 41, 68]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[63, 37, 44, 57, 25, 122, 32, 38, 41, 76, 126, 70, 127, 121, 26, 2]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[52, 21, 110, 27, 14, 96, 19, 65, 23, 17, 77, 58, 6, 5, 115, 40]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[62, 49, 32, 106, 101, 59, 33, 126, 104, 5, 83, 98, 3, 108, 35, 54]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[38, 17, 107, 21, 0, 125, 68, 110, 92, 84, 120, 15, 119, 121, 65, 37]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[125, 82, 116, 46, 33, 40, 21, 34, 55, 28, 85, 38, 2, 108, 23, 109]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 256])\n",
      "==========Exiting Body.concept_matchup==========\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 62, 256])\n",
      "Integer 'i': Value=0\n",
      "Tuple 'x0s':\n",
      "    Tensor 'x0s[0]' shape: torch.Size([32, 240, 128])\n",
      "    Tensor 'x0s[1]' shape: torch.Size([32, 60, 256])\n",
      "Integer 'training': Value=False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 62, 256])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "==========Entering Body.concept_matchup==========\n",
      "Inputs:\n",
      "Tensor 'c' shape: torch.Size([32, 256])\n",
      "Integer 'cvec_samples': Value=16\n",
      "Integer 'cvec_greedy': Value=False\n",
      "Other-type 'cvec_temp': Type=float, Value=1.0\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[113, 71, 85, 111, 100, 6, 108, 48, 51, 106, 122, 84, 65, 112, 31, 61]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[22, 78, 111, 5, 57, 106, 107, 9, 0, 112, 104, 68, 8, 116, 33, 83]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[63, 47, 122, 121, 35, 119, 93, 46, 25, 51, 83, 100, 44, 67, 26, 38]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[58, 100, 30, 7, 87, 80, 88, 96, 111, 95, 24, 67, 123, 4, 81, 9]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[114, 94, 68, 33, 31, 73, 118, 42, 50, 14, 1, 119, 28, 117, 66, 82]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[121, 21, 110, 81, 63, 28, 38, 127, 14, 27, 92, 49, 47, 17, 108, 88]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[43, 51, 29, 35, 70, 10, 47, 68, 100, 124, 69, 103, 61, 38, 94, 108]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[126, 56, 101, 42, 95, 83, 2, 99, 48, 62, 38, 60, 90, 86, 30, 40]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[11, 1, 24, 7, 106, 65, 108, 124, 99, 45, 93, 37, 31, 117, 27, 26]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[34, 125, 2, 8, 46, 106, 30, 103, 83, 9, 29, 54, 73, 92, 55, 110]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[110, 89, 105, 121, 27, 77, 120, 92, 52, 60, 103, 21, 106, 50, 10, 19]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[92, 17, 9, 125, 119, 33, 0, 110, 20, 102, 121, 27, 7, 1, 21, 84]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[10, 108, 27, 51, 33, 15, 77, 103, 84, 40, 113, 44, 59, 80, 100, 99]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[83, 123, 47, 9, 118, 40, 100, 126, 44, 20, 55, 105, 99, 121, 32, 119]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[126, 104, 40, 90, 32, 2, 42, 102, 85, 62, 43, 70, 83, 64, 37, 49]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[50, 69, 61, 101, 30, 106, 114, 85, 35, 40, 43, 68, 122, 19, 18, 91]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[34, 55, 46, 106, 14, 83, 45, 105, 125, 82, 77, 9, 84, 26, 11, 6]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[88, 116, 31, 58, 18, 48, 4, 50, 19, 101, 1, 21, 36, 43, 75, 8]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[97, 98, 99, 83, 26, 62, 28, 96, 73, 70, 85, 116, 32, 14, 108, 105]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[122, 75, 65, 26, 63, 1, 25, 50, 86, 10, 60, 101, 116, 71, 125, 47]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[71, 69, 61, 85, 106, 51, 101, 122, 0, 74, 60, 91, 100, 35, 80, 117]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[25, 111, 122, 22, 117, 107, 71, 8, 85, 57, 6, 79, 125, 120, 75, 51]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[97, 105, 83, 26, 106, 46, 112, 99, 29, 37, 62, 78, 108, 110, 43, 34]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[43, 69, 61, 35, 29, 70, 68, 91, 101, 51, 126, 50, 30, 38, 40, 105]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[65, 37, 17, 122, 44, 11, 2, 70, 26, 23, 52, 21, 85, 1, 47, 75]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[122, 126, 63, 99, 30, 101, 90, 60, 23, 95, 125, 74, 32, 28, 52, 85]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[38, 116, 73, 75, 1, 82, 67, 58, 21, 112, 62, 16, 68, 94, 42, 85]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[26, 25, 122, 39, 63, 107, 57, 98, 79, 37, 44, 75, 28, 121, 36, 110]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[58, 110, 81, 24, 80, 52, 105, 121, 7, 21, 77, 92, 30, 13, 5, 88]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[35, 5, 78, 66, 81, 49, 12, 0, 51, 103, 47, 46, 69, 45, 63, 23]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[120, 125, 17, 9, 110, 111, 105, 122, 81, 7, 34, 38, 8, 57, 1, 92]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[38, 109, 68, 63, 82, 85, 108, 42, 116, 124, 81, 73, 67, 46, 121, 107]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 256])\n",
      "==========Exiting Body.concept_matchup==========\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 63, 256])\n",
      "Integer 'i': Value=0\n",
      "Tuple 'x0s':\n",
      "    Tensor 'x0s[0]' shape: torch.Size([32, 240, 128])\n",
      "    Tensor 'x0s[1]' shape: torch.Size([32, 60, 256])\n",
      "Integer 'training': Value=False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 63, 256])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "==========Entering Body.concept_matchup==========\n",
      "Inputs:\n",
      "Tensor 'c' shape: torch.Size([32, 256])\n",
      "Integer 'cvec_samples': Value=16\n",
      "Integer 'cvec_greedy': Value=False\n",
      "Other-type 'cvec_temp': Type=float, Value=1.0\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[84, 71, 122, 0, 80, 39, 61, 51, 74, 17, 53, 10, 86, 47, 100, 77]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[106, 8, 11, 46, 34, 69, 78, 60, 79, 36, 95, 57, 125, 114, 22, 61]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[122, 46, 26, 63, 47, 74, 125, 1, 75, 90, 65, 106, 28, 87, 38, 93]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[9, 81, 17, 123, 120, 19, 125, 90, 58, 127, 92, 64, 110, 5, 121, 54]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[94, 14, 21, 119, 34, 124, 20, 33, 40, 7, 43, 15, 97, 76, 73, 114]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[127, 21, 14, 96, 115, 81, 20, 27, 28, 121, 37, 110, 6, 50, 4, 72]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[35, 94, 43, 51, 47, 118, 33, 66, 116, 42, 112, 68, 69, 14, 24, 74]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[90, 48, 126, 99, 42, 79, 36, 74, 83, 108, 9, 113, 20, 111, 77, 114]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[108, 11, 80, 33, 117, 65, 113, 82, 15, 27, 51, 42, 100, 106, 40, 24]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[103, 92, 20, 1, 29, 125, 10, 89, 33, 74, 27, 97, 121, 51, 44, 9]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[27, 21, 89, 14, 10, 117, 49, 17, 40, 108, 92, 28, 23, 82, 65, 115]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[33, 27, 92, 108, 89, 10, 53, 23, 77, 40, 115, 82, 11, 124, 21, 25]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[33, 80, 108, 40, 11, 100, 77, 84, 82, 69, 23, 94, 73, 124, 92, 123]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[83, 118, 97, 123, 47, 124, 20, 12, 56, 44, 16, 9, 106, 90, 121, 34]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[126, 62, 32, 104, 101, 83, 99, 95, 60, 40, 106, 23, 30, 103, 96, 116]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[106, 43, 34, 50, 118, 51, 69, 46, 126, 97, 4, 31, 61, 29, 11, 70]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[26, 55, 46, 93, 122, 44, 34, 19, 98, 97, 105, 112, 77, 114, 16, 9]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[18, 1, 88, 29, 24, 102, 2, 73, 122, 36, 12, 99, 92, 116, 114, 9]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[99, 116, 1, 97, 16, 28, 48, 67, 69, 62, 122, 37, 95, 101, 96, 103]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[101, 116, 69, 62, 96, 16, 75, 30, 56, 122, 50, 103, 89, 97, 48, 60]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[91, 61, 69, 12, 57, 85, 43, 8, 99, 68, 107, 67, 11, 106, 35, 124]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[79, 120, 57, 7, 111, 9, 42, 48, 8, 28, 105, 119, 65, 11, 23, 31]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[97, 62, 96, 83, 99, 106, 116, 24, 73, 32, 51, 101, 78, 103, 56, 124]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[40, 61, 69, 80, 57, 12, 28, 85, 108, 43, 67, 3, 50, 126, 99, 52]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[70, 23, 7, 30, 37, 97, 77, 65, 126, 40, 58, 119, 43, 82, 115, 28]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[28, 90, 98, 122, 99, 42, 79, 40, 114, 93, 77, 65, 49, 126, 13, 57]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[67, 116, 1, 82, 28, 58, 38, 62, 104, 40, 123, 127, 94, 86, 35, 16]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[63, 25, 122, 121, 57, 39, 38, 36, 95, 44, 112, 76, 29, 79, 51, 83]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[92, 21, 27, 17, 20, 110, 121, 14, 127, 33, 108, 1, 38, 24, 125, 49]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[66, 5, 93, 115, 95, 78, 49, 23, 94, 104, 47, 118, 81, 76, 92, 46]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[38, 125, 17, 92, 46, 2, 116, 110, 0, 1, 50, 73, 25, 85, 84, 122]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "==========Entering Body.create_concept_embeddings==========\n",
      "Inputs:\n",
      "Tensor 'E' shape: torch.Size([128, 256])\n",
      "Other-type 'indices': Type=list, Value=[[82, 46, 106, 21, 87, 108, 55, 28, 75, 34, 38, 67, 63, 85, 26, 109]]\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 120, 256])\n",
      "Other-type 'output[1]': Type=list, Value=[120]\n",
      "==========Exiting Body.create_concept_embeddings==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 256])\n",
      "==========Exiting Body.concept_matchup==========\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 64, 256])\n",
      "Integer 'i': Value=0\n",
      "Tuple 'x0s':\n",
      "    Tensor 'x0s[0]' shape: torch.Size([32, 240, 128])\n",
      "    Tensor 'x0s[1]' shape: torch.Size([32, 60, 256])\n",
      "Integer 'training': Value=False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 64, 256])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "==========Entering Body.layers_loop==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 240, 128])\n",
      "Integer 'i': Value=1\n",
      "Tuple 'x0s':\n",
      "    Tensor 'x0s[0]' shape: torch.Size([32, 240, 128])\n",
      "    Tensor 'x0s[1]' shape: torch.Size([32, 60, 256])\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 240, 128])\n",
      "==========Exiting Body.layers_loop==========\n",
      "\n",
      "==========Entering Body.concept_matchup==========\n",
      "Inputs:\n",
      "Tensor 'c' shape: torch.Size([32, 128])\n",
      "Integer 'cvec_samples': Value=16\n",
      "Integer 'cvec_greedy': Value=False\n",
      "Other-type 'cvec_temp': Type=float, Value=1.0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (128) must match the size of tensor b (256) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m c \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m32\u001b[39m, (config\u001b[38;5;241m.\u001b[39mmax_seq_len \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m config\u001b[38;5;241m.\u001b[39mcombo) \u001b[38;5;241m-\u001b[39m config\u001b[38;5;241m.\u001b[39mcombo, config\u001b[38;5;241m.\u001b[39membed_dim)\n\u001b[1;32m     12\u001b[0m x0s \u001b[38;5;241m=\u001b[39m (x,c)\n\u001b[0;32m---> 13\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx0s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcvec_samples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m config\u001b[38;5;241m.\u001b[39mlevels \u001b[38;5;241m=\u001b[39m hold\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m embedding, hold, module, x, c, x0s, output\n",
      "File \u001b[0;32m~/Documents/next-concept-predictor/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/next-concept-predictor/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 32\u001b[0m, in \u001b[0;36mlog_io.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(arg_names, arg_values):\n\u001b[1;32m     30\u001b[0m     log_item(value, name)\n\u001b[0;32m---> 32\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOutputs:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mtuple\u001b[39m):\n",
      "Cell \u001b[0;32mIn[63], line 50\u001b[0m, in \u001b[0;36mBody.forward\u001b[0;34m(self, x0s, cvec_samples, cvec_greedy, cvec_temp, training)\u001b[0m\n\u001b[1;32m     47\u001b[0m x_ \u001b[38;5;241m=\u001b[39m x_[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:]\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# norm? no, i'm worried about affine transformations messing with it since this process wasn't present during training\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# select most similar concept vectors to be appended to the sequence\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcept_matchup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcvec_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcvec_greedy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcvec_temp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# append to x\u001b[39;00m\n\u001b[1;32m     52\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mconcat([x, c\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 32\u001b[0m, in \u001b[0;36mlog_io.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(arg_names, arg_values):\n\u001b[1;32m     30\u001b[0m     log_item(value, name)\n\u001b[0;32m---> 32\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOutputs:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mtuple\u001b[39m):\n",
      "Cell \u001b[0;32mIn[63], line 93\u001b[0m, in \u001b[0;36mBody.concept_matchup\u001b[0;34m(self, c, cvec_samples, cvec_greedy, cvec_temp)\u001b[0m\n\u001b[1;32m     87\u001b[0m vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Batch cosine similarity\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Reshape c: (batch_size x 1 x embedding_dim)\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Reshape embedding: (1 x vocab_size x embedding_dim)\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Resulting similarity: (batch_size x vocab_size)\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m token_similarities \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# how many tokens will we sample to build up our chosen concept vector?\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cvec_samples \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (128) must match the size of tensor b (256) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "# now 2 levels partial sequence (so like we're doing inference)\n",
    "embedding = nn.Embedding(config.vocab_size, config.embed_dim)\n",
    "hold = config.levels\n",
    "config.levels = 2\n",
    "module = Body(config, embedding)\n",
    "module.enable_logging()\n",
    "#module.layers.enable_logging()\n",
    "#module.ca_layers.enable_logging()\n",
    "#module.final_norm.enable_logging()\n",
    "x = torch.randn(32, config.max_seq_len - (config.combo**2), config.embed_dim // config.split)\n",
    "c = torch.randn(32, (config.max_seq_len // config.combo) - config.combo, config.embed_dim)\n",
    "x0s = (x,c)\n",
    "output = module(x0s, cvec_samples = config.combo ** config.levels)\n",
    "config.levels = hold\n",
    "del embedding, hold, module, x, c, x0s, output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "13c35cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 256])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.randn(32, 256)\n",
    "tensor.unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df74cb9f",
   "metadata": {},
   "source": [
    "# embedding vector combination function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c07f7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombineEmbeddings(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.padding_vector = nn.Parameter(torch.zeros(embed_dim), requires_grad=True)\n",
    "        \n",
    "        self.logging_enabled = False\n",
    "    def enable_logging(self):\n",
    "        self.logging_enabled = True\n",
    "    def disable_logging(self):\n",
    "        self.logging_enabled = False\n",
    "\n",
    "    @log_io\n",
    "    def forward(self, tensor, combine_factor):\n",
    "        b, t, d = tensor.shape\n",
    "\n",
    "        # Calculate the necessary amount of padding\n",
    "        remainder = t % combine_factor\n",
    "        padding_needed = 0 if remainder == 0 else combine_factor - remainder\n",
    "        \n",
    "        if padding_needed > 0:\n",
    "            # Replicate the padding vector the necessary number of times\n",
    "            padding = self.padding_vector.repeat(padding_needed, 1).unsqueeze(0).expand(b, -1, -1)\n",
    "            tensor = torch.cat([padding[...,:d], tensor], dim=1) # subset padding to fit with matryoshka size\n",
    "        \n",
    "        # Update t after padding\n",
    "        t_padded = t + padding_needed\n",
    "        \n",
    "        # Reshape the tensor to group 'combine_factor' entries along the t dimension\n",
    "        reshaped_tensor = tensor.view(b, t_padded // combine_factor, combine_factor, d)\n",
    "        \n",
    "        # Sum over the groups\n",
    "        combined_tensor = reshaped_tensor.sum(dim=2)\n",
    "\n",
    "        return combined_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9be31e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### demonstration/debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51028eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "module = CombineEmbeddings(config.embed_dim)\n",
    "module.enable_logging()\n",
    "x = torch.randn(32, config.max_seq_len, config.embed_dim // config.combo)\n",
    "output = module(x, config.combo)\n",
    "del module, x, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf5f333",
   "metadata": {},
   "source": [
    "# Matryoshka Concept Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4707ea52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class matryoshkaConceptLoss(nn.Module):\n",
    "    def __init__(self, config: Config, embedding_combiner):\n",
    "        super().__init__()\n",
    "        self.combo = config.combo\n",
    "        self.levels = config.levels\n",
    "        self.split = config.split\n",
    "        self.level_loss_weight = config.level_loss_weight\n",
    "        self.embedding_combiner = embedding_combiner\n",
    "\n",
    "        if config.concept_loss == \"mae\":\n",
    "            self.concept_loss_fn = nn.L1Loss()\n",
    "        elif config.concept_loss == \"mse\":\n",
    "            self.concept_loss_fn = nn.MSELoss()\n",
    "        else: # defaults to cosine similarity loss\n",
    "            self.concept_loss_fn = nn.CosineSimilarity(dim=-1, eps=1e-6)\n",
    "        \n",
    "        self.logging_enabled = False\n",
    "    def enable_logging(self):\n",
    "        self.logging_enabled = True\n",
    "    def disable_logging(self):\n",
    "        self.logging_enabled = False\n",
    "        \n",
    "    @log_io\n",
    "    def create_target_vecs(self,\n",
    "                           lvl_output: torch.Tensor,\n",
    "                           target_token_ids: torch.Tensor,\n",
    "                           input_len: int,\n",
    "                           embedder: nn.Module,\n",
    "                           i: int\n",
    "                          ) -> torch.Tensor:\n",
    "        lvl_combo = self.combo ** (self.levels-1-i)\n",
    "        target_token_ids_adj = target_token_ids[:, lvl_combo:lvl_combo + input_len]\n",
    "        raw_target_vectors = embedder(target_token_ids_adj)[...,:lvl_output.shape[-1]]\n",
    "        # remember to detach/clone so that we don't mess with the embeddings we want to train on\n",
    "        return self.embedding_combiner(raw_target_vectors, lvl_combo).detach().clone()\n",
    "            \n",
    "    @log_io\n",
    "    def forward(self, \n",
    "                xfs: Tuple[torch.Tensor], \n",
    "                target_token_ids: torch.Tensor, \n",
    "                input_len: int,\n",
    "                embedder: nn.Module,\n",
    "               ) -> torch.Tensor:\n",
    "        # iterate through all concept-embedding layers and calculate loss\n",
    "        concept_loss = torch.tensor(0.0)\n",
    "        for i in range(self.levels - 1):\n",
    "            # select our relevant final residual state\n",
    "            lvl_output = xfs[i]\n",
    "            \n",
    "            # calculate the decay value placed on this level's total amount of loss\n",
    "            lambadada = (self.level_loss_weight ** (self.levels -1 -i))\n",
    "            \n",
    "            # create the target vectors for this level\n",
    "            target_vectors = self.create_target_vecs(lvl_output,\n",
    "                                                     target_token_ids,\n",
    "                                                     input_len,\n",
    "                                                     embedder,\n",
    "                                                     i)\n",
    "            \n",
    "            # now choose whether we're doing mse/mae vs cos sim loss bc they get calc'd differently\n",
    "            if config.concept_loss == 'mae' or config.concept_loss == 'mse':\n",
    "                # Reshape output and target_vectors to combine batch and seq_len dimensions\n",
    "                lvl_output_flat = lvl_output.view(-1, lvl_output.size(-1))\n",
    "                target_vectors_flat = target_vectors.view(-1, target_vectors.size(-1))\n",
    "                \n",
    "                # Calculate MSE or MAE & add it to the total\n",
    "                concept_loss = concept_loss + self.concept_loss_fn(lvl_output_flat, target_vectors_flat) * lambadada\n",
    "            else: # defaults to cosine similarity loss\n",
    "                cosine_loss = (1 - self.concept_loss_fn(lvl_output, target_vectors)).mean()\n",
    "                concept_loss = concept_loss + cosine_loss * lambadada\n",
    "\n",
    "        return concept_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7773d79",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### demonstration/debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c6e5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hold = config.levels\n",
    "config.levels = 3 # i'm too lazy to make this part dynamic so let's just set this to 3\n",
    "embedding_combiner = CombineEmbeddings(config.embed_dim)\n",
    "module = matryoshkaConceptLoss(config, embedding_combiner)\n",
    "module.enable_logging()\n",
    "#module.embedding_combiner.enable_logging()\n",
    "\n",
    "embedder = nn.Embedding(config.vocab_size, config.embed_dim)\n",
    "target_token_ids = torch.randint(config.vocab_size, (32, config.max_seq_len + (config.combo**(config.levels-1))))\n",
    "x = torch.randn(32, config.max_seq_len, config.embed_dim // (config.combo**(config.levels-1)))\n",
    "c2 = torch.randn(32, config.max_seq_len // config.combo, config.embed_dim // config.combo)\n",
    "c4 = torch.randn(32, config.max_seq_len // (config.combo**(config.levels-1)), config.embed_dim)\n",
    "xfs = (c4, c2, x)\n",
    "output = module(xfs, target_token_ids, config.max_seq_len, embedder)\n",
    "config.levels = hold\n",
    "del hold, embedding_combiner, module, embedder, target_token_ids, x, c2, c4, xfs, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467e16de",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba491820",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCP_MatFormer(nn.Module):\n",
    "    def __init__(self, config: Config, tokenizer: tokenizer):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        ### general hyperparameters\n",
    "        self.max_seq_len = config.max_seq_len\n",
    "        self.sa_head_dim = config.sa_head_dim\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.embed_dim = config.embed_dim\n",
    "        \n",
    "        ### matryoshka hyperparameters\n",
    "        self.combo = config.combo\n",
    "        self.levels = config.levels\n",
    "        self.split = config.split\n",
    "        \n",
    "        ### cross-attention\n",
    "        self.ca_connections = config.ca_connections\n",
    "        self.ca_interval = config.ca_interval\n",
    "        \n",
    "        ### embedding\n",
    "        # the embedding matrix. for converting tokens to the first residual state, and the last residual state to logits\n",
    "        self.embedder = nn.Embedding(config.vocab_size, config.embed_dim)\n",
    "        # the function that combines embeddings into higher level concept residual states\n",
    "        self.embedding_combiner = CombineEmbeddings(config.embed_dim)\n",
    "        self.norm_init_embed = config.norm_init_embed # whether to norm the first residual states\n",
    "        if config.norm_init_embed: # if so, define its norm\n",
    "            self.init_embed_norm = Norm(config)\n",
    "        self.scale_init_embed = config.scale_init_embed # whether to scale first residuals by nth root of respective matryoshka dimension\n",
    "        self.init_scale_degree = config.init_scale_degree # the nth root to take\n",
    "        # should we norm the final embed before calculating logits? idk i can't decide\n",
    "        self.norm_final_embed = config.norm_final_embed\n",
    "        if config.norm_final_embed:\n",
    "            self.final_embed_norm = Norm(config)\n",
    "\n",
    "        # the actual bulk of the model\n",
    "        self.body = Body(config, self.embedder)\n",
    "                \n",
    "        ### the loss functions\n",
    "        # lowest-level token model\n",
    "        self.ce_loss_fn = nn.CrossEntropyLoss()\n",
    "        # concept models\n",
    "        self.concept_loss_fn = matryoshkaConceptLoss(config, self.embedding_combiner)\n",
    "        \n",
    "        self.logging_enabled = False\n",
    "    def enable_logging(self):\n",
    "        self.logging_enabled = True\n",
    "    def disable_logging(self):\n",
    "        self.logging_enabled = False\n",
    "        \n",
    "    @log_io\n",
    "    def create_x0s(self, input_token_ids: torch.Tensor, training: bool = False) -> Tuple[torch.Tensor]:\n",
    "        # turn the input tokens into the first residual state using the embedding matrix\n",
    "        x0 = self.embedder(input_token_ids) # (batch_size, input_len, embed_dim)\n",
    "        \n",
    "        ### prepping our first token-wise residual state\n",
    "        # find what matryoshka dimension it should be\n",
    "        first_dim = self.embed_dim // (self.split ** (self.levels-1)) \n",
    "        # splice to said matryoshka dimension\n",
    "        x0t = x0[...,:first_dim] # t stands for token. need to separate it from the raw x0 which will be used later\n",
    "        # optionally norm it\n",
    "        if self.norm_init_embed: x0t = self.init_embed_norm(x0t, training) \n",
    "        # optionally scale by n'th root of dimension\n",
    "        if self.scale_init_embed: x0t = x0t * (first_dim ** (1/self.init_scale_degree)) \n",
    "        # finally instantiate the tuple that'll hold all the residual states\n",
    "        x0s = (x0t,) \n",
    "        \n",
    "        ### iterating through levels to create each higher-level concept residual state\n",
    "        for i in range(self.levels-1):\n",
    "            # combine into smaller tensor by adding token (or lower level concept) embeddings together\n",
    "            lvl_combo = self.combo ** (i+1)\n",
    "            x0c = self.embedding_combiner(x0, lvl_combo) # c stands for concept\n",
    "            \n",
    "            # find the correct matryoshka dimension for this level\n",
    "            this_level_dim = self.embed_dim // (self.split ** (self.levels - 2 - i))\n",
    "            # splice to said matryoshka dimension\n",
    "            x0c = x0c[...,:this_level_dim]\n",
    "            # optionally norm it\n",
    "            if self.norm_init_embed: x0c = self.init_embed_norm(x0c, training) \n",
    "            # optionally scale by n'th root of dimension\n",
    "            if self.scale_init_embed: x0c = x0c * (this_level_dim ** (1/self.init_scale_degree)) \n",
    "            # finally add it to the tuple of residual states\n",
    "            x0s += (x0c,)\n",
    "        \n",
    "        return x0s, first_dim\n",
    "        \n",
    "    @log_io\n",
    "    def forward(\n",
    "        self,\n",
    "        input_token_ids: torch.Tensor, # a shape (batch_size, input_seq_len) list of integer token ids to run forward pass on\n",
    "        target_token_ids: torch.Tensor = None, # a shape (batch_size, input_seq_len + combo ** (levels-1)) list of token ids to train on\n",
    "        ) -> torch.Tensor:\n",
    "        training = True if target_token_ids is not None else False\n",
    "\n",
    "        # create the tuple of initial residual states to calculate on\n",
    "        x0s, first_dim = self.create_x0s(input_token_ids, training) # also, first_dim will be used later\n",
    "\n",
    "        # the body of the model that iterates through the decoder & cross-attention layers\n",
    "        xfs = self.body(x0s, training)\n",
    "\n",
    "        # grabbing the weights of the embedding matrix shape (vocab_size, embed_dim) for use as the output layer\n",
    "        embedder_weight = self.embedder.weight\n",
    "        # optionally norm it\n",
    "        if self.norm_final_embed: embedder_weight = self.final_embed_norm(embedder_weight, training)\n",
    "        # calculating output logits\n",
    "        logits = xfs[-1] @ embedder_weight[:,:first_dim].t()\n",
    "        \n",
    "        if target_token_ids is None: # if we're not training, then we don't need to calculate loss\n",
    "            loss = None\n",
    "        else: # if we are training\n",
    "            ### first up is regular CE token loss\n",
    "            batch_size, input_len, vocab_size = logits.shape\n",
    "            # splice target tokens to exclude the ones that were only to be used by concept levels\n",
    "            target_token_ids_spliced = target_token_ids[:,:input_len]\n",
    "            # we reshape our logits & targets before calculating cross-entropy loss\n",
    "            ce_loss = self.ce_loss_fn(logits.view(batch_size*input_len, vocab_size),\n",
    "                                      target_token_ids_spliced.reshape(batch_size*input_len))\n",
    "\n",
    "            ### the new thing, a regression loss for all our concept-embedding layers\n",
    "            concept_loss = self.concept_loss_fn(xfs, target_token_ids, \n",
    "                                                input_len, self.embedder)\n",
    "            loss = ce_loss + concept_loss\n",
    "            \n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad() # no need to keep track of gradients during inference\n",
    "    @log_io\n",
    "    def Sampler(\n",
    "        self,\n",
    "        logits: torch.Tensor, # shape (batch_size, input_len, vocab_size)\n",
    "        temperature: float, # controls how boring vs random the outputs should be\n",
    "        top_p: float, # the maximum cumulative probability of output options we're willing to consider\n",
    "        top_k: int, # the maximum number of output options we're willing to consider\n",
    "    ) -> torch.Tensor:\n",
    "        # Select the last element for each sequence & apply temperature scaling\n",
    "        logits = logits[:,-1,:].div_(temperature) # -> (batch_size, vocab_size)\n",
    "\n",
    "        # Calculate probabilities with softmax.\n",
    "        probs = torch.softmax(logits, dim=-1, dtype=torch.float) # dim=-1 is the vocab_size dimension that we calculate along\n",
    "\n",
    "        # sort the probabilities to for use in top-p & top-k. both are (batch_size, vocab_size)\n",
    "        probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
    "\n",
    "        ### calculating top-p\n",
    "        # creates same-size tensor of cumulatve probabilities instead of indivdiual probs\n",
    "        probs_sum = torch.cumsum(probs_sort, dim=-1) \n",
    "        # mask where 0's are top-p selections & 1's are to be excluded\n",
    "        top_ps_mask = (probs_sum - probs_sort) > top_p\n",
    "        # the original probabilities with excluded tokens changed to 0.0\n",
    "        probs_sort = torch.where(top_ps_mask, 0, probs_sort) \n",
    "\n",
    "        ### calculating top_k\n",
    "        # create a shape (vocab_size) tensor that just iterates up by 1's\n",
    "        top_ks_mask = torch.arange(probs_idx.shape[-1], device=probs_idx.device) \n",
    "        # expand our mask along the batch_size dimension to become size (batch_size, vocab_size)\n",
    "        top_ks_mask = top_ks_mask.expand(probs_idx.shape[0], -1)\n",
    "        # top_ks is a list of integers. we keep whichever entries in top_ks_mask are greater than their corresponding entries in top_ks\n",
    "        top_ks_mask = top_ks_mask >= top_k\n",
    "\n",
    "        # we'll be combining top-p with top-k and using whichever gives us fewer tokens. a very conservative approach\n",
    "        # this trims probs_sort to also fit within our top_k requirement\n",
    "        probs_sort = torch.where(top_ks_mask, 0, probs_sort)\n",
    "\n",
    "        # Re-normalization so that total probabilities add up to 1\n",
    "        probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
    "        \n",
    "        # now we rearrange the modified probabilities in probs_sort back to their original order according to probs_idx\n",
    "        probs = torch.gather(probs_sort, dim=-1, index=torch.argsort(probs_idx, dim=-1))\n",
    "        \n",
    "        # samples from the distribution\n",
    "        next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        return next_token_id # returns the predicted token\n",
    "        \n",
    "    @log_io\n",
    "    def generate(self,\n",
    "                 prompt: str,\n",
    "                 output_len: int = 1, # the model will output 1 token by default\n",
    "                 temperature: float = 0.7, # 1.0 would be no effect\n",
    "                 top_p: float = 0.8,\n",
    "                 top_k: int = 4,\n",
    "                ) -> str: \n",
    "        \"\"\" Wrapper around sampler() that deals with manipulation of the sequence \"\"\"\n",
    "        # encoding the prompt into token indices\n",
    "        tokens = self.tokenizer.encode(prompt)\n",
    "\n",
    "        # turning it into the right tensor shape\n",
    "        tokens = torch.tensor(tokens, device=config.device).unsqueeze(0)\n",
    "        \n",
    "        # we wouldn't want to go past the maximum context length we trained on\n",
    "        if len(tokens) + output_len > self.config.max_seq_len:\n",
    "            output_len = self.max_seq_len - len(tokens)\n",
    "            print(\"capping output at maximum sequence length\")\n",
    "\n",
    "        for i in range(output_len):\n",
    "            # get the model's output logits and ignore the loss, which would be a NoneType object\n",
    "            logits, _ = self(tokens[:,:self.max_seq_len])\n",
    "            \n",
    "            next_token = self.Sampler(logits, temperature, top_p, top_k)\n",
    "\n",
    "            # add our new token to the sequence\n",
    "            tokens = torch.cat((tokens, next_token), dim=1)\n",
    "\n",
    "        # decode our list of tokens to an actual string\n",
    "        return self.tokenizer.decode(tokens.squeeze(0).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78154851",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### demonstration/debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609e9efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "module = NCP_MatFormer(config, tokenizer)\n",
    "module.enable_logging()\n",
    "#module.embedding_combiner.enable_logging()\n",
    "#module.init_embed_norm.enable_logging()\n",
    "#module.body.enable_logging()\n",
    "#module.final_embed_norm.enable_logging()\n",
    "#module.concept_loss_fn.enable_logging()\n",
    "input_token_ids = torch.randint(config.vocab_size, \n",
    "                                (32, config.max_seq_len))\n",
    "target_token_ids = torch.randint(config.vocab_size, \n",
    "                                 (32, config.max_seq_len + (config.combo ** (config.levels-1))))\n",
    "output, loss = module(input_token_ids, target_token_ids)\n",
    "del module, input_token_ids, target_token_ids, output, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b72153",
   "metadata": {},
   "source": [
    "# Instantiate a brand new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a14d73d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5673.216 K parameters\n",
      "NCP_MatFormer(\n",
      "  (embedder): Embedding(128, 256)\n",
      "  (embedding_combiner): CombineEmbeddings()\n",
      "  (init_embed_norm): Norm()\n",
      "  (final_embed_norm): Norm()\n",
      "  (body): Body(\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x Layer(\n",
      "        (mqa): selfMQA()\n",
      "        (mlp): MLP()\n",
      "        (pre_mqa_norm): Norm()\n",
      "        (post_mqa_norm): Norm()\n",
      "        (pre_mlp_norm): Norm()\n",
      "        (post_mlp_norm): Norm()\n",
      "      )\n",
      "    )\n",
      "    (ca_layers): ModuleList(\n",
      "      (0-5): 6 x crossLayer(\n",
      "        (mqa): crossMQA()\n",
      "        (pre_mqa_norm_x): Norm()\n",
      "        (pre_mqa_norm_c): Norm()\n",
      "        (post_mqa_norm): Norm()\n",
      "      )\n",
      "    )\n",
      "    (final_norm): Norm()\n",
      "  )\n",
      "  (ce_loss_fn): CrossEntropyLoss()\n",
      "  (concept_loss_fn): matryoshkaConceptLoss(\n",
      "    (embedding_combiner): CombineEmbeddings()\n",
      "    (concept_loss_fn): CosineSimilarity()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NCP_MatFormer(config, tokenizer).to(config.device)\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dee2ce1",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00e70b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be our training dataset, the rest for validation\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "abe7827c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading for training which generates a small batch of data of inputs x and targets y\n",
    "def get_batch(split, batch_size):\n",
    "    # whether we grab from our training or validation dataset\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - (config.max_seq_len + (config.combo ** (config.levels-1))), (batch_size,))\n",
    "    x = torch.stack([data[i:i+config.max_seq_len] for i in ix])\n",
    "    ### i actually need the y tensor to be + (config.combo ** (config.levels-1)) to fit the future concepts\n",
    "    y = torch.stack([data[i+1:i+1+(config.max_seq_len + (config.combo ** (config.levels-1)))] for i in ix])\n",
    "    x, y = x.to(config.device), y.to(config.device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d087510",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, batch_size, eval_iters = 5): # to estimate loss during the training loop\n",
    "    out = {}\n",
    "    model.eval() # sets model to eval mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split, batch_size)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # just resets to training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c2da0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "# this is not what they used, but this learning rate & weight decay work for our tiny minGemma\n",
    "config.learning_rate = 1e-5\n",
    "config.weight_decay = 0.02\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "\n",
    "# how long we want to train for\n",
    "config.max_iters = 2\n",
    "\n",
    "# how often we want to check & see how our loss is doing\n",
    "eval_interval = 1\n",
    "\n",
    "# batch size to use\n",
    "config.batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f04d517b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 6.5168, val loss 6.5191, time elapsed: 1.88 seconds\n",
      "step 1: train loss 6.5127, val loss 6.5208, time elapsed: 9.99 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Enable anomaly detection. uncomment these lines if you need to do extensive debugging\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "for iter in range(config.max_iters):\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train', config.batch_size)\n",
    "    \n",
    "    # train\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == config.max_iters - 1:\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - start_time\n",
    "        losses = estimate_loss(model, config.batch_size)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, time elapsed: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Disable anomaly detection after the training loop\n",
    "#torch.autograd.set_detect_anomaly(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bb37af",
   "metadata": {},
   "source": [
    "# Saving your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05320a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = f'models/{model.__class__.__name__}_{time.strftime(\"%Y-%m-%d|%H-%M-%S\")}'\n",
    "torch.save(model.state_dict(), f'{name}.pth')\n",
    "\n",
    "# Convert the dataclass object to a dictionary\n",
    "config_dict = asdict(config)\n",
    "\n",
    "# Serialize the dictionary to a JSON file\n",
    "with open(f'{name}.json', 'w') as f:\n",
    "    json.dump(config_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fb4006",
   "metadata": {},
   "source": [
    "# Load a Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a6c808",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'NCP_MatFormer_2024-03-27|01-39-19'\n",
    "\n",
    "# Deserialize the JSON file back to a dictionary\n",
    "with open(f'models/{name}.json', 'r') as f:\n",
    "    config_dict = json.load(f)\n",
    "\n",
    "# Convert the dictionary back to a dataclass object\n",
    "config = Config(**config_dict)\n",
    "\n",
    "# Initialize a blank model\n",
    "model = NCP_MatFormer(config, tokenizer).to(config.device)  \n",
    "\n",
    "# here's the path to a minGemma model that i've trained with roughly 1m parameters\n",
    "path = f'models/{name}.pth'\n",
    "\n",
    "# Load the saved state dictionary\n",
    "model.load_state_dict(torch.load(path)) \n",
    "# REMEMBER TO CHANGE VALUES IN CONFIG TO MATCH THE MODEL YOU'VE LOADED\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')\n",
    "\n",
    "# If you only plan to do inference, switch to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# If you plan to continue training the model, switch to training mode\n",
    "#model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073fc904",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17955b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou  s thy s to to, s su te thut sots, so tearts susts s,  e  ss triss sis,  tr ts  sut ,  t  so,  so,  ,  t  s, ti  s   tearss,  trus tr e t  s sist s  so, t t tess, s ,   , sost, shouous sit,  sif sotttset, es,  sis ttett  sur sos,  eart, t t\n"
     ]
    }
   ],
   "source": [
    "input_str = \"JULIET:\\nO Romeo, Romeo! wherefore art thou \" # the classic line\n",
    "max_useable_output_len = config.max_seq_len - len(input_str)\n",
    "output = model.generate(input_str, output_len = max_useable_output_len)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d966c71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e385ecdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
